[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dieser persönliche Blog dient als Sammlung für sämtliche Data-Science Projekte in Zusammenhang mit dem Studium und Selbststudium."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hate Speech auf Twitter\n\n\n\n\n\n\n\nTextanalyse\n\n\nKlassifikation\n\n\nHuggingface\n\n\nPython\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2024\n\n\nRaphael Balzer, Matr.Nr.: 00163021\n\n\n\n\n\n\n  \n\n\n\n\nHate Speech auf Reddit\n\n\n\n\n\n\n\nTextanalyse\n\n\nKlassifikation\n\n\nHuggingface\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nRaphael Balzer\n\n\n\n\n\n\n  \n\n\n\n\nHatespeech Klassifikation\n\n\n\n\n\n\n\nTextanalyse\n\n\nKlassifikation\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nRaphael Balzer\n\n\n\n\n\n\n  \n\n\n\n\nMini-Textanalyse DS\n\n\n\n\n\n\n\nText-Analyse\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nRaphael Balzer\n\n\n\n\n\n\n  \n\n\n\n\nbikeshare python\n\n\n\n\n\n\n\nEDA\n\n\nRegression\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nRaphael Balzer\n\n\n\n\n\n\n  \n\n\n\n\nbikeshare prediction\n\n\n\n\n\n\n\nEDA\n\n\nRegression\n\n\ntidymodels\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2023\n\n\nRaphael Balzer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html",
    "href": "posts/Bikeshare Analyse/bikeshare.html",
    "title": "bikeshare prediction",
    "section": "",
    "text": "Im Folgenden sollen Fahrradausleihungen vorhergesagt werden. Gegenstand der Analysen ist ein Datensatz, der unter anderem Wetterdaten, Auskunft über das Datum und die Uhrzeit und die Anzahl der täglich geliehenen Fahrräder zu jeder Stunde enthält. Zunächst soll ein Überblick über den Datensatz und die Wechselwirkungen der Variablen untereinander verschafft werden. Im zweiten Teil werden Modelle mit einigen Vorverarbeitungsschritten trainiert, um dann im letzten Schritt die Vorhersagedatei zu erstellen. Als Framework für die Modellierung wird Tidymodels verwendet.\n\n\n\n\n\nlibrary(ggcorrplot)\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✖ datawizard  0.7.1    ✖ effectsize  0.8.3 \n✖ insight     0.19.2   ✔ modelbased  0.8.6 \n✖ performance 0.10.3   ✖ parameters  0.21.1\n✔ report      0.5.7    ✖ see         0.7.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.2.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ dplyr        1.1.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\nWarning: package 'broom' was built under R version 4.2.3\n\n\nWarning: package 'dials' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'modeldata' was built under R version 4.2.3\n\n\nWarning: package 'parsnip' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'rsample' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'tune' was built under R version 4.2.3\n\n\nWarning: package 'workflowsets' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()         masks scales::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ yardstick::get_weights() masks insight::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::mae()         masks performance::mae()\n✖ parsnip::null_model()    masks insight::null_model()\n✖ infer::p_value()         masks parameters::p_value()\n✖ tune::parameters()       masks dials::parameters(), parameters::parameters()\n✖ yardstick::rmse()        masks performance::rmse()\n✖ dials::smoothness()      masks datawizard::smoothness()\n✖ recipes::step()          masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ readr   2.1.3     ✔ forcats 1.0.0\n✔ stringr 1.5.0     \n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\n\nlibrary(corrr)\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.2.3\n\nlibrary(ggthemes)\nlibrary(ggplot2)\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(Cubist)\n\nWarning: package 'Cubist' was built under R version 4.2.3\n\n\nLoading required package: lattice\n\nlibrary(rules)\n\nWarning: package 'rules' was built under R version 4.2.3\n\n\n\nAttaching package: 'rules'\n\nThe following object is masked from 'package:dials':\n\n    max_rules\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.2.3\n\n\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nThe following object is masked from 'package:parameters':\n\n    compare_models\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.2.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\n\n\n\nlibrary(readr)\nbikeshare_test &lt;- read_csv(\"bikeshare_test.csv\")\n\nRows: 2192 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, season, holiday, func\ndbl (9): hour, temp, humidity, windspeed, visibility, dewpointtemp, solar, r...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(readr)\nbikeshare_train &lt;- read_csv(\"bikeshare_train.csv\")\n\nRows: 6568 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): date, season, holiday, func\ndbl (10): count, hour, temp, humidity, windspeed, visibility, dewpointtemp, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikeshare_train\n\n# A tibble: 6,568 × 14\n   date       count  hour  temp humidity windspeed visibility dewpointtemp solar\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 01/12/2017   173     2  -6         39       1         2000        -17.7     0\n 2 01/12/2017   107     3  -6.2       40       0.9       2000        -17.6     0\n 3 01/12/2017    78     4  -6         36       2.3       2000        -18.6     0\n 4 01/12/2017   100     5  -6.4       37       1.5       2000        -18.7     0\n 5 01/12/2017   181     6  -6.6       35       1.3       2000        -19.5     0\n 6 02/12/2017   167     3  -3.5       81       2.2       1221         -6.2     0\n 7 02/12/2017    89     4  -3.8       79       2         1167         -6.9     0\n 8 02/12/2017    70     6  -4.3       82       2.1       1178         -6.9     0\n 9 02/12/2017   146     7  -4.4       81       2.5       1276         -7.1     0\n10 03/12/2017    32     5   3.9       75       1.9        914         -0.1     0\n# ℹ 6,558 more rows\n# ℹ 5 more variables: rain &lt;dbl&gt;, snow &lt;dbl&gt;, season &lt;chr&gt;, holiday &lt;chr&gt;,\n#   func &lt;chr&gt;\n\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  ggcorr(label = TRUE)\n\nWarning in ggcorr(., label = TRUE): data in column(s) 'date', 'season',\n'holiday', 'func' are not numeric and were ignored\n\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  summarise((across(everything(),~sum(is.na(.x)))))\n\n# A tibble: 1 × 14\n   date count  hour  temp humidity windspeed visibility dewpointtemp solar  rain\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;      &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0     0        0         0          0            0     0     0\n# ℹ 4 more variables: snow &lt;int&gt;, season &lt;int&gt;, holiday &lt;int&gt;, func &lt;int&gt;\n\n\n\nbikeshare_test %&gt;%\n  summarise((across(everything(),~sum(is.na(.x)))))\n\n# A tibble: 1 × 13\n   date  hour  temp humidity windspeed visibility dewpointtemp solar  rain  snow\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;      &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0        0         0          0            0     0     0     0\n# ℹ 3 more variables: season &lt;int&gt;, holiday &lt;int&gt;, func &lt;int&gt;\n\n\n\nvisdat::vis_dat(bikeshare_train)\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_boxplot(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n\n\n\n\nbikeshare_test %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_boxplot(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_histogram(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nbikeshare_test %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;%  \nggplot(., aes(x = value)) +\n  geom_histogram(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = season)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = holiday)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = func)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  ggplot(aes(x = season, fill = holiday)) +\n  geom_bar(alpha = 0.8) +\n  coord_flip() +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  ggplot(aes(x = season, fill = func)) +\n  geom_bar(alpha = 0.8) +\n  coord_flip() +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  mutate(hour = factor(hour)) %&gt;% \n  ggplot()+\n  aes(hour, count, fill = hour) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.8) +\n  scale_fill_viridis_d() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, count) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, temp) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, humidity) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, windspeed) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, visibility) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nEs gibt keine fehlenden Werte, Extremwerte sind auch äußerst rar. Durch die explorativen Datenanalyse ist deutlich zu erkennen, dass die Ausleihungen nach Jahres- und Uhrzeit stark variieren. Außerdem sind die Ausleihungen an Arbeitstagen höher. Bei nicht funktionalen Tagen finden keine Ausleihungen statt. Diese Beobachtung gilt es für die Vorhersagen im Hinterkopf zu behalten. Außerdem haben die Wettervariablen ihre Hoch- oder Tiefpunkte zu ungefähr derselben Uhrzeit, zu der auch am meisten Fahrräder geliehen werden.\n\n\n\n\n\n\n\nset.seed(42)\n\ntrain_test_split &lt;- initial_split(bikeshare_train, prop = 0.7497717)\nbikeshare_train1 &lt;- training(train_test_split)\nbikeshare_test1 &lt;- testing(train_test_split)\n\n\n\n\nDas Hauptaugenmerk bei den Rezepten liegt auf der Datumsspalte und den Interaktionen. Nach der Umwandlung in ein Datumsformat können mit step_date() einige interessante Features extrahiert werden. Außerdem gibt es einige interessante Interaktionseffekte. Die folgenden zwei Rezepte liefern die besten Vorhersagen und unterscheiden sich nur hinsichtlich der Normalisierung der Prädiktoren:\n\nrec72 &lt;- \n  recipe(count ~., data = bikeshare_train1) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_normalize(all_numeric_predictors(), -c(hour, date_doy, date_dow, date_week)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\") \n\n\nrec81 &lt;- \n  recipe(count ~., data = bikeshare_train1) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\")\n\n\n\n\n\nEs werden zwei starke Modelle berechnet, ein XGboost und ein Cubist. Die Wahl der Modellarten basiert hauptsächlich auf persönlichen Präferenzen. Es wird außerdem fünffache Kreuzvalidierung mit drei Wiederholungen verwendet.\n\ncv_scheme &lt;- vfold_cv(bikeshare_train1,\n  v = 5, \n  repeats = 3)\n\n\ndoParallel::registerDoParallel()\n\n\nmod_tree &lt;-\n  decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune(),\n                mode = \"regression\")\n\n\nmod_xg &lt;- boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\", nthreads = 4) %&gt;%\n  set_mode(\"regression\")\n\n\nmod_cubist &lt;- cubist_rules(\n  committees = tune(),\n  neighbors = tune(),\n  max_rules = tune()) %&gt;%\n  set_engine(\"Cubist\", nthreads = 4) %&gt;%\n  set_mode(\"regression\")\n\n\npreproc &lt;- list(rec81 = rec81, rec72 = rec72)\n\nmodels &lt;- list(cubist = mod_cubist, xgboost = mod_xg)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;% \nworkflow_map(\n  resamples = cv_scheme,\n  grid = 10,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 4 tuning:     rec81_cubist\n\n\n✔ 1 of 4 tuning:     rec81_cubist (14m 36.8s)\n\n\ni 2 of 4 tuning:     rec81_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 4 tuning:     rec81_xgboost (29m 41s)\n\n\ni 3 of 4 tuning:     rec72_cubist\n\n\n✔ 3 of 4 tuning:     rec72_cubist (19m 50.1s)\n\n\ni 4 of 4 tuning:     rec72_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 4 of 4 tuning:     rec72_xgboost (10m 41.3s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 80 × 9\n   wflow_id      .config    preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec81_xgboost Preproces… recipe  boos… rmse    standard    273.    15    2.42\n 2 rec81_xgboost Preproces… recipe  boos… rmse    standard    270.    15    2.74\n 3 rec81_xgboost Preproces… recipe  boos… rmse    standard    267.    15    3.25\n 4 rec72_xgboost Preproces… recipe  boos… rmse    standard    267.    15    2.56\n 5 rec72_xgboost Preproces… recipe  boos… rmse    standard    266.    15    2.68\n 6 rec81_xgboost Preproces… recipe  boos… rmse    standard    249.    15    3.37\n 7 rec72_xgboost Preproces… recipe  boos… rmse    standard    244.    15    2.87\n 8 rec72_xgboost Preproces… recipe  boos… rmse    standard    243.    15    3.22\n 9 rec72_cubist  Preproces… recipe  cubi… rmse    standard    200.    15    3.25\n10 rec81_cubist  Preproces… recipe  cubi… rmse    standard    188.    15    3.16\n# ℹ 70 more rows\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec81_cubist\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec81_cubist\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;-\n  best_wf_finalized %&gt;% \n  last_fit(train_test_split)\n\ncollect_metrics(fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     141.    Preprocessor1_Model1\n2 rsq     standard       0.954 Preprocessor1_Model1\n\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip() \n\n\n\n\n\n\n\n\nrecfinal &lt;- \n  recipe(count ~., data = bikeshare_train) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\")\n\n\ncv_scheme2 &lt;- vfold_cv(bikeshare_train,\n  v = 5, \n  repeats = 3)\n\n\npreproc2 &lt;- list(rec81 = recfinal)\n\nmodels2 &lt;- list(cubist = mod_cubist, xgboost = mod_xg)\n\nall_workflows2 &lt;- workflow_set(preproc2, models2)\n\nmodel_set2 &lt;-\nall_workflows2 %&gt;% \nworkflow_map(\n  resamples = cv_scheme2,\n  grid = 10,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 2 tuning:     rec81_cubist\n\n\n✔ 1 of 2 tuning:     rec81_cubist (21m 49.8s)\n\n\ni 2 of 2 tuning:     rec81_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 2 tuning:     rec81_xgboost (11m 21.1s)\n\n\n\ntune::autoplot(model_set2) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set2 %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 40 × 9\n   wflow_id      .config    preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec81_xgboost Preproces… recipe  boos… rmse    standard    274.    15    2.03\n 2 rec81_xgboost Preproces… recipe  boos… rmse    standard    272.    15    2.24\n 3 rec81_xgboost Preproces… recipe  boos… rmse    standard    260.    15    1.87\n 4 rec81_xgboost Preproces… recipe  boos… rmse    standard    242.    15    1.89\n 5 rec81_cubist  Preproces… recipe  cubi… rmse    standard    183.    15    2.65\n 6 rec81_cubist  Preproces… recipe  cubi… rmse    standard    169.    15    2.57\n 7 rec81_xgboost Preproces… recipe  boos… rmse    standard    160.    15    2.33\n 8 rec81_xgboost Preproces… recipe  boos… rmse    standard    159.    15    2.10\n 9 rec81_xgboost Preproces… recipe  boos… rmse    standard    159.    15    2.24\n10 rec81_xgboost Preproces… recipe  boos… rmse    standard    151.    15    2.48\n# ℹ 30 more rows\n\n\n\nbest_model_params2 &lt;- \n  extract_workflow_set_result(model_set2, \"rec81_cubist\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_wf2 &lt;- \nall_workflows2 %&gt;% \n  extract_workflow(\"rec81_cubist\")\n\nbest_wf_finalized2 &lt;- \n  best_wf2 %&gt;% \n  finalize_workflow(best_model_params2)\n\nfit_final2 &lt;-\n  best_wf_finalized2 %&gt;% \n  fit(bikeshare_train)\n\n\n\n\nBei der Vorhersage ist zu beachten, dass es im Train-Sample keine Ausleihungen an funktionalen Tagen gab. Es ist eine vernünftige Annahme, dass dies im Test-Sample wahrscheinlich genauso sein wird. Daher werden manuell alle Vorhersagen für nicht funktionale Tage auf null gesetzt.\n\nfinal_preds &lt;- \n  fit_final2 %&gt;% \n  predict(new_data = bikeshare_test) %&gt;% \n  bind_cols(bikeshare_test)\n\nsubmission_df &lt;-\n  final_preds %&gt;%\n  mutate(id = row_number()) %&gt;%\n  mutate(pred = case_when(func == \"No\" ~ 0,\n                            TRUE ~ .pred)) %&gt;% \n  select(id, pred)\n\n\nsubmission_df %&gt;% \n  ggplot() +\n  aes(pred) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nwrite.csv(submission_df, file = \"Balzer_Raphael_00163021_Prognose.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#vorbereitung",
    "href": "posts/Bikeshare Analyse/bikeshare.html#vorbereitung",
    "title": "bikeshare prediction",
    "section": "",
    "text": "library(ggcorrplot)\n\nLoading required package: ggplot2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.6.0 (red = needs update)\n✔ bayestestR  0.13.1   ✔ correlation 0.8.4 \n✖ datawizard  0.7.1    ✖ effectsize  0.8.3 \n✖ insight     0.19.2   ✔ modelbased  0.8.6 \n✖ performance 0.10.3   ✖ parameters  0.21.1\n✔ report      0.5.7    ✖ see         0.7.5 \n\nRestart the R-Session and update packages in red with `easystats::easystats_update()`.\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.2.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ dplyr        1.1.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n\n\nWarning: package 'broom' was built under R version 4.2.3\n\n\nWarning: package 'dials' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'modeldata' was built under R version 4.2.3\n\n\nWarning: package 'parsnip' was built under R version 4.2.3\n\n\nWarning: package 'recipes' was built under R version 4.2.3\n\n\nWarning: package 'rsample' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'tune' was built under R version 4.2.3\n\n\nWarning: package 'workflowsets' was built under R version 4.2.3\n\n\nWarning: package 'yardstick' was built under R version 4.2.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()         masks scales::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ yardstick::get_weights() masks insight::get_weights()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::mae()         masks performance::mae()\n✖ parsnip::null_model()    masks insight::null_model()\n✖ infer::p_value()         masks parameters::p_value()\n✖ tune::parameters()       masks dials::parameters(), parameters::parameters()\n✖ yardstick::rmse()        masks performance::rmse()\n✖ dials::smoothness()      masks datawizard::smoothness()\n✖ recipes::step()          masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ readr   2.1.3     ✔ forcats 1.0.0\n✔ stringr 1.5.0     \n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\n\nlibrary(corrr)\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.2.3\n\nlibrary(ggthemes)\nlibrary(ggplot2)\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.2.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(lubridate)\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nlibrary(Cubist)\n\nWarning: package 'Cubist' was built under R version 4.2.3\n\n\nLoading required package: lattice\n\nlibrary(rules)\n\nWarning: package 'rules' was built under R version 4.2.3\n\n\n\nAttaching package: 'rules'\n\nThe following object is masked from 'package:dials':\n\n    max_rules\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.2.3\n\n\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nThe following object is masked from 'package:parameters':\n\n    compare_models\n\nlibrary(vip)\n\nWarning: package 'vip' was built under R version 4.2.3\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\n\n\n\nlibrary(readr)\nbikeshare_test &lt;- read_csv(\"bikeshare_test.csv\")\n\nRows: 2192 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, season, holiday, func\ndbl (9): hour, temp, humidity, windspeed, visibility, dewpointtemp, solar, r...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nlibrary(readr)\nbikeshare_train &lt;- read_csv(\"bikeshare_train.csv\")\n\nRows: 6568 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): date, season, holiday, func\ndbl (10): count, hour, temp, humidity, windspeed, visibility, dewpointtemp, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbikeshare_train\n\n# A tibble: 6,568 × 14\n   date       count  hour  temp humidity windspeed visibility dewpointtemp solar\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 01/12/2017   173     2  -6         39       1         2000        -17.7     0\n 2 01/12/2017   107     3  -6.2       40       0.9       2000        -17.6     0\n 3 01/12/2017    78     4  -6         36       2.3       2000        -18.6     0\n 4 01/12/2017   100     5  -6.4       37       1.5       2000        -18.7     0\n 5 01/12/2017   181     6  -6.6       35       1.3       2000        -19.5     0\n 6 02/12/2017   167     3  -3.5       81       2.2       1221         -6.2     0\n 7 02/12/2017    89     4  -3.8       79       2         1167         -6.9     0\n 8 02/12/2017    70     6  -4.3       82       2.1       1178         -6.9     0\n 9 02/12/2017   146     7  -4.4       81       2.5       1276         -7.1     0\n10 03/12/2017    32     5   3.9       75       1.9        914         -0.1     0\n# ℹ 6,558 more rows\n# ℹ 5 more variables: rain &lt;dbl&gt;, snow &lt;dbl&gt;, season &lt;chr&gt;, holiday &lt;chr&gt;,\n#   func &lt;chr&gt;"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#explorative-datenanalyse",
    "href": "posts/Bikeshare Analyse/bikeshare.html#explorative-datenanalyse",
    "title": "bikeshare prediction",
    "section": "",
    "text": "bikeshare_train %&gt;% \n  ggcorr(label = TRUE)\n\nWarning in ggcorr(., label = TRUE): data in column(s) 'date', 'season',\n'holiday', 'func' are not numeric and were ignored\n\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  summarise((across(everything(),~sum(is.na(.x)))))\n\n# A tibble: 1 × 14\n   date count  hour  temp humidity windspeed visibility dewpointtemp solar  rain\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;      &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0     0        0         0          0            0     0     0\n# ℹ 4 more variables: snow &lt;int&gt;, season &lt;int&gt;, holiday &lt;int&gt;, func &lt;int&gt;\n\n\n\nbikeshare_test %&gt;%\n  summarise((across(everything(),~sum(is.na(.x)))))\n\n# A tibble: 1 × 13\n   date  hour  temp humidity windspeed visibility dewpointtemp solar  rain  snow\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;     &lt;int&gt;      &lt;int&gt;        &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     0     0     0        0         0          0            0     0     0     0\n# ℹ 3 more variables: season &lt;int&gt;, holiday &lt;int&gt;, func &lt;int&gt;\n\n\n\nvisdat::vis_dat(bikeshare_train)\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_boxplot(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n\n\n\n\nbikeshare_test %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_boxplot(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;% \nggplot(., aes(x = value)) +\n  geom_histogram(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nbikeshare_test %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pivot_longer(everything()) %&gt;%  \nggplot(., aes(x = value)) +\n  geom_histogram(fill = \"#4E79A7\") +\n  theme_minimal() +\n  facet_wrap(~ name, scales = \"free_x\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = season)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = holiday)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%\n  pivot_longer(cols = 2:11) %&gt;%  \n  ggplot(., aes(x = value, fill = func)) +\n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_x\") +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  ggplot(aes(x = season, fill = holiday)) +\n  geom_bar(alpha = 0.8) +\n  coord_flip() +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  ggplot(aes(x = season, fill = func)) +\n  geom_bar(alpha = 0.8) +\n  coord_flip() +\n  scale_fill_tableau() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;% \n  mutate(hour = factor(hour)) %&gt;% \n  ggplot()+\n  aes(hour, count, fill = hour) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.8) +\n  scale_fill_viridis_d() +\n  theme_minimal()\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, count) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, temp) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, humidity) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, windspeed) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\nbikeshare_train %&gt;%  \n  ggplot()+\n  aes(hour, visibility) +\n  geom_smooth(color = \"#4E79A7\", linewidth = 2 ) +\n  theme_minimal()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\nEs gibt keine fehlenden Werte, Extremwerte sind auch äußerst rar. Durch die explorativen Datenanalyse ist deutlich zu erkennen, dass die Ausleihungen nach Jahres- und Uhrzeit stark variieren. Außerdem sind die Ausleihungen an Arbeitstagen höher. Bei nicht funktionalen Tagen finden keine Ausleihungen statt. Diese Beobachtung gilt es für die Vorhersagen im Hinterkopf zu behalten. Außerdem haben die Wettervariablen ihre Hoch- oder Tiefpunkte zu ungefähr derselben Uhrzeit, zu der auch am meisten Fahrräder geliehen werden."
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#modellierung",
    "href": "posts/Bikeshare Analyse/bikeshare.html#modellierung",
    "title": "bikeshare prediction",
    "section": "",
    "text": "set.seed(42)\n\ntrain_test_split &lt;- initial_split(bikeshare_train, prop = 0.7497717)\nbikeshare_train1 &lt;- training(train_test_split)\nbikeshare_test1 &lt;- testing(train_test_split)\n\n\n\n\nDas Hauptaugenmerk bei den Rezepten liegt auf der Datumsspalte und den Interaktionen. Nach der Umwandlung in ein Datumsformat können mit step_date() einige interessante Features extrahiert werden. Außerdem gibt es einige interessante Interaktionseffekte. Die folgenden zwei Rezepte liefern die besten Vorhersagen und unterscheiden sich nur hinsichtlich der Normalisierung der Prädiktoren:\n\nrec72 &lt;- \n  recipe(count ~., data = bikeshare_train1) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_normalize(all_numeric_predictors(), -c(hour, date_doy, date_dow, date_week)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\") \n\n\nrec81 &lt;- \n  recipe(count ~., data = bikeshare_train1) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\")"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#modelle",
    "href": "posts/Bikeshare Analyse/bikeshare.html#modelle",
    "title": "bikeshare prediction",
    "section": "",
    "text": "Es werden zwei starke Modelle berechnet, ein XGboost und ein Cubist. Die Wahl der Modellarten basiert hauptsächlich auf persönlichen Präferenzen. Es wird außerdem fünffache Kreuzvalidierung mit drei Wiederholungen verwendet.\n\ncv_scheme &lt;- vfold_cv(bikeshare_train1,\n  v = 5, \n  repeats = 3)\n\n\ndoParallel::registerDoParallel()\n\n\nmod_tree &lt;-\n  decision_tree(cost_complexity = tune(),\n                tree_depth = tune(),\n                min_n = tune(),\n                mode = \"regression\")\n\n\nmod_xg &lt;- boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\", nthreads = 4) %&gt;%\n  set_mode(\"regression\")\n\n\nmod_cubist &lt;- cubist_rules(\n  committees = tune(),\n  neighbors = tune(),\n  max_rules = tune()) %&gt;%\n  set_engine(\"Cubist\", nthreads = 4) %&gt;%\n  set_mode(\"regression\")\n\n\npreproc &lt;- list(rec81 = rec81, rec72 = rec72)\n\nmodels &lt;- list(cubist = mod_cubist, xgboost = mod_xg)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;% \nworkflow_map(\n  resamples = cv_scheme,\n  grid = 10,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 4 tuning:     rec81_cubist\n\n\n✔ 1 of 4 tuning:     rec81_cubist (14m 36.8s)\n\n\ni 2 of 4 tuning:     rec81_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 4 tuning:     rec81_xgboost (29m 41s)\n\n\ni 3 of 4 tuning:     rec72_cubist\n\n\n✔ 3 of 4 tuning:     rec72_cubist (19m 50.1s)\n\n\ni 4 of 4 tuning:     rec72_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 4 of 4 tuning:     rec72_xgboost (10m 41.3s)"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#ergebnisse",
    "href": "posts/Bikeshare Analyse/bikeshare.html#ergebnisse",
    "title": "bikeshare prediction",
    "section": "",
    "text": "tune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 80 × 9\n   wflow_id      .config    preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec81_xgboost Preproces… recipe  boos… rmse    standard    273.    15    2.42\n 2 rec81_xgboost Preproces… recipe  boos… rmse    standard    270.    15    2.74\n 3 rec81_xgboost Preproces… recipe  boos… rmse    standard    267.    15    3.25\n 4 rec72_xgboost Preproces… recipe  boos… rmse    standard    267.    15    2.56\n 5 rec72_xgboost Preproces… recipe  boos… rmse    standard    266.    15    2.68\n 6 rec81_xgboost Preproces… recipe  boos… rmse    standard    249.    15    3.37\n 7 rec72_xgboost Preproces… recipe  boos… rmse    standard    244.    15    2.87\n 8 rec72_xgboost Preproces… recipe  boos… rmse    standard    243.    15    3.22\n 9 rec72_cubist  Preproces… recipe  cubi… rmse    standard    200.    15    3.25\n10 rec81_cubist  Preproces… recipe  cubi… rmse    standard    188.    15    3.16\n# ℹ 70 more rows\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec81_cubist\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec81_cubist\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;-\n  best_wf_finalized %&gt;% \n  last_fit(train_test_split)\n\ncollect_metrics(fit_final)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard     141.    Preprocessor1_Model1\n2 rsq     standard       0.954 Preprocessor1_Model1\n\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#trainieren-und-fitten-des-modells-auf-den-ursprünglichen-trainingsdaten",
    "href": "posts/Bikeshare Analyse/bikeshare.html#trainieren-und-fitten-des-modells-auf-den-ursprünglichen-trainingsdaten",
    "title": "bikeshare prediction",
    "section": "",
    "text": "recfinal &lt;- \n  recipe(count ~., data = bikeshare_train) %&gt;%\n  step_mutate(date = lubridate::dmy(date)) %&gt;%\n  step_date(date,  features = c(\"dow\", \"doy\", \"week\"), keep_original_cols = FALSE) %&gt;%\n  step_mutate(date_dow = as.numeric(date_dow),\n              date_week = as.numeric(date_week)) %&gt;%\n  step_dummy(all_nominal_predictors())%&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):hour, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"holiday\"):humidity, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):rain, role = \"predictor\") %&gt;%\n  step_interact(terms = ~starts_with(\"holiday\"):date_dow, role = \"predictor\") %&gt;% \n  step_interact(terms = ~starts_with(\"func\"):temp, role = \"predictor\")\n\n\ncv_scheme2 &lt;- vfold_cv(bikeshare_train,\n  v = 5, \n  repeats = 3)\n\n\npreproc2 &lt;- list(rec81 = recfinal)\n\nmodels2 &lt;- list(cubist = mod_cubist, xgboost = mod_xg)\n\nall_workflows2 &lt;- workflow_set(preproc2, models2)\n\nmodel_set2 &lt;-\nall_workflows2 %&gt;% \nworkflow_map(\n  resamples = cv_scheme2,\n  grid = 10,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 2 tuning:     rec81_cubist\n\n\n✔ 1 of 2 tuning:     rec81_cubist (21m 49.8s)\n\n\ni 2 of 2 tuning:     rec81_xgboost\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 2 tuning:     rec81_xgboost (11m 21.1s)\n\n\n\ntune::autoplot(model_set2) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set2 %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n# A tibble: 40 × 9\n   wflow_id      .config    preproc model .metric .estimator  mean     n std_err\n   &lt;chr&gt;         &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 rec81_xgboost Preproces… recipe  boos… rmse    standard    274.    15    2.03\n 2 rec81_xgboost Preproces… recipe  boos… rmse    standard    272.    15    2.24\n 3 rec81_xgboost Preproces… recipe  boos… rmse    standard    260.    15    1.87\n 4 rec81_xgboost Preproces… recipe  boos… rmse    standard    242.    15    1.89\n 5 rec81_cubist  Preproces… recipe  cubi… rmse    standard    183.    15    2.65\n 6 rec81_cubist  Preproces… recipe  cubi… rmse    standard    169.    15    2.57\n 7 rec81_xgboost Preproces… recipe  boos… rmse    standard    160.    15    2.33\n 8 rec81_xgboost Preproces… recipe  boos… rmse    standard    159.    15    2.10\n 9 rec81_xgboost Preproces… recipe  boos… rmse    standard    159.    15    2.24\n10 rec81_xgboost Preproces… recipe  boos… rmse    standard    151.    15    2.48\n# ℹ 30 more rows\n\n\n\nbest_model_params2 &lt;- \n  extract_workflow_set_result(model_set2, \"rec81_cubist\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'rmse' will be used.\n\nbest_wf2 &lt;- \nall_workflows2 %&gt;% \n  extract_workflow(\"rec81_cubist\")\n\nbest_wf_finalized2 &lt;- \n  best_wf2 %&gt;% \n  finalize_workflow(best_model_params2)\n\nfit_final2 &lt;-\n  best_wf_finalized2 %&gt;% \n  fit(bikeshare_train)"
  },
  {
    "objectID": "posts/Bikeshare Analyse/bikeshare.html#vorhersage-auf-das-test-sample",
    "href": "posts/Bikeshare Analyse/bikeshare.html#vorhersage-auf-das-test-sample",
    "title": "bikeshare prediction",
    "section": "",
    "text": "Bei der Vorhersage ist zu beachten, dass es im Train-Sample keine Ausleihungen an funktionalen Tagen gab. Es ist eine vernünftige Annahme, dass dies im Test-Sample wahrscheinlich genauso sein wird. Daher werden manuell alle Vorhersagen für nicht funktionale Tage auf null gesetzt.\n\nfinal_preds &lt;- \n  fit_final2 %&gt;% \n  predict(new_data = bikeshare_test) %&gt;% \n  bind_cols(bikeshare_test)\n\nsubmission_df &lt;-\n  final_preds %&gt;%\n  mutate(id = row_number()) %&gt;%\n  mutate(pred = case_when(func == \"No\" ~ 0,\n                            TRUE ~ .pred)) %&gt;% \n  select(id, pred)\n\n\nsubmission_df %&gt;% \n  ggplot() +\n  aes(pred) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nwrite.csv(submission_df, file = \"Balzer_Raphael_00163021_Prognose.csv\", row.names = FALSE)"
  },
  {
    "objectID": "posts/bikeshare_python/bike_python.html",
    "href": "posts/bikeshare_python/bike_python.html",
    "title": "bikeshare python",
    "section": "",
    "text": "Bikeshare-Vorhersage aber diesmal in Python.\n\n\n\n\n\nimport pandas as pd\n\n\ndf=pd.read_csv('https://archive.ics.uci.edu/static/public/560/seoul+bike+sharing+demand.zip',encoding= 'unicode_escape',parse_dates=[\"Date\"],dayfirst= \"true\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\n\n\n\n\n0\n2017-12-01\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n1\n2017-12-01\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n2\n2017-12-01\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n3\n2017-12-01\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n4\n2017-12-01\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   Date                       8760 non-null   datetime64[ns]\n 1   Rented Bike Count          8760 non-null   int64         \n 2   Hour                       8760 non-null   int64         \n 3   Temperature(°C)            8760 non-null   float64       \n 4   Humidity(%)                8760 non-null   int64         \n 5   Wind speed (m/s)           8760 non-null   float64       \n 6   Visibility (10m)           8760 non-null   int64         \n 7   Dew point temperature(°C)  8760 non-null   float64       \n 8   Solar Radiation (MJ/m2)    8760 non-null   float64       \n 9   Rainfall(mm)               8760 non-null   float64       \n 10  Snowfall (cm)              8760 non-null   float64       \n 11  Seasons                    8760 non-null   object        \n 12  Holiday                    8760 non-null   object        \n 13  Functioning Day            8760 non-null   object        \ndtypes: datetime64[ns](1), float64(6), int64(4), object(3)\nmemory usage: 958.3+ KB\n\n\n\n\n\n\ndf.nunique().sort_values()\n\nHoliday                         2\nFunctioning Day                 2\nSeasons                         4\nHour                           24\nSnowfall (cm)                  51\nRainfall(mm)                   61\nWind speed (m/s)               65\nHumidity(%)                    90\nSolar Radiation (MJ/m2)       345\nDate                          365\nTemperature(°C)               546\nDew point temperature(°C)     556\nVisibility (10m)             1789\nRented Bike Count            2166\ndtype: int64\n\n\n\n\n\n\ndf.isnull().sum()\n\nDate                         0\nRented Bike Count            0\nHour                         0\nTemperature(°C)              0\nHumidity(%)                  0\nWind speed (m/s)             0\nVisibility (10m)             0\nDew point temperature(°C)    0\nSolar Radiation (MJ/m2)      0\nRainfall(mm)                 0\nSnowfall (cm)                0\nSeasons                      0\nHoliday                      0\nFunctioning Day              0\ndtype: int64\n\n\n\n\n\n\n\n\n\ndf=pd.get_dummies(df,columns=['Holiday','Seasons','Functioning Day'],drop_first=True)\n\n\n\n\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf['Day']=df['Date'].dt.day\ndf['Month']=df['Date'].dt.month\ndf['Year']=df['Date'].dt.year\ndf.drop(columns=['Date'],inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nRented Bike Count\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n0\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n1\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n2\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n3\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n4\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny = df[\"Rented Bike Count\"]\ny\n\n0        254\n1        204\n2        173\n3        107\n4         78\n        ... \n8755    1003\n8756     764\n8757     694\n8758     712\n8759     584\nName: Rented Bike Count, Length: 8760, dtype: int64\n\n\n\nX = df.drop([\"Rented Bike Count\"], axis = 1)\nX\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n0\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n1\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n2\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n3\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n4\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n19\n4.2\n34\n2.6\n1894\n-10.3\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8756\n20\n3.4\n37\n2.3\n2000\n-9.9\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8757\n21\n2.6\n39\n0.3\n1968\n-9.9\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8758\n22\n2.1\n41\n1.0\n1859\n-9.8\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8759\n23\n1.9\n43\n1.3\n1909\n-9.3\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n\n\n8760 rows × 17 columns\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n8415\n15\n13.2\n61\n3.9\n719\n5.8\n1.03\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n11\n2018\n\n\n5049\n9\n22.9\n86\n1.7\n538\n20.4\n0.76\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n29\n6\n2018\n\n\n8395\n19\n11.2\n46\n1.4\n869\n0.0\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n15\n11\n2018\n\n\n1535\n23\n-2.6\n69\n2.0\n1434\n-7.5\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n2\n2\n2018\n\n\n5518\n22\n27.2\n73\n1.5\n1005\n21.9\n0.00\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n18\n7\n2018\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5734\n22\n29.9\n74\n2.0\n1201\n24.7\n0.00\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n27\n7\n2018\n\n\n5191\n7\n23.5\n90\n0.5\n445\n21.7\n0.05\n0.5\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n5\n7\n2018\n\n\n5390\n14\n29.5\n62\n2.7\n1941\n21.4\n1.79\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n13\n7\n2018\n\n\n860\n20\n-3.4\n51\n1.1\n1391\n-12.1\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n1\n2018\n\n\n7270\n22\n19.3\n55\n0.5\n2000\n10.0\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n29\n9\n2018\n\n\n\n\n7008 rows × 17 columns\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n6056\n8\n27.2\n69\n1.8\n1999\n21.0\n0.70\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n10\n8\n2018\n\n\n5556\n12\n32.6\n51\n2.1\n800\n21.1\n3.21\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n20\n7\n2018\n\n\n5990\n14\n34.0\n50\n1.2\n1744\n22.1\n1.68\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n7\n8\n2018\n\n\n7674\n18\n16.9\n47\n1.4\n1637\n5.5\n0.11\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n10\n2018\n\n\n3319\n7\n6.4\n51\n1.0\n1398\n-3.0\n0.19\n0.0\n0.0\nTrue\nTrue\nFalse\nFalse\nTrue\n18\n4\n2018\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8307\n3\n4.6\n65\n0.1\n2000\n-1.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n12\n11\n2018\n\n\n100\n4\n-7.2\n34\n3.0\n2000\n-20.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n12\n2017\n\n\n6605\n5\n20.6\n65\n1.1\n2000\n13.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n2\n9\n2018\n\n\n1783\n7\n-7.2\n70\n1.9\n1946\n-11.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n13\n2\n2018\n\n\n6013\n13\n34.4\n48\n2.1\n1921\n21.7\n2.37\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n8\n8\n2018\n\n\n\n\n1752 rows × 17 columns\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n\n\ny_lm_train_pred = lm.predict(X_train)\ny_lm_test_pred = lm.predict(X_test)\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nlm_train_rmse=np.sqrt(mean_squared_error(y_train, y_lm_train_pred))\nlm_train_rmse\n\n428.36787459866156\n\n\n\nlm_test_rmse = np.sqrt(mean_squared_error(y_test, y_lm_test_pred))\nlm_test_rmse\n\n440.49073091861925\n\n\n\nr2_score(y_train, y_lm_train_pred)\n\n0.5586908577971319\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 4, 5],\n}\n\n\nxgb_model = xgb.XGBRegressor()\n\n\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None, m...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                         'max_depth': [3, 4, 5],\n                         'n_estimators': [100, 200, 300]},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None, m...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                         'max_depth': [3, 4, 5],\n                         'n_estimators': [100, 200, 300]},\n             scoring='neg_mean_squared_error')estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n\n\nbest_model = grid_search.best_estimator_\nbest_model\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=300, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=300, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n\n\n\n\n\nbest_model.score(X_test, y_test)\n\n0.8850799773550728\n\n\n\ny_xgb_test_pred = best_model.predict(X_test)\n\n\nxgb_test_rmse = np.sqrt(mean_squared_error(y_test, y_xgb_test_pred))\nxgb_test_rmse\n\n218.81726410952444\n\n\n\n\n\n\nX_test['pred'] = y_xgb_test_pred\nX_test.loc[X_test['Functioning Day_Yes'] == 0, 'pred'] = 0\nX_test.loc[X_test[\"pred\"] &lt; 0, \"pred\"] = 0\nX_test\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\npred\n\n\n\n\n6056\n8\n27.2\n69\n1.8\n1999\n21.0\n0.70\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n10\n8\n2018\n1524.514648\n\n\n5556\n12\n32.6\n51\n2.1\n800\n21.1\n3.21\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n20\n7\n2018\n732.632812\n\n\n5990\n14\n34.0\n50\n1.2\n1744\n22.1\n1.68\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n7\n8\n2018\n712.731140\n\n\n7674\n18\n16.9\n47\n1.4\n1637\n5.5\n0.11\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n10\n2018\n2194.891357\n\n\n3319\n7\n6.4\n51\n1.0\n1398\n-3.0\n0.19\n0.0\n0.0\nTrue\nTrue\nFalse\nFalse\nTrue\n18\n4\n2018\n639.635742\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8307\n3\n4.6\n65\n0.1\n2000\n-1.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n12\n11\n2018\n292.208466\n\n\n100\n4\n-7.2\n34\n3.0\n2000\n-20.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n12\n2017\n89.085770\n\n\n6605\n5\n20.6\n65\n1.1\n2000\n13.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n2\n9\n2018\n266.967682\n\n\n1783\n7\n-7.2\n70\n1.9\n1946\n-11.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n13\n2\n2018\n220.198990\n\n\n6013\n13\n34.4\n48\n2.1\n1921\n21.7\n2.37\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n8\n8\n2018\n663.002563\n\n\n\n\n1752 rows × 18 columns\n\n\n\n\nnp.sqrt(mean_squared_error(y_test, X_test[\"pred\"]))\n\n217.4355409172396\n\n\n\npred_df = X_test[['pred']]\npred_df = pred_df.reset_index(drop=True)\npred_df.insert(0, 'id', range(1, len(pred_df) + 1))\npred_df\n\n\n\n\n\n\n\n\nid\npred\n\n\n\n\n0\n1\n1524.514648\n\n\n1\n2\n732.632812\n\n\n2\n3\n712.731140\n\n\n3\n4\n2194.891357\n\n\n4\n5\n639.635742\n\n\n...\n...\n...\n\n\n1747\n1748\n292.208466\n\n\n1748\n1749\n89.085770\n\n\n1749\n1750\n266.967682\n\n\n1750\n1751\n220.198990\n\n\n1751\n1752\n663.002563\n\n\n\n\n1752 rows × 2 columns\n\n\n\n\npred_df.to_csv('pred_values.csv', index = False)"
  },
  {
    "objectID": "posts/bikeshare_python/bike_python.html#eda",
    "href": "posts/bikeshare_python/bike_python.html#eda",
    "title": "bikeshare python",
    "section": "",
    "text": "import pandas as pd\n\n\ndf=pd.read_csv('https://archive.ics.uci.edu/static/public/560/seoul+bike+sharing+demand.zip',encoding= 'unicode_escape',parse_dates=[\"Date\"],dayfirst= \"true\")\n\n\ndf.head(5)\n\n\n\n\n\n\n\n\nDate\nRented Bike Count\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nSeasons\nHoliday\nFunctioning Day\n\n\n\n\n0\n2017-12-01\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n1\n2017-12-01\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n2\n2017-12-01\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n3\n2017-12-01\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n4\n2017-12-01\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nWinter\nNo Holiday\nYes\n\n\n\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   Date                       8760 non-null   datetime64[ns]\n 1   Rented Bike Count          8760 non-null   int64         \n 2   Hour                       8760 non-null   int64         \n 3   Temperature(°C)            8760 non-null   float64       \n 4   Humidity(%)                8760 non-null   int64         \n 5   Wind speed (m/s)           8760 non-null   float64       \n 6   Visibility (10m)           8760 non-null   int64         \n 7   Dew point temperature(°C)  8760 non-null   float64       \n 8   Solar Radiation (MJ/m2)    8760 non-null   float64       \n 9   Rainfall(mm)               8760 non-null   float64       \n 10  Snowfall (cm)              8760 non-null   float64       \n 11  Seasons                    8760 non-null   object        \n 12  Holiday                    8760 non-null   object        \n 13  Functioning Day            8760 non-null   object        \ndtypes: datetime64[ns](1), float64(6), int64(4), object(3)\nmemory usage: 958.3+ KB\n\n\n\n\n\n\ndf.nunique().sort_values()\n\nHoliday                         2\nFunctioning Day                 2\nSeasons                         4\nHour                           24\nSnowfall (cm)                  51\nRainfall(mm)                   61\nWind speed (m/s)               65\nHumidity(%)                    90\nSolar Radiation (MJ/m2)       345\nDate                          365\nTemperature(°C)               546\nDew point temperature(°C)     556\nVisibility (10m)             1789\nRented Bike Count            2166\ndtype: int64\n\n\n\n\n\n\ndf.isnull().sum()\n\nDate                         0\nRented Bike Count            0\nHour                         0\nTemperature(°C)              0\nHumidity(%)                  0\nWind speed (m/s)             0\nVisibility (10m)             0\nDew point temperature(°C)    0\nSolar Radiation (MJ/m2)      0\nRainfall(mm)                 0\nSnowfall (cm)                0\nSeasons                      0\nHoliday                      0\nFunctioning Day              0\ndtype: int64"
  },
  {
    "objectID": "posts/bikeshare_python/bike_python.html#vorverarbeitung",
    "href": "posts/bikeshare_python/bike_python.html#vorverarbeitung",
    "title": "bikeshare python",
    "section": "",
    "text": "df=pd.get_dummies(df,columns=['Holiday','Seasons','Functioning Day'],drop_first=True)\n\n\n\n\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf['Day']=df['Date'].dt.day\ndf['Month']=df['Date'].dt.month\ndf['Year']=df['Date'].dt.year\ndf.drop(columns=['Date'],inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nRented Bike Count\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n0\n254\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n1\n204\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n2\n173\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n3\n107\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n4\n78\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017"
  },
  {
    "objectID": "posts/bikeshare_python/bike_python.html#modellierung",
    "href": "posts/bikeshare_python/bike_python.html#modellierung",
    "title": "bikeshare python",
    "section": "",
    "text": "y = df[\"Rented Bike Count\"]\ny\n\n0        254\n1        204\n2        173\n3        107\n4         78\n        ... \n8755    1003\n8756     764\n8757     694\n8758     712\n8759     584\nName: Rented Bike Count, Length: 8760, dtype: int64\n\n\n\nX = df.drop([\"Rented Bike Count\"], axis = 1)\nX\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n0\n0\n-5.2\n37\n2.2\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n1\n1\n-5.5\n38\n0.8\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n2\n2\n-6.0\n39\n1.0\n2000\n-17.7\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n3\n3\n-6.2\n40\n0.9\n2000\n-17.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n4\n4\n-6.0\n36\n2.3\n2000\n-18.6\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n1\n12\n2017\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8755\n19\n4.2\n34\n2.6\n1894\n-10.3\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8756\n20\n3.4\n37\n2.3\n2000\n-9.9\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8757\n21\n2.6\n39\n0.3\n1968\n-9.9\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8758\n22\n2.1\n41\n1.0\n1859\n-9.8\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n8759\n23\n1.9\n43\n1.3\n1909\n-9.3\n0.0\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n30\n11\n2018\n\n\n\n\n8760 rows × 17 columns\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\nX_train\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n8415\n15\n13.2\n61\n3.9\n719\n5.8\n1.03\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n11\n2018\n\n\n5049\n9\n22.9\n86\n1.7\n538\n20.4\n0.76\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n29\n6\n2018\n\n\n8395\n19\n11.2\n46\n1.4\n869\n0.0\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n15\n11\n2018\n\n\n1535\n23\n-2.6\n69\n2.0\n1434\n-7.5\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n2\n2\n2018\n\n\n5518\n22\n27.2\n73\n1.5\n1005\n21.9\n0.00\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n18\n7\n2018\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5734\n22\n29.9\n74\n2.0\n1201\n24.7\n0.00\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n27\n7\n2018\n\n\n5191\n7\n23.5\n90\n0.5\n445\n21.7\n0.05\n0.5\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n5\n7\n2018\n\n\n5390\n14\n29.5\n62\n2.7\n1941\n21.4\n1.79\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n13\n7\n2018\n\n\n860\n20\n-3.4\n51\n1.1\n1391\n-12.1\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n1\n2018\n\n\n7270\n22\n19.3\n55\n0.5\n2000\n10.0\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n29\n9\n2018\n\n\n\n\n7008 rows × 17 columns\n\n\n\n\nX_test\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\n\n\n\n\n6056\n8\n27.2\n69\n1.8\n1999\n21.0\n0.70\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n10\n8\n2018\n\n\n5556\n12\n32.6\n51\n2.1\n800\n21.1\n3.21\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n20\n7\n2018\n\n\n5990\n14\n34.0\n50\n1.2\n1744\n22.1\n1.68\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n7\n8\n2018\n\n\n7674\n18\n16.9\n47\n1.4\n1637\n5.5\n0.11\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n10\n2018\n\n\n3319\n7\n6.4\n51\n1.0\n1398\n-3.0\n0.19\n0.0\n0.0\nTrue\nTrue\nFalse\nFalse\nTrue\n18\n4\n2018\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8307\n3\n4.6\n65\n0.1\n2000\n-1.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n12\n11\n2018\n\n\n100\n4\n-7.2\n34\n3.0\n2000\n-20.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n12\n2017\n\n\n6605\n5\n20.6\n65\n1.1\n2000\n13.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n2\n9\n2018\n\n\n1783\n7\n-7.2\n70\n1.9\n1946\n-11.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n13\n2\n2018\n\n\n6013\n13\n34.4\n48\n2.1\n1921\n21.7\n2.37\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n8\n8\n2018\n\n\n\n\n1752 rows × 17 columns\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n\n\ny_lm_train_pred = lm.predict(X_train)\ny_lm_test_pred = lm.predict(X_test)\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nlm_train_rmse=np.sqrt(mean_squared_error(y_train, y_lm_train_pred))\nlm_train_rmse\n\n428.36787459866156\n\n\n\nlm_test_rmse = np.sqrt(mean_squared_error(y_test, y_lm_test_pred))\nlm_test_rmse\n\n440.49073091861925\n\n\n\nr2_score(y_train, y_lm_train_pred)\n\n0.5586908577971319\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 4, 5],\n}\n\n\nxgb_model = xgb.XGBRegressor()\n\n\ngrid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n\n\ngrid_search.fit(X_train, y_train)\n\nGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None, m...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                         'max_depth': [3, 4, 5],\n                         'n_estimators': [100, 200, 300]},\n             scoring='neg_mean_squared_error')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=5,\n             estimator=XGBRegressor(base_score=None, booster=None,\n                                    callbacks=None, colsample_bylevel=None,\n                                    colsample_bynode=None,\n                                    colsample_bytree=None, device=None,\n                                    early_stopping_rounds=None,\n                                    enable_categorical=False, eval_metric=None,\n                                    feature_types=None, gamma=None,\n                                    grow_policy=None, importance_type=None,\n                                    interaction_constraints=None,\n                                    learning_rate=None, m...\n                                    max_cat_to_onehot=None, max_delta_step=None,\n                                    max_depth=None, max_leaves=None,\n                                    min_child_weight=None, missing=nan,\n                                    monotone_constraints=None,\n                                    multi_strategy=None, n_estimators=None,\n                                    n_jobs=None, num_parallel_tree=None,\n                                    random_state=None, ...),\n             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n                         'max_depth': [3, 4, 5],\n                         'n_estimators': [100, 200, 300]},\n             scoring='neg_mean_squared_error')estimator: XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=None, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n\n\nbest_model = grid_search.best_estimator_\nbest_model\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=300, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=5, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=300, n_jobs=None,\n             num_parallel_tree=None, random_state=None, ...)\n\n\n\n\n\n\nbest_model.score(X_test, y_test)\n\n0.8850799773550728\n\n\n\ny_xgb_test_pred = best_model.predict(X_test)\n\n\nxgb_test_rmse = np.sqrt(mean_squared_error(y_test, y_xgb_test_pred))\nxgb_test_rmse\n\n218.81726410952444\n\n\n\n\n\n\nX_test['pred'] = y_xgb_test_pred\nX_test.loc[X_test['Functioning Day_Yes'] == 0, 'pred'] = 0\nX_test.loc[X_test[\"pred\"] &lt; 0, \"pred\"] = 0\nX_test\n\n\n\n\n\n\n\n\nHour\nTemperature(°C)\nHumidity(%)\nWind speed (m/s)\nVisibility (10m)\nDew point temperature(°C)\nSolar Radiation (MJ/m2)\nRainfall(mm)\nSnowfall (cm)\nHoliday_No Holiday\nSeasons_Spring\nSeasons_Summer\nSeasons_Winter\nFunctioning Day_Yes\nDay\nMonth\nYear\npred\n\n\n\n\n6056\n8\n27.2\n69\n1.8\n1999\n21.0\n0.70\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n10\n8\n2018\n1524.514648\n\n\n5556\n12\n32.6\n51\n2.1\n800\n21.1\n3.21\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n20\n7\n2018\n732.632812\n\n\n5990\n14\n34.0\n50\n1.2\n1744\n22.1\n1.68\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n7\n8\n2018\n712.731140\n\n\n7674\n18\n16.9\n47\n1.4\n1637\n5.5\n0.11\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n16\n10\n2018\n2194.891357\n\n\n3319\n7\n6.4\n51\n1.0\n1398\n-3.0\n0.19\n0.0\n0.0\nTrue\nTrue\nFalse\nFalse\nTrue\n18\n4\n2018\n639.635742\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n8307\n3\n4.6\n65\n0.1\n2000\n-1.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n12\n11\n2018\n292.208466\n\n\n100\n4\n-7.2\n34\n3.0\n2000\n-20.4\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n5\n12\n2017\n89.085770\n\n\n6605\n5\n20.6\n65\n1.1\n2000\n13.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nFalse\nTrue\n2\n9\n2018\n266.967682\n\n\n1783\n7\n-7.2\n70\n1.9\n1946\n-11.7\n0.00\n0.0\n0.0\nTrue\nFalse\nFalse\nTrue\nTrue\n13\n2\n2018\n220.198990\n\n\n6013\n13\n34.4\n48\n2.1\n1921\n21.7\n2.37\n0.0\n0.0\nTrue\nFalse\nTrue\nFalse\nTrue\n8\n8\n2018\n663.002563\n\n\n\n\n1752 rows × 18 columns\n\n\n\n\nnp.sqrt(mean_squared_error(y_test, X_test[\"pred\"]))\n\n217.4355409172396\n\n\n\npred_df = X_test[['pred']]\npred_df = pred_df.reset_index(drop=True)\npred_df.insert(0, 'id', range(1, len(pred_df) + 1))\npred_df\n\n\n\n\n\n\n\n\nid\npred\n\n\n\n\n0\n1\n1524.514648\n\n\n1\n2\n732.632812\n\n\n2\n3\n712.731140\n\n\n3\n4\n2194.891357\n\n\n4\n5\n639.635742\n\n\n...\n...\n...\n\n\n1747\n1748\n292.208466\n\n\n1748\n1749\n89.085770\n\n\n1749\n1750\n266.967682\n\n\n1750\n1751\n220.198990\n\n\n1751\n1752\n663.002563\n\n\n\n\n1752 rows × 2 columns\n\n\n\n\npred_df.to_csv('pred_values.csv', index = False)"
  },
  {
    "objectID": "posts/Goldener-Topf_Analyse/goto.html",
    "href": "posts/Goldener-Topf_Analyse/goto.html",
    "title": "Mini-Textanalyse DS",
    "section": "",
    "text": "library(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\n\n\n\nTextanalyse von E.T.A-Hoffmanns “Der goldene Topf”.\n\ntopf &lt;- read.delim2(\"https://www.gutenberg.org/cache/epub/17362/pg17362.txt\")\ntopf &lt;- as_tibble(topf)\ntopf &lt;- topf[-c(1:24, 2678:2979), ]\ntopf\n\n# A tibble: 2,653 × 1\n   The.Project.Gutenberg.eBook.of.Der.Goldene.Topf                  \n   &lt;chr&gt;                                                            \n 1 DER GOLDENE TOPF                                                 \n 2 von                                                              \n 3 E.T.A. HOFFMANN:                                                 \n 4 Mit 11 Federzeichnungen von Edmund Schaefer                      \n 5 [Illustration: Titelbild. Die Frauenkirche in Dresden]           \n 6 Erstes bis fünftes Tausend                                       \n 7 Verlag von Gustav Kiepenheuer Weimar 1913                        \n 8 ERSTE VIGILIE.                                                   \n 9 Die Unglücksfälle des Studenten Anselmus. Des Konrektors Paulmann\n10 Sanitätsknaster und die goldgrünen Schlangen.                    \n# ℹ 2,643 more rows\n\n\n\n\n\ntopf_token &lt;- topf %&gt;% \n  unnest_tokens(output = token, input = The.Project.Gutenberg.eBook.of.Der.Goldene.Topf) %&gt;% \n  filter(str_detect(token, \"[a-z]\"))\ntopf_token\n\n# A tibble: 29,219 × 1\n   token           \n   &lt;chr&gt;           \n 1 der             \n 2 goldene         \n 3 topf            \n 4 von             \n 5 e.t.a           \n 6 hoffmann        \n 7 mit             \n 8 federzeichnungen\n 9 von             \n10 edmund          \n# ℹ 29,209 more rows\n\n\n\n\n\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de &lt;- tibble(word = stopwords_de)\n\nstopwords_de &lt;- stopwords_de %&gt;% \n  rename(token = word)  \n\ntopf_token &lt;- topf_token %&gt;% \n  anti_join(stopwords_de)\n\nJoining with `by = join_by(token)`\n\ntopf_token %&gt;% \n  count(token, sort = TRUE) %&gt;% \n  print()\n\n# A tibble: 5,508 × 2\n   token           n\n   &lt;chr&gt;       &lt;int&gt;\n 1 er            446\n 2 anselmus      291\n 3 archivarius   160\n 4 denn          151\n 5 du            144\n 6 nun           137\n 7 ihn           118\n 8 veronika      105\n 9 student        97\n10 wohl           94\n# ℹ 5,498 more rows\n\n\n\n\n\n\n\ndata(sentiws, package = \"pradadata\")\ntopf_senti &lt;- topf_token %&gt;% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %&gt;% \n  select(-inflections)\n\nWarning in inner_join(., sentiws, by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 2698 of `x` matches multiple rows in `y`.\nℹ Row 3187 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntopf_senti\n\n# A tibble: 600 × 3\n   token     neg_pos   value\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 glücklich pos      0.115 \n 2 besonders pos      0.539 \n 3 schnell   pos      0.117 \n 4 lachen    pos      0.0135\n 5 entziehen neg     -0.0048\n 6 weise     pos      0.224 \n 7 festlich  pos      0.202 \n 8 kurz      neg     -0.0048\n 9 langsam   neg     -0.0167\n10 einsam    neg     -0.163 \n# ℹ 590 more rows\n\n\n\ntopf_senti %&gt;%\n  count(token, neg_pos, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  ungroup() %&gt;%\n  mutate(token = reorder(token, n)) %&gt;%\n  ggplot(aes(n, token, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Wort\") +\n  theme_minimal() +\n  scale_fill_tableau(palette = \"Nuriel Stone\")\n\n\n\n\n\ntopf_senti %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %&gt;% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %&gt;% \n           round(2))\n\n# A tibble: 2 × 4\n  neg_pos polarity_sum polarity_count polarity_prop\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;int&gt;         &lt;dbl&gt;\n1 neg            -29.5            184          0.31\n2 pos             72.3            416          0.69\n\n\n\ntopf_senti %&gt;% \n  distinct(token, .keep_all = TRUE) %&gt;% \n  mutate(value_abs = abs(value)) %&gt;%\n  group_by(neg_pos) %&gt;%\n  top_n(10, value_abs) %&gt;%\n  mutate(token = reorder(token, value_abs)) %&gt;%\n  ggplot(aes(value_abs, token, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Effektstärke\",\n       y = \"Wort\") +\n  theme_minimal() +\n  scale_fill_tableau(palette = \"Nuriel Stone\")\n\n\n\n\n\n\n\nHäufigkeiten der Bigramme\n\ntopf_bigram &lt;- \n  topf %&gt;%\n  unnest_tokens(bigram, The.Project.Gutenberg.eBook.of.Der.Goldene.Topf, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\n\ntopf_bigram %&gt;% \ncount(bigram, sort = TRUE) %&gt;% \n  print()\n\n# A tibble: 19,046 × 2\n   bigram                    n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 in der                   89\n 2 der student              85\n 3 student anselmus         77\n 4 archivarius lindhorst    72\n 5 der archivarius          67\n 6 in den                   49\n 7 in die                   49\n 8 der konrektor            46\n 9 in dem                   46\n10 konrektor paulmann       46\n# ℹ 19,036 more rows\n\n\n\n\n\ntopf_bigra_sep &lt;- topf_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntopf_bigra_sep %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram)) +\n  geom_col(fill = \"#8175aa\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Bigram\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nVerneinungen &lt;- c(\"nicht\", \"nie\", \"niemals\", \"keine\", \"kein\")\n\ntopf_bigra_sep %&gt;%\n  filter(word1 %in% Verneinungen) %&gt;%\n  inner_join(sentiws, by = c(word2 = \"word\")) %&gt;%\n  count(word1, word2, value, sort = TRUE) %&gt;% \n  mutate(Beitrag = n * value) %&gt;%\n  arrange(desc(abs(Beitrag))) %&gt;%\n  head(20) %&gt;%\n  mutate(word2 = reorder(word2, Beitrag)) %&gt;%\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Sentiment-Wert * Häufigkeit\",\n       y = \"Verneinungen\") +\n  theme_minimal()+\n  scale_fill_tableau(\"Nuriel Stone\")\n\n\n\n\n\n\n\n\n\ntopf_dtm &lt;- DocumentTermMatrix(topf_token)\ntopf_lda &lt;- LDA(topf_dtm, k = 4, control = list(seed = 42))\n\n\ntopf_themen &lt;- tidy(topf_lda, matrix = \"beta\")\n\ntopf_themen &lt;- topf_themen %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntopf_themen %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  theme_minimal() +\n  scale_fill_tableau(\"Nuriel Stone\")"
  },
  {
    "objectID": "posts/Goldener-Topf_Analyse/goto.html#datenimport",
    "href": "posts/Goldener-Topf_Analyse/goto.html#datenimport",
    "title": "Mini-Textanalyse DS",
    "section": "",
    "text": "Textanalyse von E.T.A-Hoffmanns “Der goldene Topf”.\n\ntopf &lt;- read.delim2(\"https://www.gutenberg.org/cache/epub/17362/pg17362.txt\")\ntopf &lt;- as_tibble(topf)\ntopf &lt;- topf[-c(1:24, 2678:2979), ]\ntopf\n\n# A tibble: 2,653 × 1\n   The.Project.Gutenberg.eBook.of.Der.Goldene.Topf                  \n   &lt;chr&gt;                                                            \n 1 DER GOLDENE TOPF                                                 \n 2 von                                                              \n 3 E.T.A. HOFFMANN:                                                 \n 4 Mit 11 Federzeichnungen von Edmund Schaefer                      \n 5 [Illustration: Titelbild. Die Frauenkirche in Dresden]           \n 6 Erstes bis fünftes Tausend                                       \n 7 Verlag von Gustav Kiepenheuer Weimar 1913                        \n 8 ERSTE VIGILIE.                                                   \n 9 Die Unglücksfälle des Studenten Anselmus. Des Konrektors Paulmann\n10 Sanitätsknaster und die goldgrünen Schlangen.                    \n# ℹ 2,643 more rows\n\n\n\n\n\ntopf_token &lt;- topf %&gt;% \n  unnest_tokens(output = token, input = The.Project.Gutenberg.eBook.of.Der.Goldene.Topf) %&gt;% \n  filter(str_detect(token, \"[a-z]\"))\ntopf_token\n\n# A tibble: 29,219 × 1\n   token           \n   &lt;chr&gt;           \n 1 der             \n 2 goldene         \n 3 topf            \n 4 von             \n 5 e.t.a           \n 6 hoffmann        \n 7 mit             \n 8 federzeichnungen\n 9 von             \n10 edmund          \n# ℹ 29,209 more rows\n\n\n\n\n\n\ndata(stopwords_de, package = \"lsa\")\n\nstopwords_de &lt;- tibble(word = stopwords_de)\n\nstopwords_de &lt;- stopwords_de %&gt;% \n  rename(token = word)  \n\ntopf_token &lt;- topf_token %&gt;% \n  anti_join(stopwords_de)\n\nJoining with `by = join_by(token)`\n\ntopf_token %&gt;% \n  count(token, sort = TRUE) %&gt;% \n  print()\n\n# A tibble: 5,508 × 2\n   token           n\n   &lt;chr&gt;       &lt;int&gt;\n 1 er            446\n 2 anselmus      291\n 3 archivarius   160\n 4 denn          151\n 5 du            144\n 6 nun           137\n 7 ihn           118\n 8 veronika      105\n 9 student        97\n10 wohl           94\n# ℹ 5,498 more rows"
  },
  {
    "objectID": "posts/Goldener-Topf_Analyse/goto.html#sentimentanalyse",
    "href": "posts/Goldener-Topf_Analyse/goto.html#sentimentanalyse",
    "title": "Mini-Textanalyse DS",
    "section": "",
    "text": "data(sentiws, package = \"pradadata\")\ntopf_senti &lt;- topf_token %&gt;% \n  inner_join(sentiws, by = c(\"token\" = \"word\")) %&gt;% \n  select(-inflections)\n\nWarning in inner_join(., sentiws, by = c(token = \"word\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 2698 of `x` matches multiple rows in `y`.\nℹ Row 3187 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntopf_senti\n\n# A tibble: 600 × 3\n   token     neg_pos   value\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 glücklich pos      0.115 \n 2 besonders pos      0.539 \n 3 schnell   pos      0.117 \n 4 lachen    pos      0.0135\n 5 entziehen neg     -0.0048\n 6 weise     pos      0.224 \n 7 festlich  pos      0.202 \n 8 kurz      neg     -0.0048\n 9 langsam   neg     -0.0167\n10 einsam    neg     -0.163 \n# ℹ 590 more rows\n\n\n\ntopf_senti %&gt;%\n  count(token, neg_pos, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  ungroup() %&gt;%\n  mutate(token = reorder(token, n)) %&gt;%\n  ggplot(aes(n, token, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Wort\") +\n  theme_minimal() +\n  scale_fill_tableau(palette = \"Nuriel Stone\")\n\n\n\n\n\ntopf_senti %&gt;% \n  group_by(neg_pos) %&gt;% \n  summarise(polarity_sum = sum(value),\n            polarity_count = n()) %&gt;% \n  mutate(polarity_prop = (polarity_count / sum(polarity_count)) %&gt;% \n           round(2))\n\n# A tibble: 2 × 4\n  neg_pos polarity_sum polarity_count polarity_prop\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;int&gt;         &lt;dbl&gt;\n1 neg            -29.5            184          0.31\n2 pos             72.3            416          0.69\n\n\n\ntopf_senti %&gt;% \n  distinct(token, .keep_all = TRUE) %&gt;% \n  mutate(value_abs = abs(value)) %&gt;%\n  group_by(neg_pos) %&gt;%\n  top_n(10, value_abs) %&gt;%\n  mutate(token = reorder(token, value_abs)) %&gt;%\n  ggplot(aes(value_abs, token, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Effektstärke\",\n       y = \"Wort\") +\n  theme_minimal() +\n  scale_fill_tableau(palette = \"Nuriel Stone\")"
  },
  {
    "objectID": "posts/Goldener-Topf_Analyse/goto.html#n-gram---analyse",
    "href": "posts/Goldener-Topf_Analyse/goto.html#n-gram---analyse",
    "title": "Mini-Textanalyse DS",
    "section": "",
    "text": "Häufigkeiten der Bigramme\n\ntopf_bigram &lt;- \n  topf %&gt;%\n  unnest_tokens(bigram, The.Project.Gutenberg.eBook.of.Der.Goldene.Topf, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\n\ntopf_bigram %&gt;% \ncount(bigram, sort = TRUE) %&gt;% \n  print()\n\n# A tibble: 19,046 × 2\n   bigram                    n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 in der                   89\n 2 der student              85\n 3 student anselmus         77\n 4 archivarius lindhorst    72\n 5 der archivarius          67\n 6 in den                   49\n 7 in die                   49\n 8 der konrektor            46\n 9 in dem                   46\n10 konrektor paulmann       46\n# ℹ 19,036 more rows\n\n\n\n\n\ntopf_bigra_sep &lt;- topf_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntopf_bigra_sep %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram)) +\n  geom_col(fill = \"#8175aa\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Bigram\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nVerneinungen &lt;- c(\"nicht\", \"nie\", \"niemals\", \"keine\", \"kein\")\n\ntopf_bigra_sep %&gt;%\n  filter(word1 %in% Verneinungen) %&gt;%\n  inner_join(sentiws, by = c(word2 = \"word\")) %&gt;%\n  count(word1, word2, value, sort = TRUE) %&gt;% \n  mutate(Beitrag = n * value) %&gt;%\n  arrange(desc(abs(Beitrag))) %&gt;%\n  head(20) %&gt;%\n  mutate(word2 = reorder(word2, Beitrag)) %&gt;%\n  ggplot(aes(n * value, word2, fill = n * value &gt; 0)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = \"Sentiment-Wert * Häufigkeit\",\n       y = \"Verneinungen\") +\n  theme_minimal()+\n  scale_fill_tableau(\"Nuriel Stone\")"
  },
  {
    "objectID": "posts/Goldener-Topf_Analyse/goto.html#themenanalyse",
    "href": "posts/Goldener-Topf_Analyse/goto.html#themenanalyse",
    "title": "Mini-Textanalyse DS",
    "section": "",
    "text": "topf_dtm &lt;- DocumentTermMatrix(topf_token)\ntopf_lda &lt;- LDA(topf_dtm, k = 4, control = list(seed = 42))\n\n\ntopf_themen &lt;- tidy(topf_lda, matrix = \"beta\")\n\ntopf_themen &lt;- topf_themen %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntopf_themen %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  theme_minimal() +\n  scale_fill_tableau(\"Nuriel Stone\")"
  },
  {
    "objectID": "posts/Hatespeech germeval/germeval.html",
    "href": "posts/Hatespeech germeval/germeval.html",
    "title": "Hatespeech Klassifikation",
    "section": "",
    "text": "Klassifikation von Hatespeech auf Grundlage der Germeval-Daten.\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(syuzhet)\nlibrary(stringr)\nlibrary(slider)\nlibrary(tidytext)\nlibrary(furrr)\nlibrary(widyr)\nlibrary(irlba)\nlibrary(datawizard)\nlibrary(lightgbm)\nlibrary(bonsai)\nlibrary(vip)\ndata(\"schimpfwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(wild_emojis, package = \"pradadata\")\n\n\n\n\nBei den Daten handelt es sich um die Trainings- und Testdaten (deutsche Tweets) aus der GermEval 2018 Shared Task zum Erkennen von beleidigender Sprache.\n\nd_train &lt;- \n  data_read(\"germeval2018.training.txt\",\n         header = FALSE,\n         quote = \"\")\nd_test &lt;- \n  data_read(\"germeval2018.test.txt\",\n         header = FALSE,\n         quote = \"\")\nnames(d_train) &lt;- c(\"text\", \"c1\", \"c2\")\nnames(d_test) &lt;- c(\"text\", \"c1\", \"c2\")\n\n\n\n\n\nZiel ist es, auf Grundlage der Tweets einige nützliche Features zu generieren, die sich als Prädiktor für die AV (Hatespeech oder nicht) eignen.\nDa das Attribut lexicon Funktion get_sentiment ein Dataframe mit mindestens zwei Spalten mit dem Namen “word” und “value” haben muss, füge ich die Spalte “value” zum Schimpfwörterlexikon hinzu, um Wörter als Schimpfwort zu kennzeichnen.\n\nschimpfwoerter$value &lt;- 1\n\n\n\nUm “wilde” Emojis zu kennzeichnen, verwende ich ein Lexikon, das solche Emojis enthält und schreibe eine Funktion, die zählt, wieviele wilde Emojis in einem Tweet vorkommen.\n\ncount_wild_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor für die Zählungen\n  counts &lt;- numeric(length(wild_emojis$emoji))\n\n  # Iteriere über jedes Emoji und zähle die Übereinstimmungen im Text\n  for (i in seq_along(wild_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, wild_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der Übereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"🗑\", \"bogen\", \"😠\", \"👹\", \"💩\", \"baby\", \"und\", \"🆗\")\ncount_wild_emojis(dummy)\n\n[1] 4\n\n\n\n\n\n\nnested_words &lt;- d_train %&gt;%\n  select(text) %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  nest(words = c(word))\n\nSkipgrams identifizieren:\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nPMI berechnen:\n\ntidy_pmi &lt;- nested_words %&gt;%\n  mutate(words = future_map(words, slide_windows, 4L)) %&gt;%\n  unnest(words) %&gt;%\n  pairwise_pmi(word, window_id)\ntidy_pmi\n\n\n\n  \n\n\n\nWortvektoren erstellen:\n\ntidy_word_vectors &lt;- tidy_pmi %&gt;%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100, maxit = 1000\n  )\n\ntidy_word_vectors\n\n\n\n  \n\n\n\n\n\n\n\n\n\nRezept 1 enthält Schimpfwörter, Sentimentwerte, aggressive Emojis und Word Embeddings\n\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) %&gt;% \n  update_role(c2, new_role = \"ignore\") %&gt;%  \n  step_text_normalization(text) %&gt;%\n  step_mutate(schimpfw = get_sentiment(text,\n                                       method = \"custom\",\n                                       lexicon = schimpfwoerter)) %&gt;% \n  step_mutate(senti = get_sentiment(text,\n                                    method = \"custom\",\n                                    lexicon = sentiws)) %&gt;%\n  step_mutate(wild_emojis_n = map_int(text, \n                                      count_wild_emojis)) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text,\n                       embeddings = tidy_word_vectors,\n                       aggregation = \"mean\")\n\nRezept 2 enthält statt Word-Embeddings tfidf.\n\nrec2 &lt;-\n  recipe(c1 ~ ., data = d_train) %&gt;% \n  update_role(c2, new_role = \"ignore\") %&gt;%  \n  step_text_normalization(text) %&gt;%\n  step_mutate(schimpfw = get_sentiment(text,\n                                       method = \"custom\",\n                                       lexicon = schimpfwoerter)) %&gt;% \n  step_mutate(senti = get_sentiment(text,\n                                    method = \"custom\",\n                                    lexicon = sentiws)) %&gt;%\n  step_mutate(wild_emojis_n = map_int(text, \n                                      count_wild_emojis)) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_tfidf(text)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n\n\n  \n\n\n\n\nbaked2 &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n\n\n  \n\n\n\n\n\n\nIch verwende für die Modellierung zum einen einen K-Nearest-Neighbour-Algorithums und zum anderen XGBoost.\n\nknn &lt;- \n  nearest_neighbor(\n  neighbors = tune(),\n  weight_func = tune(),\n  dist_power = tune()\n) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") %&gt;% \n  translate()\n\nWarning: package 'kknn' was built under R version 4.2.3\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb, knn = knn)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1),\n  grid = 5,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 4 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n✔ 1 of 4 tuning:     rec1_xgb (3m 0.8s)\n\n\ni 2 of 4 tuning:     rec1_knn\n\n\n✔ 2 of 4 tuning:     rec1_knn (3m 19.2s)\n\n\ni 3 of 4 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 3 of 4 tuning:     rec2_xgb (4m 35.7s)\n\n\ni 4 of 4 tuning:     rec2_knn\n\n\n✔ 4 of 4 tuning:     rec2_knn (3m 56.9s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n\n\n  \n\n\n\nLightGBM hat deutlich besser abgeschnitten als KNN. Wählen wir nun das beste Modell aus und fitten es:\n\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec1_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec1_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip() \n\n\n\n\n\n\n\n\n\npreds &lt;- predict(fit_final, d_test)\npreds\n\n\n\n  \n\n\n\n\n\n\nd_test &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(c1 = as.factor(c1))\nd_test\n\n\n\n  \n\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)"
  },
  {
    "objectID": "posts/Hatespeech germeval/germeval.html#vorbereitung",
    "href": "posts/Hatespeech germeval/germeval.html#vorbereitung",
    "title": "Hatespeech Klassifikation",
    "section": "",
    "text": "library(tidymodels)\nlibrary(textrecipes)\nlibrary(syuzhet)\nlibrary(stringr)\nlibrary(slider)\nlibrary(tidytext)\nlibrary(furrr)\nlibrary(widyr)\nlibrary(irlba)\nlibrary(datawizard)\nlibrary(lightgbm)\nlibrary(bonsai)\nlibrary(vip)\ndata(\"schimpfwoerter\", package = \"pradadata\")\ndata(\"sentiws\", package = \"pradadata\")\ndata(wild_emojis, package = \"pradadata\")\n\n\n\n\nBei den Daten handelt es sich um die Trainings- und Testdaten (deutsche Tweets) aus der GermEval 2018 Shared Task zum Erkennen von beleidigender Sprache.\n\nd_train &lt;- \n  data_read(\"germeval2018.training.txt\",\n         header = FALSE,\n         quote = \"\")\nd_test &lt;- \n  data_read(\"germeval2018.test.txt\",\n         header = FALSE,\n         quote = \"\")\nnames(d_train) &lt;- c(\"text\", \"c1\", \"c2\")\nnames(d_test) &lt;- c(\"text\", \"c1\", \"c2\")"
  },
  {
    "objectID": "posts/Hatespeech germeval/germeval.html#feature-engineering",
    "href": "posts/Hatespeech germeval/germeval.html#feature-engineering",
    "title": "Hatespeech Klassifikation",
    "section": "",
    "text": "Ziel ist es, auf Grundlage der Tweets einige nützliche Features zu generieren, die sich als Prädiktor für die AV (Hatespeech oder nicht) eignen.\nDa das Attribut lexicon Funktion get_sentiment ein Dataframe mit mindestens zwei Spalten mit dem Namen “word” und “value” haben muss, füge ich die Spalte “value” zum Schimpfwörterlexikon hinzu, um Wörter als Schimpfwort zu kennzeichnen.\n\nschimpfwoerter$value &lt;- 1\n\n\n\nUm “wilde” Emojis zu kennzeichnen, verwende ich ein Lexikon, das solche Emojis enthält und schreibe eine Funktion, die zählt, wieviele wilde Emojis in einem Tweet vorkommen.\n\ncount_wild_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor für die Zählungen\n  counts &lt;- numeric(length(wild_emojis$emoji))\n\n  # Iteriere über jedes Emoji und zähle die Übereinstimmungen im Text\n  for (i in seq_along(wild_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, wild_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der Übereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"🗑\", \"bogen\", \"😠\", \"👹\", \"💩\", \"baby\", \"und\", \"🆗\")\ncount_wild_emojis(dummy)\n\n[1] 4\n\n\n\n\n\n\nnested_words &lt;- d_train %&gt;%\n  select(text) %&gt;% \n  unnest_tokens(word, text) %&gt;%\n  nest(words = c(word))\n\nSkipgrams identifizieren:\n\nslide_windows &lt;- function(tbl, window_size) {\n  skipgrams &lt;- slider::slide(\n    tbl, \n    ~.x, \n    .after = window_size - 1, \n    .step = 1, \n    .complete = TRUE\n  )\n  \n  safe_mutate &lt;- safely(mutate)\n  \n  out &lt;- map2(skipgrams,\n              1:length(skipgrams),\n              ~ safe_mutate(.x, window_id = .y))\n  \n  out %&gt;%\n    transpose() %&gt;%\n    pluck(\"result\") %&gt;%\n    compact() %&gt;%\n    bind_rows()\n}\n\nPMI berechnen:\n\ntidy_pmi &lt;- nested_words %&gt;%\n  mutate(words = future_map(words, slide_windows, 4L)) %&gt;%\n  unnest(words) %&gt;%\n  pairwise_pmi(word, window_id)\ntidy_pmi\n\n\n\n  \n\n\n\nWortvektoren erstellen:\n\ntidy_word_vectors &lt;- tidy_pmi %&gt;%\n  widely_svd(\n    item1, item2, pmi,\n    nv = 100, maxit = 1000\n  )\n\ntidy_word_vectors"
  },
  {
    "objectID": "posts/Hatespeech germeval/germeval.html#modellierung",
    "href": "posts/Hatespeech germeval/germeval.html#modellierung",
    "title": "Hatespeech Klassifikation",
    "section": "",
    "text": "Rezept 1 enthält Schimpfwörter, Sentimentwerte, aggressive Emojis und Word Embeddings\n\nrec1 &lt;-\n  recipe(c1 ~ ., data = d_train) %&gt;% \n  update_role(c2, new_role = \"ignore\") %&gt;%  \n  step_text_normalization(text) %&gt;%\n  step_mutate(schimpfw = get_sentiment(text,\n                                       method = \"custom\",\n                                       lexicon = schimpfwoerter)) %&gt;% \n  step_mutate(senti = get_sentiment(text,\n                                    method = \"custom\",\n                                    lexicon = sentiws)) %&gt;%\n  step_mutate(wild_emojis_n = map_int(text, \n                                      count_wild_emojis)) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_word_embeddings(text,\n                       embeddings = tidy_word_vectors,\n                       aggregation = \"mean\")\n\nRezept 2 enthält statt Word-Embeddings tfidf.\n\nrec2 &lt;-\n  recipe(c1 ~ ., data = d_train) %&gt;% \n  update_role(c2, new_role = \"ignore\") %&gt;%  \n  step_text_normalization(text) %&gt;%\n  step_mutate(schimpfw = get_sentiment(text,\n                                       method = \"custom\",\n                                       lexicon = schimpfwoerter)) %&gt;% \n  step_mutate(senti = get_sentiment(text,\n                                    method = \"custom\",\n                                    lexicon = sentiws)) %&gt;%\n  step_mutate(wild_emojis_n = map_int(text, \n                                      count_wild_emojis)) %&gt;% \n  step_tokenize(text, token = \"words\") %&gt;% \n  step_stem(text) %&gt;% \n  step_tokenfilter(text, max_tokens = 1e2) %&gt;%\n  step_stopwords(text, language = \"de\", stopword_source = \"snowball\") %&gt;% \n  step_tfidf(text)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n\n\n  \n\n\n\n\nbaked2 &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n\n\n  \n\n\n\n\n\n\nIch verwende für die Modellierung zum einen einen K-Nearest-Neighbour-Algorithums und zum anderen XGBoost.\n\nknn &lt;- \n  nearest_neighbor(\n  neighbors = tune(),\n  weight_func = tune(),\n  dist_power = tune()\n) %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\") %&gt;% \n  translate()\n\nWarning: package 'kknn' was built under R version 4.2.3\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb, knn = knn)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1),\n  grid = 5,\n  seed = 42,\n  verbose = TRUE)\n\ni 1 of 4 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n✔ 1 of 4 tuning:     rec1_xgb (3m 0.8s)\n\n\ni 2 of 4 tuning:     rec1_knn\n\n\n✔ 2 of 4 tuning:     rec1_knn (3m 19.2s)\n\n\ni 3 of 4 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 3 of 4 tuning:     rec2_xgb (4m 35.7s)\n\n\ni 4 of 4 tuning:     rec2_knn\n\n\n✔ 4 of 4 tuning:     rec2_knn (3m 56.9s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n\n\n  \n\n\n\nLightGBM hat deutlich besser abgeschnitten als KNN. Wählen wir nun das beste Modell aus und fitten es:\n\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec1_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec1_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()"
  },
  {
    "objectID": "posts/Hatespeech germeval/germeval.html#vorhersagen",
    "href": "posts/Hatespeech germeval/germeval.html#vorhersagen",
    "title": "Hatespeech Klassifikation",
    "section": "",
    "text": "preds &lt;- predict(fit_final, d_test)\npreds\n\n\n\n  \n\n\n\n\n\n\nd_test &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(c1 = as.factor(c1))\nd_test\n\n\n\n  \n\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test,\n           truth = c1,\n           estimate = .pred_class)"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html",
    "title": "Hate Speech auf Reddit",
    "section": "",
    "text": "Ziel dieser Analyse ist es, die Reddit-Threads, deren ursprüngliche Posts das Stichwort “Kanye West” enthalten, in Bezug auf Sentiment, Wortwahl und Hate-Speech zu untersuchen. Hierbei ist vor allem die Veränderung der Sentimente der Posts im Zeitverlauf interessant. Dieser Post soll außerdem der Antwort auf die Frage näherkommen, ob die Nutzung der kostenlosen Reddit-API eine Alternative zur inzwischen kostenpflichtigen Twitter-API darstellt, um Hate Speech auf Social-Media-Plattformen zu analysieren. Der Post lässt sich grob in zwei Teile gliedern, nämlich wird zunächst die Analyse der Posts und Kommentare in Bezug auf Sentimente und Themen vorgenommen, woraufhin die Beiträge auf im zweiten Teil auf Hate Speech geprüft werden.\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(stringr)\nlibrary(RedditExtractoR)\nlibrary(httr)"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#laden-der-posts",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#laden-der-posts",
    "title": "Hate Speech auf Reddit",
    "section": "1.1 Laden der Posts",
    "text": "1.1 Laden der Posts\nDie Posts werden mit Hilfe des Pakets RedditExtractoR, welches die Reddit-API anspricht, extrahiert. Ich lade alle Posts, die das Keyword “Kanye West” enthalten (und innerhalb des Download-Limits sind). Dadurch dass ich das API Download-Limit für heute bereits erreicht habe, importiere ich die Posts und Kommentare als Csv-Datei.\n\nposts &lt;- find_thread_urls(\n  keywords = \"Kanye West\",\n  sort_by=\"top\", \n  period = \"all\"\n  )\nstr(posts)\n\n\nposts &lt;- read_csv(\"posts.csv\")\n\nNew names:\nRows: 228 Columns: 8\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(5): ...1, title, text, subreddit, url dbl (2): timestamp, comments date (1):\ndate_utc\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#laden-der-kommentare",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#laden-der-kommentare",
    "title": "Hate Speech auf Reddit",
    "section": "1.2 Laden der Kommentare",
    "text": "1.2 Laden der Kommentare\nMit der Funktion get_thread_content() ist es möglich, bis zu 500 Kommentare zu einer Post-URL herunterzuladen. Um maximal 500 Kommentare zu allen n Posts herunterzuladen, wende ich eine Schleife an. Die Ausgabe ist dann eine Liste, die zwei Dataframes enthält. Ich extrahiere den Dataframe comments und speichere jeden Dataframe zu jeder URL in einer Liste ab. Manchmal kann es zu Problemen bei der Zusammenführung der Listenelemente zu einem Dataframe kommen, da die Spalte comment_id in manchen Fällen von der Standardfornatierung als Character abweicht und als Integer gespeichert wird. Dieses Problem löse ich, indem ich alle solche Spalten in Character umwandle. Zuletzt entferne ich noch sowohl alle entfernten und gelöschten Posts als auch Links und Zahlen.\n\ncomments_list &lt;- list()\n\nfor (i in 1:nrow(posts)) {\n  comments &lt;- get_thread_content(posts$url[i])$comments\n  comments_list[[i]] &lt;- comments\n}\n\numwandlung_character &lt;- function(df) {\n  if (\"comment_id\" %in% names(df) && is.integer(df$comment_id)) {\n    df$comment_id &lt;- as.character(df$comment_id)\n  }\n  return(df)\n}\n\nliste_dataframes_umgewandelt &lt;- lapply(comments_list, umwandlung_character)\nall_comments &lt;- bind_rows(liste_dataframes_umgewandelt)\nall_comments &lt;- all_comments %&gt;%\n  mutate(comment = str_remove_all(comment, pattern = 'http[s]?://\\\\S+|\\\\d+')) %&gt;%\n  mutate(timestamp = as_datetime(timestamp, origin = \"1970-01-01\")) %&gt;% \n  select(timestamp, comment, comment_id, url)\nall_comments &lt;- all_comments %&gt;%\n  filter(!grepl(\"\\\\[deleted\\\\]|\\\\[removed\\\\]\", comment))\n\n\nall_comments &lt;- read_csv(\"all_comments.csv\")\n\nNew names:\nRows: 98044 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): comment, comment_id, url dbl (2): ...1, timestamp\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n\nIm Folgenden werden die Kommentare und Posts einzeln betrachtet. Dies hat den Grund, dass Reddit-Posts tendenziell mehr aus sachlichen Inhalten bestehen, während die Kommentarsektion inhaltlich freier und dadaurch, dass sich dort noch einmal ganz eigene Themen entspinnen, nicht mit den Posts vergleichbar sind. Hinzu kommt, dass die Anzahl der Kommentare die der Posts um ein Vielfaches übersteigt, sodass diese separat analysiert werden sollten."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#tokenisierung",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#tokenisierung",
    "title": "Hate Speech auf Reddit",
    "section": "1.3 Tokenisierung",
    "text": "1.3 Tokenisierung\nIm Folgenden wandle ich die Posts und Kommentare in Tokens um, entferne alle Links und Zahlen und wandle timestamp in ein Datumsobjekt um. Um alle Text-Inhalte der Posts zu erfassen, vereine ich die Titel der Posts mit den Posts, da es viele Posts gibt, deren Kernaussage im Titel steht, die durch ein Bild verdeutlicht wird.\n\npoststoken &lt;- posts %&gt;%\n  unite(text, c(title, text)) %&gt;%\n  mutate(text = str_remove_all(text, pattern = '\"http[s]?://\\\\S+\"|\\\\d+')) %&gt;%\n  mutate(timestamp = as_datetime(timestamp, origin = \"1970-01-01\")) %&gt;% \n  unnest_tokens(word, text)\nnrow(poststoken)\n\n[1] 22897\n\nlength(unique(poststoken$id))\n\nWarning: Unknown or uninitialised column: `id`.\n\n\n[1] 0\n\n\n\ncommstoken &lt;- all_comments %&gt;%\n  mutate(timestamp = as_datetime(timestamp, origin = \"1970-01-01\")) %&gt;% \n  unnest_tokens(word, comment)"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#entfernung-der-stopwords",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#entfernung-der-stopwords",
    "title": "Hate Speech auf Reddit",
    "section": "1.4 Entfernung der Stopwords",
    "text": "1.4 Entfernung der Stopwords\nNun werden alle Stopwords entfernt. Ich gehe davon aus, dass die Kommentare und Posts fast ausschließlich englischer Sprache sind. Für den Fall, dass sich im Datensatz jedoch sowohl deutsch- als auch englischsprachige Posts finden, kombiniere ich zwei Stopwords-Lexika (deutsch und englisch) zu einem.\n\ndata(stopwords_de, package = \"lsa\")\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\nstopwords_de &lt;- tibble(word = stopwords_de)\nstopwords &lt;- bind_rows(stopwords_de, stopwords_en)\n\n\npoststoken &lt;- poststoken %&gt;%\n  anti_join(stopwords)\n\nJoining with `by = join_by(word)`\n\nnrow(poststoken)\n\n[1] 12280\n\nlength(unique(poststoken$id))\n\nWarning: Unknown or uninitialised column: `id`.\n\n\n[1] 0\n\n\n\ncommstoken &lt;- commstoken %&gt;%\n  anti_join(stopwords)\n\nJoining with `by = join_by(word)`"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#sentimentanalyse",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#sentimentanalyse",
    "title": "Hate Speech auf Reddit",
    "section": "1.5 Sentimentanalyse",
    "text": "1.5 Sentimentanalyse\nFür die Sentimentanalyse gilt dasselbe wie für das Stopwords-Lexikon: Ich möchte, dass es zweisprachig ist. Die gewählten Lexika afinn und sentiws haben jedoch nicht dieselbe Skalierung, weshalb ich noch eine Min-Max-Normalisierung vornehme, um eine Vergleichbarkeit der deutschen und englischen Sentimentwerte zu gewährleisten.\n\nsenti_en &lt;- get_sentiments(\"afinn\") %&gt;% \n  mutate(neg_pos = case_when(value &gt; 0 ~ \"pos\",\n                             TRUE ~ \"neg\"))\n\ndata(sentiws, package = \"pradadata\")\nsentiws &lt;- sentiws %&gt;% \n  select(word, value, neg_pos)\n\nmin_max_normalize &lt;- function(x, old_min, old_max, new_min, new_max) {\n  return (((x - old_min) / (old_max - old_min) * (new_max - new_min)) + new_min)\n}\n\nsenti_en &lt;- senti_en %&gt;% \n  mutate(value = min_max_normalize(value, -5, 5, -1, 1))\n\nsenti &lt;- bind_rows(sentiws, senti_en)\n\n\npoststoken_senti &lt;- poststoken %&gt;%\ninner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., senti): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 88 of `x` matches multiple rows in `y`.\nℹ Row 4591 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nnrow(poststoken_senti)\n\n[1] 1269\n\nlength(unique(poststoken_senti$timestamp))\n\n[1] 124\n\n\n\ncommstoken_senti &lt;- commstoken %&gt;% \n  inner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., senti): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 778 of `x` matches multiple rows in `y`.\nℹ Row 5008 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#berechnung-des-sentiments-pro-jahr",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#berechnung-des-sentiments-pro-jahr",
    "title": "Hate Speech auf Reddit",
    "section": "1.6 Berechnung des Sentiments pro Jahr",
    "text": "1.6 Berechnung des Sentiments pro Jahr\nZuletzt werden noch die Sentimentwerte aller Posts in einem Jahr zusammengefasst und mit der Anzahl der Posts gewichtet.\n\npostssenti &lt;- poststoken_senti %&gt;% \n  mutate(year = year(timestamp)) %&gt;%\n  group_by(year) %&gt;%\n  summarize(value = mean(value),\n            postcount = length(unique((timestamp)))) %&gt;% \n  mutate(sentiment = (value * postcount) / sum(postcount))\n\nnrow(postssenti)\n\n[1] 9\n\n\n\ncommssenti &lt;- commstoken_senti %&gt;%\n  mutate(year = year(timestamp)) %&gt;% \n  group_by(year) %&gt;% \n  summarize(value = mean(value),\n            postcount = length(unique((timestamp)))) %&gt;% \n  mutate(sentiment = (value * postcount) / sum(postcount))\nnrow(commssenti)\n\n[1] 10\n\n\n\npostssenti %&gt;% \n  ggplot(aes(year, sentiment)) + \n  geom_line(linewidth = 1, colour = \"#6fb899\") +\n  labs(title = \"Sentimentwerte der Posts im Zeitverlauf\",\n       x = \"Jahr\",\n       y = \"Sentiment\") +\n  theme_minimal()\n\n\n\n\n\ncommssenti %&gt;% \n  ggplot(aes(year, sentiment)) + \n  geom_line(linewidth = 1, colour = \"#6fb899\") +\n  scale_x_continuous(breaks = 2013:2024)+\n  labs(title = \"Sentimentwerte der Kommentare im Zeitverlauf\",\n       x = \"Jahr\",\n       y = \"Sentiment\") +\n  theme_minimal()\n\n\n\n\nDie Sentimentwerte im Zeitverlauf sind überaus interessant. Auch wenn der Sentimentwert um den Nullbereich liegt, denke ich dennoch, dass er eine gewisse Aussagekraft hat. Gerade wenn er im Jahr 2022 sowohl bei den Posts als auch bei den Kommentaren rapide einbricht, um seinen absoluten Tiefpunkt zu erreichen, sollte man genauer hinsehen. Im Jahr 2022 stand es um die öffentliche Meinung zu Kanye West so schlecht wie noch nie zuvor. Durch ständige antisemitische Äußerungen, das Posten eines Hakenkreuzes und Tragen eines “White Lifes Matter”-Hoodies erreichte er die Sperrung seiner Social Media Accounts, die Kündigung der Kooperation mit Adidas und die Auslösung eines Shit-Storms in den sozialen Medien. Dieser Zusammenhang könnte obige Diagramme erklären."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#analyse-der-sentimentwerte-und--counts",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#analyse-der-sentimentwerte-und--counts",
    "title": "Hate Speech auf Reddit",
    "section": "1.7 Analyse der Sentimentwerte und -counts",
    "text": "1.7 Analyse der Sentimentwerte und -counts\n\npoststoken_senti %&gt;%\n  count(word, neg_pos, sort = TRUE) %&gt;%\n  ungroup() %&gt;%\n  group_by(neg_pos) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word, fill = neg_pos)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~neg_pos, scales = \"free_y\") +\n  labs(x = \"Häufigkeit\",\n       y = \"Wort\") +\n  theme_minimal() +\n  scale_fill_tableau(palette = \"Nuriel Stone\")\n\n\n\n\nDie häufigsten positiven Wörter sind insofern interessant, als sie den Themen des christlichen Glaubens widerspiegeln. Denn Kanye West bekennt sich regelmäßig in Interviews und Tweets zum Christentum. Inwiefern seine antisemitschen Äußerungen mit diesem Glauben vereinbar sind, sei mal dahingestellt.\n\ncomms_bigram &lt;- \n  all_comments %&gt;%\n  unnest_tokens(bigram, comment, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ncomms_bigram &lt;- comms_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ncomms_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10)%&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram)) +\n  geom_col(fill = \"#8175aa\") +\n  labs(title = \"Bigramme nach Häufigkeit\",\n       x = \"Häufigkeit\",\n       y = \"Bigram\") +\n  theme_minimal()\n\n\n\n\nInteressant ist bei der Betrachtung der häufigsten Bigramme, die in den Kommentaren vorkommen, wie präsent das Thema der psychischen Störungen ist. Die Mutmaßung, dass in den Kommentaren über Kanye Wests mentale Gesundheit diskutiert wird, ist nicht unplausibel."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#themenanalyse",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#themenanalyse",
    "title": "Hate Speech auf Reddit",
    "section": "1.8 Themenanalyse",
    "text": "1.8 Themenanalyse\n\nposts_dtm2 &lt;- commstoken_senti %&gt;% \n  mutate(timestamp = as_datetime(timestamp),\n         year = year(timestamp)) %&gt;% \n  filter(year == 2022) %&gt;%\n  select(word) %&gt;% \n  DocumentTermMatrix(commstoken_senti)\n\nWarning: Unknown or uninitialised column: `tokenize`.\n\n\nWarning: Unknown or uninitialised column: `tolower`.\n\n\nWarning: Unknown or uninitialised column: `stopwords`.\nUnknown or uninitialised column: `stopwords`.\n\n\nWarning: Unknown or uninitialised column: `dictionary`.\n\n\nWarning: Unknown or uninitialised column: `bounds`.\n\n\nWarning: Unknown or uninitialised column: `wordLengths`.\n\n\nWarning: Unknown or uninitialised column: `removePunctuation`.\n\n\nWarning: Unknown or uninitialised column: `removeNumbers`.\n\n\nWarning: Unknown or uninitialised column: `stemming`.\n\n\nWarning: Unknown or uninitialised column: `bounds`.\n\n\nWarning: Unknown or uninitialised column: `weighting`.\n\nposts_lda2 &lt;- LDA(posts_dtm2, k = 4, control = list(seed = 42))\n\nposts_themen2 &lt;- tidy(posts_lda2, matrix = \"beta\")\n\nposts_themen2 &lt;- posts_themen2 %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 7) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nposts_themen2 %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(title = \"Kommentarthemen in 2022 \") +\n  theme_minimal() +\n  scale_fill_tableau(\"Nuriel Stone\")\n\n\n\n\nDie Analyse der Kommentarthemen des Jahres 2022 deuten ebenfalls auf Kanye Wests Äußerungen hin, da im ersten Thema dem Wort “hitler” eine gewisse Bedeutung zugeordnet wird."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#vorbereitung",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#vorbereitung",
    "title": "Hate Speech auf Reddit",
    "section": "2.1 Vorbereitung",
    "text": "2.1 Vorbereitung\nZunächst lade ich alle erforderlichen Module sowie das Modell. Der Code zur Verwendung des pipeline-Befehls wird hilfreicherweise von Huggingface bereitgestellt.\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.2.3\n\nuse_virtualenv(\"C:/Users/rapha/venv\")\n\n\nimport pandas as pd\nimport tensorflow as tf\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n\nfrom transformers import pipeline\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training."
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#anwendung",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#anwendung",
    "title": "Hate Speech auf Reddit",
    "section": "2.2 Anwendung",
    "text": "2.2 Anwendung\nFür die Klassifikation von Hate-Speech ziehe ich aus allen Kommentaren eine zufällige Stichprobe, da RoBERTa nur eine Maximallänge von 513 Inputs akzeptiert.\n\nset.seed(123)\ncomments_sample &lt;- all_comments %&gt;%\n  sample_n(size = 513, replace = FALSE)\n\ncomments_short &lt;- comments_sample$comment\n\nNun lade ich die Kommentare der Stichprobe in das Modell.\n\ncomments = r.comments_short\nresult = classifier(comments)\n\nAnschließend füge ich der Stichprobe das Ergebnis der Klassifikation als neue Spalte hinzu.\n\nresult &lt;- py$result\nlabels &lt;- lapply(result, function(element) element$label)\ncomments_hate &lt;- cbind(comments_sample, hatespeech = unlist(labels)) %&gt;%\n  select(timestamp, comment, hatespeech, comment_id, url)"
  },
  {
    "objectID": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#analyse",
    "href": "posts/Hatespeech_Reddit/Hatespeech_Reddit.html#analyse",
    "title": "Hate Speech auf Reddit",
    "section": "2.3 Analyse",
    "text": "2.3 Analyse\nIm Folgenden untersuche ich die als Hate Speech klassifizierten Kommentare etwas näher, indem ich den Anteil der Hate Speech im Jahresverlauf sowie den Zusammenhang mit negativem Sentiment betrachte.\n\ncomments_hate %&gt;%\n   count(hatespeech == \"hate\")\n\n\n\n  \n\n\n\n\ncomments_hate %&gt;%\n  mutate(timestamp = as_datetime(timestamp, origin = \"1970-01-01\"),\n         year = year(timestamp)) %&gt;%\n  group_by(year, hatespeech) %&gt;%\n  count() %&gt;%\n  ggplot(aes(x = year, y = n, fill = hatespeech)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Hatespeech-Kommentare im Zeitverlauf\",\n       x = \"Jahr\",\n       y = \"Anzahl\",\n       fill = \"Hate Speech\") +\n  scale_fill_tableau(\"Nuriel Stone\") +\n  theme_minimal()\n\n\n\n\nAuch wenn nur ein winziger Teil der Kommentare als Hate Speech klassifiziert wurde, ist in diesem Diagramm erkennbar, dass wieder im Jahr 2022 mit Abstand die meiste Hassrede auftrat.\n\ncommstokensenti_hate &lt;- comments_hate %&gt;% \n  filter(hatespeech == \"hate\") %&gt;% \n  unnest_tokens(word, comment) %&gt;% \n  inner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\nWarning in inner_join(., senti): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 541 of `x` matches multiple rows in `y`.\nℹ Row 4906 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ncommstokensenti_hate %&gt;% \n  summarise(mean(neg_pos == \"neg\"))\n\n\n\n  \n\n\n\n\ncomments_hate %&gt;% \n  filter(hatespeech == \"hate\") %&gt;% \n  select(comment)\n\n\n\n  \n\n\n\nNur die Hälfte der als Hassrede klassifizierten Kommentare haben ein negatives Sentiment. Außerdem halte ich persönlich Sätze wie “You are an old one.” nicht für Hate Speech. In Kombination mit dem Fakt, dass nur 34 von 513 Tweets als Hate Speech erkannt wurden, lassen mich diese Ergebnisse vermuten, dass das Modell nicht allzu zuverlässige Vorhersagen macht."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel dieses Posts ist es, Hate Speech auf Twitter zu klassifizieren. Hass im Internet ist nach wie vor ein großes gesellschaftliches Problem, weshalb es sich lohnt, genauer zu untersuchen, was diesen Hass ausmacht und wie man ihn zuverlässig und automatisiert erkennen kann. Hierfür liegt ein Datensatz vor, der eine Auswahl englischer als Hate Speech oder nicht Hate Speech markierter Tweets enthält. Die Analyse dieser Daten lässt sich in zwei Teile gliedern: Zunächst werden Methoden der explorativen Datenanalyse angewandt, um Muster und Auffälligkeiten in den Tweets zu identifizieren. Anschließend werden die gewonnenen Erkenntnisse genutzt, um sowohl Shallow-Learning- als auch Deep-Learning-Algorithmen darauf zu trainieren, Tweets korrekterweise als Hate Speech einzuordnen.\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidytext)\nlibrary(syuzhet)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(stringr)\nlibrary(readr)\nlibrary(vip)\n\n\n\n\nBei den Daten handelt es sich um eine Auswahl englischer Tweets, die bereits auf Hate Speech untersucht wurden und sich daher gut für das Training von Modellen zur Erkennung von Hassrede eignen.\n\nd_hate &lt;- read_csv(\"d_hate.csv\")"
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#einleitung",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#einleitung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel dieses Posts ist es, Hate Speech auf Twitter zu klassifizieren. Hass im Internet ist nach wie vor ein großes gesellschaftliches Problem, weshalb es sich lohnt, genauer zu untersuchen, was diesen Hass ausmacht und wie man ihn zuverlässig und automatisiert erkennen kann. Hierfür liegt ein Datensatz vor, der eine Auswahl englischer als Hate Speech oder nicht Hate Speech markierter Tweets enthält. Die Analyse dieser Daten lässt sich in zwei Teile gliedern: Zunächst werden Methoden der explorativen Datenanalyse angewandt, um Muster und Auffälligkeiten in den Tweets zu identifizieren. Anschließend werden die gewonnenen Erkenntnisse genutzt, um sowohl Shallow-Learning- als auch Deep-Learning-Algorithmen darauf zu trainieren, Tweets korrekterweise als Hate Speech einzuordnen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#vorbereitung",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#vorbereitung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "library(tidymodels)\nlibrary(textrecipes)\nlibrary(tokenizers)\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(tidytext)\nlibrary(syuzhet)\nlibrary(textdata)\nlibrary(ggthemes)\nlibrary(topicmodels)\nlibrary(tm)\nlibrary(stringr)\nlibrary(readr)\nlibrary(vip)\n\n\n\n\nBei den Daten handelt es sich um eine Auswahl englischer Tweets, die bereits auf Hate Speech untersucht wurden und sich daher gut für das Training von Modellen zur Erkennung von Hassrede eignen.\n\nd_hate &lt;- read_csv(\"d_hate.csv\")"
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#explorative-datenanalyse",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#explorative-datenanalyse",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Ziel ist es, auf Grundlage der Tweets einige nützliche Features zu generieren, die sich als Prädiktor für die AV (Hatespeech oder nicht) eignen. Hierfür müssen zunächst einige Charakteristika von Tweets, die als Hate Speech gelten, herausgearbeitet werden. Die Methoden und der Code orientieren sich stark an dem Vorgehen, das in Julia Silges und David Robinsons Buch “Text Mining with R” (https://www.tidytextmining.com/) beschrieben wird. Für die nachfolgenden Visualisierungen wird eine mit Hilfe der Seite https://davidmathlogic.com/colorblind eigens erstellte Farbpalette verwendet, die gewährleistet, dass keine Art der Farbenblindheit die Lesbarkeit der Diagramme beeinträchtigt.\n\nUriah_Flint &lt;- c(\"#8175AA\", \"#6FB899\", \"#3AA2C3\", \"#8BD4F9\", \"#DDCC77\", \"#CC6677\", \"#882255\")\n\n\n\nUm eine sinnvolle Analyse durchzuführen, müssen noch einige Datenvorverarbeitungsschritte durchlaufen werden. Diese beinhalten die Tokenisierung, das Entfernen von Stopwords und das Bereinigen der Tweets, die Links oder ähnliche Elemente enthalten.\n\nd_hate_clean &lt;- d_hate %&gt;%\n  mutate(tweet = str_remove_all(tweet, pattern = 'RT\\\\s*|http[s]?://\\\\S+|\\\\d+'))\n\nset.seed(123)\ntrain_test_split &lt;- initial_split(d_hate_clean, prop = .8, strata = class)\nd_train &lt;- training(train_test_split)\nd_test &lt;- testing(train_test_split)\n\n\n\n\ntweets_token &lt;- d_train %&gt;%\n  unnest_tokens(word, tweet)\n\n\n\n\n\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\n\ntweets_token &lt;- tweets_token %&gt;%\n  anti_join(stopwords_en)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nsenti &lt;- get_sentiments(\"afinn\") %&gt;% \n  mutate(neg_pos = case_when(value &gt; 0 ~ \"pos\",\n                             TRUE ~ \"neg\"))\n\ntweets_senti &lt;- tweets_token %&gt;%\ninner_join(senti)\n\nJoining with `by = join_by(word)`\n\n\n\n\n\n\nUm sich einen ersten Überblick über die Daten zu verschaffen, ist es sinnvoll, zunächst einmal den Anteil der als Hate Speech markierten Tweets zu überprüfen.\n\ntweets_token %&gt;% \n  summarise(`Anteil Hate Speech` = mean(class == \"hate speech\")) %&gt;% \n  round(2)\n\n\n\n  \n\n\n\n\nclass_totals &lt;- tweets_token %&gt;%\n  count(class, name = \"class_total\")\n\nggplot(class_totals, aes(x = \"\", y = class_total, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil an Hate Speech\",\n       x = NULL,\n       y = NULL,\n       fill = \"Klasse\") +\n  geom_text(aes(label = class_total), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nDer Anteil der Hate Speech in diesem Datensatz beträgt 25 Prozent. Die Tweets anderer Kategorien sind also deutlich in der Mehrheit.\n\n\n\nEinen weiteren interessanten Einblick gewähren die Worthäufigkeiten. Durch die Visualisierung der am meisten verwendeten Wörter und Wortpaare ist es bereits möglich, einen Einblick in das Vokabular zu erhalten und dieses unter den Klassen zu vergleichen.\n\ntweets_count_senti &lt;- tweets_senti %&gt;%\n  group_by(class) %&gt;% \n  count(class, word, sort = TRUE) %&gt;% \n  slice_head(n = 10)\n\nword_counts &lt;- left_join(tweets_count_senti, class_totals, by = \"class\")\n\n# Berechnung der gewichteten Häufigkeit\nword_counts &lt;- word_counts %&gt;%\n  mutate(weighted_frequency = n / class_total)\n\n# Visualisierung der gewichteten Häufigkeiten\nggplot(word_counts, aes(x = reorder(word, weighted_frequency), y = weighted_frequency, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip() +\n  labs(title = \"Gewichtete Häufigkeiten der Wörter in Abhängigkeit von der Klasse\",\n       x = \"Wort\",\n       y = \"Gewichtete Häufigkeit\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nBeim Vergleich der häufigsten Wörter fällt direkt auf, dass Beleidigungen und Schimpfwörter charakteristisch für Hate Speech sind, da die Liste der zehn häufigsten Wörter fast nur aus solchen Einträgen besteht. Das Vokabular der anderen Kategorie ist im Vergleich dazu überaus harmlos. Diese Harmlosigkeit wird durch das häufigste Wort “lol” noch auf die Spitze getrieben. Interessant ist jedoch auch, dass sich das Wort “hate” in dieser Liste wiederfindet. Hier wäre es interessant, im weiteren Verlauf der Analyse den Kontext in Erfahrung zu bringen. Auf der anderen Seite ist “hate” jedoch ein sehr gängiges Wort und dient zur Beschreibung normaler Gefühlszustände, ohne direkt Hass zu verbreiten.\n\ntweets_bigram &lt;- \n  d_train %&gt;%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ntweets_bigram &lt;- tweets_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntweets_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  group_by(class) %&gt;% \n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram, fill = class) ) +\n  facet_wrap(~class, scales = \"free_y\") +\n  geom_col() +\n  labs(title = \"Bigramme nach Häufigkeit\",\n       x = \"Häufigkeit\",\n       y = \"Bigramm\") +\n  scale_fill_manual(values = Uriah_Flint) +\n  theme_light()\n\n\n\n\nDie Analyse der häufigsten Wortpaare deckt sich mit der Analyse der häufigsten Wörter. Sie bringt insofern neue Erkenntnisse, als deutlich wird, dass sich der Hass hauptsächlich gegen ethnische und sexuelle Minderheiten richtet. Dies wird anhand von Begriffen wie “white trash” und “fucking faggot” deutlich. Bemerkenswert ist ebenfalls, dass es sich keinesfalls hauptsächlich um Hass gegen Schwarze handelt, sondern genauso auch Menschen mit heller Hautfarbe ethnisch beleidigt werden.\n\n\n\nIm Folgenden werden alle Wortpaare, die häufiger als sechs Mal vorkommen, visualisiert. Hierdurch werden die Kontexte der Wörter deutlicher und die Beziehungen können uns Aufschluss darüber geben, in welchem Zusammenhang “hate” verwendet wird.\n\ntweets_bigram_count &lt;- tweets_bigram %&gt;% \n   count(word1, word2, sort = TRUE)\n\nvisualize_bigrams &lt;- function(bigrams) {\n  set.seed(2016)\n  a &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n  \n  bigrams %&gt;%\n    graph_from_data_frame() %&gt;%\n    ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +\n    geom_node_point(color = \"#6FB899\", size = 5) +\n    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n    theme_void()\n}\n\ntweets_bigram_count %&gt;%\n  filter(n &gt; 6,\n         !str_detect(word1, \"\\\\d\"),\n         !str_detect(word2, \"\\\\d\")) %&gt;%\n  visualize_bigrams()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nZum Kontext des Wortes “hate” erhalten wir hier keine neuen Hinweise. Jedoch wird klar ersichtlich, in welchen Kombinationen Schimpfwörter verwendet werden. Außerdem werden Ambiguitäten deutlich, da Wörter wie “trash” und “colored” sowohl als rassistische Beleidigung als auch als Beschreibung von Alltagsgegenständen auftauchen.\n\n\n\nZweck der Sentimentanalyse ist es, herauszufinden, ob die Sentimentausprägungen die beiden Klassen klar voneinander abgrenzen.\n\n# Zählen der negativen und positiven Sentimente\ntweets_senti2 &lt;- tweets_senti %&gt;% \n  group_by(class) %&gt;% \n  count(neg_pos, name = \"count\")\n\n# Visualisierung der Sentimentantanteile nach Klasse\nggplot(tweets_senti2, aes(x = \"\", y = count, fill = neg_pos)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sentimentanteile nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Sentiment\") +\n  facet_wrap(~ class) +\n  geom_text(aes(label = count), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nIn obigem Diagramm wird ersichtlich, dass hasslastige Tweets überwiegend negativ sind, während sich die Sentimente der anderen Klasse in der Waage halten. Das Sentiment ist also ein entscheidender Faktor bei der Klassifizierung von Hate Speech und sollte beim Training des Modells berücksichtigt werden.\n\n\n\nDie Themenanalyse soll Aufschluss darüber geben, ob es bestimmte Themengebiete gibt, die charakteristisch für Hate Speech sind.\n\ntweets_token_counts_hate &lt;- tweets_token %&gt;%\n  filter(class == \"hate speech\") %&gt;% \n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 19) %&gt;% \n  select(word)\n\ntweets_dtm_hate &lt;- DocumentTermMatrix(tweets_token_counts_hate)\ntweets_dtm_hate\n\n\ntweets_lda_hate &lt;- LDA(tweets_dtm_hate, k = 4, control = list(seed = 42))\n\ntweets_themen_hate &lt;- tidy(tweets_lda_hate, matrix = \"beta\")\n\ntweets_themen_hate &lt;- tweets_themen_hate %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 7) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntweets_themen_hate %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(title = \"Themen von Hate Speech\") +\n  theme_minimal() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nObwohl sich die Themen nicht eindeutig voneinander abgrenzen, sind dennoch schwache Muster erkennbar. Thema Eins scheint sich vor allem aus allgemeinen Obszönitäten zusammenzusetzen, während das zweite Thema aus Beleidigungen gegen Schwule und Schwarze und etwas härteren Wörtern wie “kill” und “shit” besteht. In Thema Drei und Vier treten Beleidigungen gegen Frauen sowie die LGBTQ-Community in den Vordergrund. Viel wichtiger als diese kleinen Unterschiede ist jedoch das große Bild der Themen, welches wie schon bei der Analyse der Worthäufigkeiten festgestellt, hauptsächlich aus ethnischen und sexuellen Beleidigungen und Schimpfwörtern besteht.\n\n\n\nSchimpfwörter scheinen eine große Rolle bei Hate Speech zu spielen. Deshalb erachte ich es als sinnvoll, Schimpfwörter als Feature in das spätere Rezept mit aufzunehmen. Hierzu verwende ich diese Liste, welche ich um ein paar Einträge (rassistische Beleidigungen) ergänzt habe: https://www.insult.wiki/list-of-insults.\n\ninsults &lt;- read.csv(\"insults.csv\")\n\ntweets_token %&gt;%\n  group_by(class) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(insults, by = \"word\") %&gt;% \n  mutate(insult = case_when(is.na(value) == TRUE ~ \"Nein\",\n                            TRUE ~ \"Ja\")) %&gt;% \n  select(-value) %&gt;% \nggplot(aes(x = \"\", y = n, fill = insult)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil der Beleidigungen nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Beleidigung\") +\n  facet_wrap(~ class, scales = \"free_y\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nTatsächlich ist der Anteil der Beleidigungen in Hate Speech Tweets höher, jedoch fällt er deutlich geringer aus als erwartet.\n\n\n\nDie Überlegung, dass Hate Speech feindselige Emojis enthält, ist sehr plausibel. Um aggressive Emojis zu kennzeichnen, verwende ich ein von mir erstelltes Lexikon, das solche Emojis enthält und schreibe eine Funktion, die zählt, wieviele feindselige Emojis in einem Tweet vorkommen. Der Totenkopf ist nicht in der Liste der feindseligen Emojis enthalten, da dieser hauptsächlich als Synonym oder Steigerung des Lach-Emojis verwendet wird (engl.: “That’s too funny. I’m dead!”).\n\nhostile_emojis &lt;- read.csv(\"hostile_emojis.csv\")\n\n\ncount_hostile_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor für die Zählungen\n  counts &lt;- numeric(length(hostile_emojis$emoji))\n\n  # Iteriere über jedes Emoji und zähle die Übereinstimmungen im Text\n  for (i in seq_along(hostile_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, hostile_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der Übereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"🗑\", \"bogen\", \"😠\", \"👹\", \"💩\", \"baby\", \"und\", \"🆗\")\ncount_hostile_emojis(dummy)\n\n[1] 3\n\n\n\nd_train %&gt;% \n  mutate(hostile_emojis_n = map_int(tweet, count_hostile_emojis)) %&gt;% \n  summarise(`Feindselige Emojis` = mean(hostile_emojis_n == 1))\n\n\n\n  \n\n\n\nDie Vermutung, dass Hate Speech feindselige Emojis enthält, stellt sich in diesem Fall als falsch heraus. Da es keinen einzigen Emoji dieser Art gibt, wird dieser Ansatz für die Modellierung verworfen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#modellierung",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#modellierung",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "In der Modellierung ist es nun das Ziel, einen Algorithmus darauf zu trainieren, möglichst präzise Hate Speech vorherzusagen. Der Algorithmus meiner Wahl ist der XGBoost. Zunächst werden jedoch noch Rezepte formuliert, die die Erkenntnisse aus der Analyse nun in nützliche Features umwandeln.\n\n\nRezept Eins enthält Schimpfwörter, Sentimentwerte und die Themenanalyse. Außerdem werden noch die üblichen Textverarbeitungsschritte durchgeführt sowie ein Tokenfilter angewandt.\n\nrec1 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;%\n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_lda(tweet, num_topics = 6)\n\nRezept Zwei enthält statt der Themenanalyse die Tf-idf-Maße.\n\nrec2 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_tfidf(tweet)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n\n\n  \n\n\n\n\nbaked2 &lt;- rec2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n\n\n  \n\n\n\n\n\n\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n\nDas Modell wird getuned. Hierfür wird zweifache Kreuzvalidierung mit einer Wiederholung verwendet. Der geringe Performance-Zuwachs durch intensiveres Tuning mit mehr Folds und Wiederholungen würde in diesem Fall nicht die höhere Rechenzeit rechtfertigen.\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n\ni 1 of 2 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n✔ 1 of 2 tuning:     rec1_xgb (1m 22.2s)\n\n\ni 2 of 2 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 2 tuning:     rec2_xgb (1m 48s)\n\n\n\n\n\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n\n\n  \n\n\n\nRezept Zwei hat besser abgeschnitten als Rezept Eins. Wählen wir nun das beste Modell aus und fitten es:\n\n\n\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec2_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec2_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\nDie Analyse der wichtigsten Prädiktoren deckt sich mit den Erkenntnissen aus der EDA. Die mit Abstand wichtigsten Features sind die Beleidigungen und Sentimentwerte, während die Tf-idf-Maße von Beleidigungen ebenfalls viel zur Prediction beitragen.\n\nwf_preds &lt;-\n  collect_predictions(model_set)\n\nwf_preds %&gt;%\n  group_by(wflow_id) %&gt;% \n  roc_curve(truth = class, `.pred_hate speech`) %&gt;% \n  autoplot()\n\n\n\n\nDie Performance im Train-Sample fällt sehr gut aus, da die Vorhersagen mit einer Genauigkeit von rund 90 Prozent sehr präzise sind.\n\n\n\n\npreds &lt;- predict(fit_final, d_test)\npreds\n\n\n\n  \n\n\n\n\n\n\n\nd_test1 &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(class = as.factor(class))\nd_test1\n\n\n\n  \n\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test1,\n           truth = class,\n           estimate = .pred_class)\n\n\n\n  \n\n\n\nAuch im Test-Sample bewährt sich das Modell mit einer sehr hohen Genauigkeit.\n\n\n\n\nEin weiterer Ansatz zur Klassifikation von Hate Speech ist es, kein eigenes Modell zu trainieren, sondern Zero-Shot-Learning anzuwenden. Das ergibt natürlich am meisten Sinn mit einem sehr fortgeschrittenen und komplexen Transformer-Modell, das bereits auf die Erkennung von Hate Speech trainiert wurde. Im Folgenden wird daher das Modell roberta-hate-speech-dynabench-r4-target von Facebook, welches auf Huggingface.co verfügbar ist, um die Tweets nach Hate Speech zu klassifizieren. Hierzu wird der Befehl pipeline aus der transformers-Library von Huggingface genutzt, um das Modell zu laden und auf das Test-Sample anzuwenden.\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.2.3\n\n\n\nuse_virtualenv(\"C:/Users/rapha/venv\")\n\n\nfrom transformers import pipeline\nimport tensorflow as tf\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n\n\n\ntweets &lt;- d_test$tweet\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult &lt;- py$results\nlabels &lt;- lapply(result, function(element) element$label)\ntweets_hate &lt;- cbind(d_test, pred = unlist(labels))\ntweets_hate &lt;- tweets_hate %&gt;% \n  mutate(class = as.factor(class),\n         pred = case_when(pred == \"hate\" ~ \"hate speech\",\n            pred == \"nothate\" ~ \"other\"),\n         pred = as.factor(pred))\n\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(tweets_hate,\n           truth = class,\n           estimate = pred)\n\n\n\n  \n\n\n\nDie Performance des Modells ist objektiv gesehen gut, verglichen mit dem XGBoost mit einer Minute Trainingszeit fällt sie jedoch mager aus.\n\n\n\nBisher wurde Hate Speech sowohl mit Hilfe eines auf den konkreten Daten trainierten Shallow-Learner als auch mit Hilfe eines vortrainierten Transformers klassifiziert. Im letzten Schritt dieses Posts sollen die Stärken dieser beiden Ansätze kombiniert werden, indem ein Deep-Learning-Algorithums, genauer gesagt ein Neuronales Netzwerk, auf den vorliegenden Daten trainiert wird. Das neuronale Netz verwendet ein vortrainiertes Wort-Einbettungsmodell mit 50 Dimensionen, das für die deutsche Sprache optimiert ist. Dieses Embedding-Modell ermöglicht es dem Netzwerk, semantische Repräsentationen der Wörter zu erlernen. Das Netzwerk besteht aus einer Eingabeschicht, die das Embedding-Modell enthält, gefolgt von einer vollständig verbundenen Schicht mit 32 Neuronen und einer Sigmoid-Aktivierungsfunktion. Weiterhin gibt es eine Schicht mit 24 Neuronen und einer ReLU-Aktivierung. Die Ausgabeschicht besteht aus einem einzelnen Neuron für binäre Klassifikation. Das Netzwerk wird mit dem Adam-Optimizer kompiliert und die binäre Kreuzentropie wird als Verlustfunktion verwendet. Die Accuracy wird als Metrik überwacht. Das Training erfolgt über 3 Epochen mit einer Batch-Größe von 48, wobei die Validierung anhand des Test-Samples durchgeführt wird.\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n\nd_train = r.d_train\nd_test = r.d_test\n\nX_train = d_train[\"tweet\"].values\nX_test = d_test[\"tweet\"].values\n\n\nd_train[\"y\"] = d_train[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_train = d_train.loc[:, \"y\"].values\n\nd_test[\"y\"] = d_test[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_test = d_test.loc[:, \"y\"].values\n\n\nembedding = \"https://tfhub.dev/google/nnlm-de-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\ntf.random.set_seed(42)\n\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(32, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(24, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n\n\nmodel.fit(X_train, y_train,\nepochs=3,\nbatch_size=48,\nvalidation_data=(X_test, y_test),\nverbose = 1)\n\nEpoch 1/3\n\n 1/94 [..............................] - ETA: 5:29 - loss: 0.7612 - accuracy: 0.7500\n 2/94 [..............................] - ETA: 1:14 - loss: 0.7540 - accuracy: 0.7604\n 3/94 [..............................] - ETA: 1:15 - loss: 0.7420 - accuracy: 0.7500\n 4/94 [&gt;.............................] - ETA: 58s - loss: 0.7310 - accuracy: 0.7240 \n 5/94 [&gt;.............................] - ETA: 49s - loss: 0.7212 - accuracy: 0.7333\n 6/94 [&gt;.............................] - ETA: 43s - loss: 0.7104 - accuracy: 0.7604\n 7/94 [=&gt;............................] - ETA: 39s - loss: 0.7030 - accuracy: 0.7560\n 8/94 [=&gt;............................] - ETA: 36s - loss: 0.6960 - accuracy: 0.7578\n 9/94 [=&gt;............................] - ETA: 34s - loss: 0.6861 - accuracy: 0.7639\n10/94 [==&gt;...........................] - ETA: 32s - loss: 0.6813 - accuracy: 0.7583\n11/94 [==&gt;...........................] - ETA: 31s - loss: 0.6698 - accuracy: 0.7689\n12/94 [==&gt;...........................] - ETA: 29s - loss: 0.6638 - accuracy: 0.7674\n13/94 [===&gt;..........................] - ETA: 28s - loss: 0.6575 - accuracy: 0.7676\n14/94 [===&gt;..........................] - ETA: 27s - loss: 0.6523 - accuracy: 0.7664\n15/94 [===&gt;..........................] - ETA: 26s - loss: 0.6486 - accuracy: 0.7625\n16/94 [====&gt;.........................] - ETA: 25s - loss: 0.6449 - accuracy: 0.7604\n17/94 [====&gt;.........................] - ETA: 25s - loss: 0.6424 - accuracy: 0.7574\n18/94 [====&gt;.........................] - ETA: 24s - loss: 0.6411 - accuracy: 0.7535\n19/94 [=====&gt;........................] - ETA: 23s - loss: 0.6422 - accuracy: 0.7478\n20/94 [=====&gt;........................] - ETA: 23s - loss: 0.6401 - accuracy: 0.7458\n21/94 [=====&gt;........................] - ETA: 22s - loss: 0.6346 - accuracy: 0.7480\n22/94 [======&gt;.......................] - ETA: 21s - loss: 0.6309 - accuracy: 0.7481\n23/94 [======&gt;.......................] - ETA: 21s - loss: 0.6251 - accuracy: 0.7509\n24/94 [======&gt;.......................] - ETA: 20s - loss: 0.6251 - accuracy: 0.7483\n25/94 [======&gt;.......................] - ETA: 20s - loss: 0.6224 - accuracy: 0.7483\n26/94 [=======&gt;......................] - ETA: 19s - loss: 0.6266 - accuracy: 0.7420\n27/94 [=======&gt;......................] - ETA: 19s - loss: 0.6284 - accuracy: 0.7377\n28/94 [=======&gt;......................] - ETA: 19s - loss: 0.6252 - accuracy: 0.7388\n29/94 [========&gt;.....................] - ETA: 18s - loss: 0.6284 - accuracy: 0.7342\n30/94 [========&gt;.....................] - ETA: 18s - loss: 0.6231 - accuracy: 0.7375\n31/94 [========&gt;.....................] - ETA: 17s - loss: 0.6187 - accuracy: 0.7399\n32/94 [=========&gt;....................] - ETA: 17s - loss: 0.6161 - accuracy: 0.7409\n33/94 [=========&gt;....................] - ETA: 17s - loss: 0.6127 - accuracy: 0.7424\n34/94 [=========&gt;....................] - ETA: 16s - loss: 0.6138 - accuracy: 0.7402\n35/94 [==========&gt;...................] - ETA: 16s - loss: 0.6112 - accuracy: 0.7417\n36/94 [==========&gt;...................] - ETA: 16s - loss: 0.6092 - accuracy: 0.7425\n37/94 [==========&gt;...................] - ETA: 15s - loss: 0.6086 - accuracy: 0.7421\n38/94 [===========&gt;..................] - ETA: 15s - loss: 0.6048 - accuracy: 0.7445\n39/94 [===========&gt;..................] - ETA: 15s - loss: 0.6033 - accuracy: 0.7447\n40/94 [===========&gt;..................] - ETA: 14s - loss: 0.6008 - accuracy: 0.7458\n41/94 [============&gt;.................] - ETA: 14s - loss: 0.5989 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 14s - loss: 0.5956 - accuracy: 0.7485\n43/94 [============&gt;.................] - ETA: 13s - loss: 0.5973 - accuracy: 0.7461\n44/94 [=============&gt;................] - ETA: 13s - loss: 0.5969 - accuracy: 0.7457\n45/94 [=============&gt;................] - ETA: 13s - loss: 0.5967 - accuracy: 0.7449\n46/94 [=============&gt;................] - ETA: 12s - loss: 0.5959 - accuracy: 0.7450\n47/94 [==============&gt;...............] - ETA: 12s - loss: 0.5935 - accuracy: 0.7465\n48/94 [==============&gt;...............] - ETA: 12s - loss: 0.5920 - accuracy: 0.7470\n49/94 [==============&gt;...............] - ETA: 11s - loss: 0.5928 - accuracy: 0.7457\n50/94 [==============&gt;...............] - ETA: 11s - loss: 0.5921 - accuracy: 0.7458\n51/94 [===============&gt;..............] - ETA: 11s - loss: 0.5909 - accuracy: 0.7463\n52/94 [===============&gt;..............] - ETA: 11s - loss: 0.5897 - accuracy: 0.7468\n53/94 [===============&gt;..............] - ETA: 10s - loss: 0.5883 - accuracy: 0.7472\n54/94 [================&gt;.............] - ETA: 10s - loss: 0.5886 - accuracy: 0.7465\n55/94 [================&gt;.............] - ETA: 10s - loss: 0.5909 - accuracy: 0.7439\n56/94 [================&gt;.............] - ETA: 9s - loss: 0.5918 - accuracy: 0.7426 \n57/94 [=================&gt;............] - ETA: 9s - loss: 0.5894 - accuracy: 0.7442\n58/94 [=================&gt;............] - ETA: 9s - loss: 0.5880 - accuracy: 0.7450\n59/94 [=================&gt;............] - ETA: 9s - loss: 0.5873 - accuracy: 0.7451\n60/94 [==================&gt;...........] - ETA: 8s - loss: 0.5867 - accuracy: 0.7451\n61/94 [==================&gt;...........] - ETA: 8s - loss: 0.5855 - accuracy: 0.7459\n62/94 [==================&gt;...........] - ETA: 8s - loss: 0.5840 - accuracy: 0.7466\n63/94 [===================&gt;..........] - ETA: 7s - loss: 0.5853 - accuracy: 0.7450\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.5857 - accuracy: 0.7441\n65/94 [===================&gt;..........] - ETA: 7s - loss: 0.5856 - accuracy: 0.7439\n66/94 [====================&gt;.........] - ETA: 7s - loss: 0.5852 - accuracy: 0.7440\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.5848 - accuracy: 0.7438\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.5842 - accuracy: 0.7439\n69/94 [=====================&gt;........] - ETA: 6s - loss: 0.5838 - accuracy: 0.7440\n70/94 [=====================&gt;........] - ETA: 6s - loss: 0.5837 - accuracy: 0.7435\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.5833 - accuracy: 0.7433\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.5842 - accuracy: 0.7419\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.5834 - accuracy: 0.7423\n74/94 [======================&gt;.......] - ETA: 5s - loss: 0.5829 - accuracy: 0.7424\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.5832 - accuracy: 0.7417\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.5841 - accuracy: 0.7404\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.5843 - accuracy: 0.7397\n78/94 [=======================&gt;......] - ETA: 4s - loss: 0.5845 - accuracy: 0.7390\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.5839 - accuracy: 0.7392\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.5832 - accuracy: 0.7396\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.5817 - accuracy: 0.7410\n82/94 [=========================&gt;....] - ETA: 3s - loss: 0.5816 - accuracy: 0.7406\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.5810 - accuracy: 0.7407\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.5805 - accuracy: 0.7408\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.5796 - accuracy: 0.7414\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.5784 - accuracy: 0.7422\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.5786 - accuracy: 0.7416\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.5788 - accuracy: 0.7410\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.5775 - accuracy: 0.7420\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.5759 - accuracy: 0.7433\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.5744 - accuracy: 0.7443\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.5749 - accuracy: 0.7434\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.5741 - accuracy: 0.7440\n94/94 [==============================] - ETA: 0s - loss: 0.5736 - accuracy: 0.7443\n94/94 [==============================] - 28s 265ms/step - loss: 0.5736 - accuracy: 0.7443 - val_loss: 0.5384 - val_accuracy: 0.7444\nEpoch 2/3\n\n 1/94 [..............................] - ETA: 20s - loss: 0.6250 - accuracy: 0.6458\n 2/94 [..............................] - ETA: 21s - loss: 0.5874 - accuracy: 0.6875\n 3/94 [..............................] - ETA: 20s - loss: 0.5864 - accuracy: 0.6944\n 4/94 [&gt;.............................] - ETA: 20s - loss: 0.5723 - accuracy: 0.7083\n 5/94 [&gt;.............................] - ETA: 20s - loss: 0.5645 - accuracy: 0.7125\n 6/94 [&gt;.............................] - ETA: 19s - loss: 0.5472 - accuracy: 0.7257\n 7/94 [=&gt;............................] - ETA: 19s - loss: 0.5401 - accuracy: 0.7321\n 8/94 [=&gt;............................] - ETA: 19s - loss: 0.5425 - accuracy: 0.7292\n 9/94 [=&gt;............................] - ETA: 19s - loss: 0.5537 - accuracy: 0.7199\n10/94 [==&gt;...........................] - ETA: 19s - loss: 0.5499 - accuracy: 0.7229\n11/94 [==&gt;...........................] - ETA: 18s - loss: 0.5413 - accuracy: 0.7311\n12/94 [==&gt;...........................] - ETA: 18s - loss: 0.5350 - accuracy: 0.7361\n13/94 [===&gt;..........................] - ETA: 18s - loss: 0.5427 - accuracy: 0.7292\n14/94 [===&gt;..........................] - ETA: 18s - loss: 0.5492 - accuracy: 0.7217\n15/94 [===&gt;..........................] - ETA: 17s - loss: 0.5397 - accuracy: 0.7306\n16/94 [====&gt;.........................] - ETA: 17s - loss: 0.5339 - accuracy: 0.7357\n17/94 [====&gt;.........................] - ETA: 17s - loss: 0.5271 - accuracy: 0.7426\n18/94 [====&gt;.........................] - ETA: 17s - loss: 0.5285 - accuracy: 0.7407\n19/94 [=====&gt;........................] - ETA: 16s - loss: 0.5347 - accuracy: 0.7346\n20/94 [=====&gt;........................] - ETA: 16s - loss: 0.5373 - accuracy: 0.7323\n21/94 [=====&gt;........................] - ETA: 16s - loss: 0.5342 - accuracy: 0.7341\n22/94 [======&gt;.......................] - ETA: 16s - loss: 0.5313 - accuracy: 0.7367\n23/94 [======&gt;.......................] - ETA: 16s - loss: 0.5267 - accuracy: 0.7400\n24/94 [======&gt;.......................] - ETA: 15s - loss: 0.5270 - accuracy: 0.7396\n25/94 [======&gt;.......................] - ETA: 15s - loss: 0.5225 - accuracy: 0.7433\n26/94 [=======&gt;......................] - ETA: 15s - loss: 0.5173 - accuracy: 0.7484\n27/94 [=======&gt;......................] - ETA: 15s - loss: 0.5167 - accuracy: 0.7485\n28/94 [=======&gt;......................] - ETA: 14s - loss: 0.5173 - accuracy: 0.7470\n29/94 [========&gt;.....................] - ETA: 14s - loss: 0.5180 - accuracy: 0.7457\n30/94 [========&gt;.....................] - ETA: 14s - loss: 0.5179 - accuracy: 0.7451\n31/94 [========&gt;.....................] - ETA: 14s - loss: 0.5167 - accuracy: 0.7460\n32/94 [=========&gt;....................] - ETA: 14s - loss: 0.5158 - accuracy: 0.7461\n33/94 [=========&gt;....................] - ETA: 13s - loss: 0.5171 - accuracy: 0.7443\n34/94 [=========&gt;....................] - ETA: 13s - loss: 0.5174 - accuracy: 0.7439\n35/94 [==========&gt;...................] - ETA: 13s - loss: 0.5161 - accuracy: 0.7440\n36/94 [==========&gt;...................] - ETA: 13s - loss: 0.5150 - accuracy: 0.7448\n37/94 [==========&gt;...................] - ETA: 12s - loss: 0.5109 - accuracy: 0.7483\n38/94 [===========&gt;..................] - ETA: 12s - loss: 0.5112 - accuracy: 0.7473\n39/94 [===========&gt;..................] - ETA: 12s - loss: 0.5105 - accuracy: 0.7468\n40/94 [===========&gt;..................] - ETA: 12s - loss: 0.5101 - accuracy: 0.7469\n41/94 [============&gt;.................] - ETA: 11s - loss: 0.5098 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 11s - loss: 0.5097 - accuracy: 0.7460\n43/94 [============&gt;.................] - ETA: 11s - loss: 0.5093 - accuracy: 0.7452\n44/94 [=============&gt;................] - ETA: 11s - loss: 0.5086 - accuracy: 0.7453\n45/94 [=============&gt;................] - ETA: 11s - loss: 0.5072 - accuracy: 0.7458\n46/94 [=============&gt;................] - ETA: 10s - loss: 0.5082 - accuracy: 0.7437\n47/94 [==============&gt;...............] - ETA: 10s - loss: 0.5065 - accuracy: 0.7438\n48/94 [==============&gt;...............] - ETA: 10s - loss: 0.5055 - accuracy: 0.7444\n49/94 [==============&gt;...............] - ETA: 10s - loss: 0.5044 - accuracy: 0.7445\n50/94 [==============&gt;...............] - ETA: 9s - loss: 0.5048 - accuracy: 0.7437 \n51/94 [===============&gt;..............] - ETA: 9s - loss: 0.5057 - accuracy: 0.7422\n52/94 [===============&gt;..............] - ETA: 9s - loss: 0.5035 - accuracy: 0.7436\n53/94 [===============&gt;..............] - ETA: 9s - loss: 0.5037 - accuracy: 0.7429\n54/94 [================&gt;.............] - ETA: 9s - loss: 0.5032 - accuracy: 0.7427\n55/94 [================&gt;.............] - ETA: 8s - loss: 0.5009 - accuracy: 0.7443\n56/94 [================&gt;.............] - ETA: 8s - loss: 0.5013 - accuracy: 0.7422\n57/94 [=================&gt;............] - ETA: 8s - loss: 0.5008 - accuracy: 0.7423\n58/94 [=================&gt;............] - ETA: 8s - loss: 0.4997 - accuracy: 0.7428\n59/94 [=================&gt;............] - ETA: 7s - loss: 0.4983 - accuracy: 0.7436\n60/94 [==================&gt;...........] - ETA: 7s - loss: 0.4981 - accuracy: 0.7431\n61/94 [==================&gt;...........] - ETA: 7s - loss: 0.4974 - accuracy: 0.7435\n62/94 [==================&gt;...........] - ETA: 7s - loss: 0.4964 - accuracy: 0.7436\n63/94 [===================&gt;..........] - ETA: 6s - loss: 0.4965 - accuracy: 0.7417\n64/94 [===================&gt;..........] - ETA: 6s - loss: 0.4957 - accuracy: 0.7419\n65/94 [===================&gt;..........] - ETA: 6s - loss: 0.4942 - accuracy: 0.7429\n66/94 [====================&gt;.........] - ETA: 6s - loss: 0.4932 - accuracy: 0.7427\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.4915 - accuracy: 0.7441\n68/94 [====================&gt;.........] - ETA: 5s - loss: 0.4910 - accuracy: 0.7430\n69/94 [=====================&gt;........] - ETA: 5s - loss: 0.4919 - accuracy: 0.7406\n70/94 [=====================&gt;........] - ETA: 5s - loss: 0.4909 - accuracy: 0.7411\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.4882 - accuracy: 0.7430\n72/94 [=====================&gt;........] - ETA: 4s - loss: 0.4880 - accuracy: 0.7425\n73/94 [======================&gt;.......] - ETA: 4s - loss: 0.4863 - accuracy: 0.7437\n74/94 [======================&gt;.......] - ETA: 4s - loss: 0.4869 - accuracy: 0.7424\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.4850 - accuracy: 0.7439\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.4830 - accuracy: 0.7456\n77/94 [=======================&gt;......] - ETA: 3s - loss: 0.4822 - accuracy: 0.7459\n78/94 [=======================&gt;......] - ETA: 3s - loss: 0.4816 - accuracy: 0.7457\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.4816 - accuracy: 0.7447\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.4811 - accuracy: 0.7443\n81/94 [========================&gt;.....] - ETA: 2s - loss: 0.4810 - accuracy: 0.7441\n82/94 [=========================&gt;....] - ETA: 2s - loss: 0.4790 - accuracy: 0.7457\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.4779 - accuracy: 0.7467\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.4776 - accuracy: 0.7460\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.4771 - accuracy: 0.7456\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.4753 - accuracy: 0.7459\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.4754 - accuracy: 0.7455\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.4744 - accuracy: 0.7453\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.4728 - accuracy: 0.7458\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.4722 - accuracy: 0.7454\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.4721 - accuracy: 0.7452\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.4716 - accuracy: 0.7448\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.4700 - accuracy: 0.7457\n94/94 [==============================] - ETA: 0s - loss: 0.4696 - accuracy: 0.7461\n94/94 [==============================] - 22s 231ms/step - loss: 0.4696 - accuracy: 0.7461 - val_loss: 0.4078 - val_accuracy: 0.7694\nEpoch 3/3\n\n 1/94 [..............................] - ETA: 20s - loss: 0.3267 - accuracy: 0.8542\n 2/94 [..............................] - ETA: 20s - loss: 0.3459 - accuracy: 0.8229\n 3/94 [..............................] - ETA: 21s - loss: 0.3373 - accuracy: 0.8125\n 4/94 [&gt;.............................] - ETA: 21s - loss: 0.3315 - accuracy: 0.8073\n 5/94 [&gt;.............................] - ETA: 21s - loss: 0.3337 - accuracy: 0.7958\n 6/94 [&gt;.............................] - ETA: 21s - loss: 0.3475 - accuracy: 0.7917\n 7/94 [=&gt;............................] - ETA: 20s - loss: 0.3404 - accuracy: 0.8006\n 8/94 [=&gt;............................] - ETA: 20s - loss: 0.3466 - accuracy: 0.7943\n 9/94 [=&gt;............................] - ETA: 20s - loss: 0.3442 - accuracy: 0.8032\n10/94 [==&gt;...........................] - ETA: 20s - loss: 0.3392 - accuracy: 0.8083\n11/94 [==&gt;...........................] - ETA: 19s - loss: 0.3358 - accuracy: 0.8068\n12/94 [==&gt;...........................] - ETA: 19s - loss: 0.3321 - accuracy: 0.8090\n13/94 [===&gt;..........................] - ETA: 19s - loss: 0.3294 - accuracy: 0.8141\n14/94 [===&gt;..........................] - ETA: 19s - loss: 0.3287 - accuracy: 0.8155\n15/94 [===&gt;..........................] - ETA: 18s - loss: 0.3260 - accuracy: 0.8194\n16/94 [====&gt;.........................] - ETA: 18s - loss: 0.3209 - accuracy: 0.8242\n17/94 [====&gt;.........................] - ETA: 18s - loss: 0.3193 - accuracy: 0.8284\n18/94 [====&gt;.........................] - ETA: 18s - loss: 0.3199 - accuracy: 0.8264\n19/94 [=====&gt;........................] - ETA: 18s - loss: 0.3217 - accuracy: 0.8224\n20/94 [=====&gt;........................] - ETA: 17s - loss: 0.3201 - accuracy: 0.8260\n21/94 [=====&gt;........................] - ETA: 17s - loss: 0.3176 - accuracy: 0.8264\n22/94 [======&gt;.......................] - ETA: 17s - loss: 0.3133 - accuracy: 0.8295\n23/94 [======&gt;.......................] - ETA: 17s - loss: 0.3119 - accuracy: 0.8288\n24/94 [======&gt;.......................] - ETA: 17s - loss: 0.3090 - accuracy: 0.8307\n25/94 [======&gt;.......................] - ETA: 16s - loss: 0.3111 - accuracy: 0.8292\n26/94 [=======&gt;......................] - ETA: 16s - loss: 0.3098 - accuracy: 0.8301\n27/94 [=======&gt;......................] - ETA: 16s - loss: 0.3088 - accuracy: 0.8302\n28/94 [=======&gt;......................] - ETA: 15s - loss: 0.3106 - accuracy: 0.8274\n29/94 [========&gt;.....................] - ETA: 15s - loss: 0.3073 - accuracy: 0.8312\n30/94 [========&gt;.....................] - ETA: 15s - loss: 0.3091 - accuracy: 0.8285\n31/94 [========&gt;.....................] - ETA: 15s - loss: 0.3089 - accuracy: 0.8293\n32/94 [=========&gt;....................] - ETA: 15s - loss: 0.3050 - accuracy: 0.8327\n33/94 [=========&gt;....................] - ETA: 14s - loss: 0.3034 - accuracy: 0.8352\n34/94 [=========&gt;....................] - ETA: 14s - loss: 0.3024 - accuracy: 0.8370\n35/94 [==========&gt;...................] - ETA: 14s - loss: 0.3010 - accuracy: 0.8387\n36/94 [==========&gt;...................] - ETA: 14s - loss: 0.2993 - accuracy: 0.8409\n37/94 [==========&gt;...................] - ETA: 13s - loss: 0.2973 - accuracy: 0.8435\n38/94 [===========&gt;..................] - ETA: 13s - loss: 0.2974 - accuracy: 0.8432\n39/94 [===========&gt;..................] - ETA: 13s - loss: 0.2952 - accuracy: 0.8456\n40/94 [===========&gt;..................] - ETA: 13s - loss: 0.2947 - accuracy: 0.8458\n41/94 [============&gt;.................] - ETA: 12s - loss: 0.2944 - accuracy: 0.8455\n42/94 [============&gt;.................] - ETA: 12s - loss: 0.2937 - accuracy: 0.8452\n43/94 [============&gt;.................] - ETA: 12s - loss: 0.2943 - accuracy: 0.8450\n44/94 [=============&gt;................] - ETA: 12s - loss: 0.2951 - accuracy: 0.8452\n45/94 [=============&gt;................] - ETA: 11s - loss: 0.2943 - accuracy: 0.8463\n46/94 [=============&gt;................] - ETA: 11s - loss: 0.2928 - accuracy: 0.8474\n47/94 [==============&gt;...............] - ETA: 11s - loss: 0.2914 - accuracy: 0.8484\n48/94 [==============&gt;...............] - ETA: 11s - loss: 0.2905 - accuracy: 0.8490\n49/94 [==============&gt;...............] - ETA: 10s - loss: 0.2901 - accuracy: 0.8491\n50/94 [==============&gt;...............] - ETA: 10s - loss: 0.2906 - accuracy: 0.8487\n51/94 [===============&gt;..............] - ETA: 10s - loss: 0.2894 - accuracy: 0.8497\n52/94 [===============&gt;..............] - ETA: 10s - loss: 0.2894 - accuracy: 0.8490\n53/94 [===============&gt;..............] - ETA: 9s - loss: 0.2883 - accuracy: 0.8510 \n54/94 [================&gt;.............] - ETA: 9s - loss: 0.2859 - accuracy: 0.8534\n55/94 [================&gt;.............] - ETA: 9s - loss: 0.2850 - accuracy: 0.8534\n56/94 [================&gt;.............] - ETA: 9s - loss: 0.2851 - accuracy: 0.8542\n57/94 [=================&gt;............] - ETA: 8s - loss: 0.2863 - accuracy: 0.8531\n58/94 [=================&gt;............] - ETA: 8s - loss: 0.2845 - accuracy: 0.8542\n59/94 [=================&gt;............] - ETA: 8s - loss: 0.2837 - accuracy: 0.8552\n60/94 [==================&gt;...........] - ETA: 8s - loss: 0.2823 - accuracy: 0.8566\n61/94 [==================&gt;...........] - ETA: 7s - loss: 0.2815 - accuracy: 0.8572\n62/94 [==================&gt;...........] - ETA: 7s - loss: 0.2806 - accuracy: 0.8575\n63/94 [===================&gt;..........] - ETA: 7s - loss: 0.2802 - accuracy: 0.8575\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.2785 - accuracy: 0.8594\n65/94 [===================&gt;..........] - ETA: 7s - loss: 0.2770 - accuracy: 0.8609\n66/94 [====================&gt;.........] - ETA: 6s - loss: 0.2754 - accuracy: 0.8621\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.2755 - accuracy: 0.8626\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.2736 - accuracy: 0.8640\n69/94 [=====================&gt;........] - ETA: 6s - loss: 0.2742 - accuracy: 0.8638\n70/94 [=====================&gt;........] - ETA: 5s - loss: 0.2721 - accuracy: 0.8649\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.2706 - accuracy: 0.8656\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.2695 - accuracy: 0.8663\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.2688 - accuracy: 0.8667\n74/94 [======================&gt;.......] - ETA: 4s - loss: 0.2681 - accuracy: 0.8671\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.2670 - accuracy: 0.8678\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.2645 - accuracy: 0.8695\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.2626 - accuracy: 0.8704\n78/94 [=======================&gt;......] - ETA: 3s - loss: 0.2615 - accuracy: 0.8713\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.2607 - accuracy: 0.8716\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.2599 - accuracy: 0.8724\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.2595 - accuracy: 0.8729\n82/94 [=========================&gt;....] - ETA: 2s - loss: 0.2587 - accuracy: 0.8735\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.2587 - accuracy: 0.8740\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.2573 - accuracy: 0.8750\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.2559 - accuracy: 0.8755\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.2546 - accuracy: 0.8762\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.2532 - accuracy: 0.8769\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.2523 - accuracy: 0.8774\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.2521 - accuracy: 0.8776\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.2519 - accuracy: 0.8775\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.2504 - accuracy: 0.8787\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.2505 - accuracy: 0.8788\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8795\n94/94 [==============================] - ETA: 0s - loss: 0.2498 - accuracy: 0.8795\n94/94 [==============================] - 23s 242ms/step - loss: 0.2498 - accuracy: 0.8795 - val_loss: 0.2964 - val_accuracy: 0.8785\n&lt;keras.src.callbacks.History object at 0x0000017233C43750&gt;\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\n\ny_pred_probs = model.predict(X_test)\n\n\n 1/35 [..............................] - ETA: 5s\n 4/35 [==&gt;...........................] - ETA: 0s\n 8/35 [=====&gt;........................] - ETA: 0s\n12/35 [=========&gt;....................] - ETA: 0s\n16/35 [============&gt;.................] - ETA: 0s\n19/35 [===============&gt;..............] - ETA: 0s\n23/35 [==================&gt;...........] - ETA: 0s\n26/35 [=====================&gt;........] - ETA: 0s\n29/35 [=======================&gt;......] - ETA: 0s\n32/35 [==========================&gt;...] - ETA: 0s\n35/35 [==============================] - 1s 18ms/step\n\ny_pred = (model.predict(X_test) &gt; 0.5).astype(\"int32\")\n\n\n 1/35 [..............................] - ETA: 1s\n 4/35 [==&gt;...........................] - ETA: 0s\n 7/35 [=====&gt;........................] - ETA: 0s\n11/35 [========&gt;.....................] - ETA: 0s\n15/35 [===========&gt;..................] - ETA: 0s\n18/35 [==============&gt;...............] - ETA: 0s\n21/35 [=================&gt;............] - ETA: 0s\n24/35 [===================&gt;..........] - ETA: 0s\n27/35 [======================&gt;.......] - ETA: 0s\n31/35 [=========================&gt;....] - ETA: 0s\n34/35 [============================&gt;.] - ETA: 0s\n35/35 [==============================] - 1s 18ms/step\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy}\")\n\nTest Accuracy: 0.8784629133154602\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\n\nConfusion Matrix:\n\nprint(conf_matrix)\n\n[[817  16]\n [120 166]]\n\n\nDas neuronale Netzwerk sagt das Train-Sample zwar perfekt vorher, hat jedoch vergleichsweise große Schwierigkeiten beim Test-Sample."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#fazit",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#fazit",
    "title": "Hate Speech auf Twitter",
    "section": "",
    "text": "Durch die explorative Datenanalyse war es möglich, einige relevante Charakteristika herauszuarbeiten, die Hate-Speech-Tweets klar von anderen Tweets abgrenzen. Hate Speech enthält nämlich einen großen Anteil an Beleidigungen und Schimpfwörtern sowie negativen Sentimenten. Diese Erkenntnisse waren für die Modellierung hilfreich, da es gelang, Features basierend auf der EDA zu generieren, die von hoher Relevanz für die Performance des Modells waren. Das Ziel der Modellierung war es, ein Modell zu trainieren, das Hate Speech in Tweets möglichst akkurat erkennt. Durch den kombinierten Ansatz aus Training und Deep Learning wurde dieses Ziel mit Erfolg erreicht, auch wenn die Deep Learning Modelle vergleichsweise schlecht abschnitten."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#vorverarbeitung",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#vorverarbeitung",
    "title": "Hate Speech auf Twitter",
    "section": "2.1 Vorverarbeitung",
    "text": "2.1 Vorverarbeitung\nUm eine sinnvolle Analyse durchzuführen, müssen noch einige Datenvorverarbeitungsschritte durchlaufen werden. Diese beinhalten die Tokenisierung, das Entfernen von Stopwords und das Bereinigen der Tweets, die Links oder ähnliche Elemente enthalten.\n\nd_hate_clean &lt;- d_hate %&gt;%\n  mutate(tweet = str_remove_all(tweet, pattern = 'RT\\\\s*|http[s]?://\\\\S+|\\\\d+'))\n\nset.seed(123)\ntrain_test_split &lt;- initial_split(d_hate_clean, prop = .8, strata = class)\nd_train &lt;- training(train_test_split)\nd_test &lt;- testing(train_test_split)\n\n\n2.1.1 Tokenisierung\n\ntweets_token &lt;- d_train %&gt;%\n  unnest_tokens(word, tweet)\n\n\n\n2.1.2 Entfernung der Stopwords\n\ndata(stopwords_en, package = \"lsa\")\nstopwords_en &lt;- tibble(word = stopwords_en)\n\ntweets_token &lt;- tweets_token %&gt;%\n  anti_join(stopwords_en)\n\nJoining with `by = join_by(word)`\n\n\n\n\n2.1.3 Sentimentwerte\n\nsenti &lt;- get_sentiments(\"afinn\") %&gt;% \n  mutate(neg_pos = case_when(value &gt; 0 ~ \"pos\",\n                             TRUE ~ \"neg\"))\n\ntweets_senti &lt;- tweets_token %&gt;%\ninner_join(senti)\n\nJoining with `by = join_by(word)`"
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#anteil-von-hate-speech",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#anteil-von-hate-speech",
    "title": "Hate Speech auf Twitter",
    "section": "2.2 Anteil von Hate Speech",
    "text": "2.2 Anteil von Hate Speech\nUm sich einen ersten Überblick über die Daten zu verschaffen, ist es sinnvoll, zunächst einmal den Anteil der als Hate Speech markierten Tweets zu überprüfen.\n\ntweets_token %&gt;% \n  summarise(`Anteil Hate Speech` = mean(class == \"hate speech\")) %&gt;% \n  round(2)\n\n\n\n  \n\n\n\n\nclass_totals &lt;- tweets_token %&gt;%\n  count(class, name = \"class_total\")\n\nggplot(class_totals, aes(x = \"\", y = class_total, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil an Hate Speech\",\n       x = NULL,\n       y = NULL,\n       fill = \"Klasse\") +\n  geom_text(aes(label = class_total), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nDer Anteil der Hate Speech in diesem Datensatz beträgt 25 Prozent. Die Tweets anderer Kategorien sind also deutlich in der Mehrheit."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#worthäufigkeiten",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#worthäufigkeiten",
    "title": "Hate Speech auf Twitter",
    "section": "2.3 Worthäufigkeiten",
    "text": "2.3 Worthäufigkeiten\nEinen weiteren interessanten Einblick gewähren die Worthäufigkeiten. Durch die Visualisierung der am meisten verwendeten Wörter und Wortpaare ist es bereits möglich, einen Einblick in das Vokabular zu erhalten und dieses unter den Klassen zu vergleichen.\n\ntweets_count_senti &lt;- tweets_senti %&gt;%\n  group_by(class) %&gt;% \n  count(class, word, sort = TRUE) %&gt;% \n  slice_head(n = 10)\n\nword_counts &lt;- left_join(tweets_count_senti, class_totals, by = \"class\")\n\n# Berechnung der gewichteten Häufigkeit\nword_counts &lt;- word_counts %&gt;%\n  mutate(weighted_frequency = n / class_total)\n\n# Visualisierung der gewichteten Häufigkeiten\nggplot(word_counts, aes(x = reorder(word, weighted_frequency), y = weighted_frequency, fill = class)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~class, scales = \"free_y\") +\n  coord_flip() +\n  labs(title = \"Gewichtete Häufigkeiten der Wörter in Abhängigkeit von der Klasse\",\n       x = \"Wort\",\n       y = \"Gewichtete Häufigkeit\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nBeim Vergleich der häufigsten Wörter fällt direkt auf, dass Beleidigungen und Schimpfwörter charakteristisch für Hate Speech sind, da die Liste der zehn häufigsten Wörter fast nur aus solchen Einträgen besteht. Das Vokabular der anderen Kategorie ist im Vergleich dazu überaus harmlos. Diese Harmlosigkeit wird durch das häufigste Wort “lol” noch auf die Spitze getrieben. Interessant ist jedoch auch, dass sich das Wort “hate” in dieser Liste wiederfindet. Hier wäre es interessant, im weiteren Verlauf der Analyse den Kontext in Erfahrung zu bringen. Auf der anderen Seite ist “hate” jedoch ein sehr gängiges Wort und dient zur Beschreibung normaler Gefühlszustände, ohne direkt Hass zu verbreiten.\n\ntweets_bigram &lt;- \n  d_train %&gt;%\n  unnest_tokens(bigram, tweet, token = \"ngrams\", n = 2) %&gt;%\n  filter(!is.na(bigram))\n\ntweets_bigram &lt;- tweets_bigram %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \")%&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\ntweets_bigram %&gt;%\n  unite(bigram, word1, word2, sep = \" \") %&gt;%\n  group_by(class) %&gt;% \n  count(bigram, sort = TRUE) %&gt;%\n  slice_max(n, n = 10) %&gt;%\n  mutate(bigram = reorder(bigram, n)) %&gt;%\n  ggplot(aes(n, bigram, fill = class) ) +\n  facet_wrap(~class, scales = \"free_y\") +\n  geom_col() +\n  labs(title = \"Bigramme nach Häufigkeit\",\n       x = \"Häufigkeit\",\n       y = \"Bigramm\") +\n  scale_fill_manual(values = Uriah_Flint) +\n  theme_light()\n\n\n\n\nDie Analyse der häufigsten Wortpaare deckt sich mit der Analyse der häufigsten Wörter. Sie bringt insofern neue Erkenntnisse, als deutlich wird, dass sich der Hass hauptsächlich gegen ethnische und sexuelle Minderheiten richtet. Dies wird anhand von Begriffen wie “white trash” und “fucking faggot” deutlich. Bemerkenswert ist ebenfalls, dass es sich keinesfalls hauptsächlich um Hass gegen Schwarze handelt, sondern genauso auch Menschen mit heller Hautfarbe ethnisch beleidigt werden."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#wortbeziehungen",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#wortbeziehungen",
    "title": "Hate Speech auf Twitter",
    "section": "2.4 Wortbeziehungen",
    "text": "2.4 Wortbeziehungen\nIm Folgenden werden alle Wortpaare, die häufiger als sechs Mal vorkommen, visualisiert. Hierdurch werden die Kontexte der Wörter deutlicher und die Beziehungen können uns Aufschluss darüber geben, in welchem Zusammenhang “hate” verwendet wird.\n\ntweets_bigram_count &lt;- tweets_bigram %&gt;% \n   count(word1, word2, sort = TRUE)\n\nvisualize_bigrams &lt;- function(bigrams) {\n  set.seed(2016)\n  a &lt;- grid::arrow(type = \"closed\", length = unit(.15, \"inches\"))\n  \n  bigrams %&gt;%\n    graph_from_data_frame() %&gt;%\n    ggraph(layout = \"fr\") +\n    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +\n    geom_node_point(color = \"#6FB899\", size = 5) +\n    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +\n    theme_void()\n}\n\ntweets_bigram_count %&gt;%\n  filter(n &gt; 6,\n         !str_detect(word1, \"\\\\d\"),\n         !str_detect(word2, \"\\\\d\")) %&gt;%\n  visualize_bigrams()\n\nWarning: Using the `size` aesthetic in this geom was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` in the `default_aes` field and elsewhere instead.\n\n\n\n\n\nZum Kontext des Wortes “hate” erhalten wir hier keine neuen Hinweise. Jedoch wird klar ersichtlich, in welchen Kombinationen Schimpfwörter verwendet werden. Außerdem werden Ambiguitäten deutlich, da Wörter wie “trash” und “colored” sowohl als rassistische Beleidigung als auch als Beschreibung von Alltagsgegenständen auftauchen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#sentimentanalyse",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#sentimentanalyse",
    "title": "Hate Speech auf Twitter",
    "section": "2.5 Sentimentanalyse",
    "text": "2.5 Sentimentanalyse\nZweck der Sentimentanalyse ist es, herauszufinden, ob die Sentimentausprägungen die beiden Klassen klar voneinander abgrenzen.\n\n# Zählen der negativen und positiven Sentimente\ntweets_senti2 &lt;- tweets_senti %&gt;% \n  group_by(class) %&gt;% \n  count(neg_pos, name = \"count\")\n\n# Visualisierung der Sentimentantanteile nach Klasse\nggplot(tweets_senti2, aes(x = \"\", y = count, fill = neg_pos)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sentimentanteile nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Sentiment\") +\n  facet_wrap(~ class) +\n  geom_text(aes(label = count), position = position_stack(vjust = 0.5)) +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nIn obigem Diagramm wird ersichtlich, dass hasslastige Tweets überwiegend negativ sind, während sich die Sentimente der anderen Klasse in der Waage halten. Das Sentiment ist also ein entscheidender Faktor bei der Klassifizierung von Hate Speech und sollte beim Training des Modells berücksichtigt werden."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#themenanalyse",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#themenanalyse",
    "title": "Hate Speech auf Twitter",
    "section": "2.6 Themenanalyse",
    "text": "2.6 Themenanalyse\nDie Themenanalyse soll Aufschluss darüber geben, ob es bestimmte Themengebiete gibt, die charakteristisch für Hate Speech sind.\n\ntweets_token_counts_hate &lt;- tweets_token %&gt;%\n  filter(class == \"hate speech\") %&gt;% \n  count(word, sort = TRUE) %&gt;%\n  filter(n &gt; 19) %&gt;% \n  select(word)\n\ntweets_dtm_hate &lt;- DocumentTermMatrix(tweets_token_counts_hate)\ntweets_dtm_hate\n\n\ntweets_lda_hate &lt;- LDA(tweets_dtm_hate, k = 4, control = list(seed = 42))\n\ntweets_themen_hate &lt;- tidy(tweets_lda_hate, matrix = \"beta\")\n\ntweets_themen_hate &lt;- tweets_themen_hate %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 7) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\ntweets_themen_hate %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered() +\n  labs(title = \"Themen von Hate Speech\") +\n  theme_minimal() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nObwohl sich die Themen nicht eindeutig voneinander abgrenzen, sind dennoch schwache Muster erkennbar. Thema Eins scheint sich vor allem aus allgemeinen Obszönitäten zusammenzusetzen, während das zweite Thema aus Beleidigungen gegen Schwule und Schwarze und etwas härteren Wörtern wie “kill” und “shit” besteht. In Thema Drei und Vier treten Beleidigungen gegen Frauen sowie die LGBTQ-Community in den Vordergrund. Viel wichtiger als diese kleinen Unterschiede ist jedoch das große Bild der Themen, welches wie schon bei der Analyse der Worthäufigkeiten festgestellt, hauptsächlich aus ethnischen und sexuellen Beleidigungen und Schimpfwörtern besteht."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#schimpfwörter",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#schimpfwörter",
    "title": "Hate Speech auf Twitter",
    "section": "2.7 Schimpfwörter",
    "text": "2.7 Schimpfwörter\nSchimpfwörter scheinen eine große Rolle bei Hate Speech zu spielen. Deshalb erachte ich es als sinnvoll, Schimpfwörter als Feature in das spätere Rezept mit aufzunehmen. Hierzu verwende ich diese Liste, welche ich um ein paar Einträge (rassistische Beleidigungen) ergänzt habe: https://www.insult.wiki/list-of-insults.\n\ninsults &lt;- read.csv(\"insults.csv\")\n\ntweets_token %&gt;%\n  group_by(class) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(insults, by = \"word\") %&gt;% \n  mutate(insult = case_when(is.na(value) == TRUE ~ \"Nein\",\n                            TRUE ~ \"Ja\")) %&gt;% \n  select(-value) %&gt;% \nggplot(aes(x = \"\", y = n, fill = insult)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Anteil der Beleidigungen nach Klasse\",\n       x = NULL,\n       y = NULL,\n       fill = \"Beleidigung\") +\n  facet_wrap(~ class, scales = \"free_y\") +\n  theme_light() +\n  scale_fill_manual(values = Uriah_Flint)\n\n\n\n\nTatsächlich ist der Anteil der Beleidigungen in Hate Speech Tweets höher, jedoch fällt er deutlich geringer aus als erwartet."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#emojis",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#emojis",
    "title": "Hate Speech auf Twitter",
    "section": "2.8 Emojis",
    "text": "2.8 Emojis\nDie Überlegung, dass Hate Speech feindselige Emojis enthält, ist sehr plausibel. Um aggressive Emojis zu kennzeichnen, verwende ich ein von mir erstelltes Lexikon, das solche Emojis enthält und schreibe eine Funktion, die zählt, wieviele feindselige Emojis in einem Tweet vorkommen. Der Totenkopf ist nicht in der Liste der feindseligen Emojis enthalten, da dieser hauptsächlich als Synonym oder Steigerung des Lach-Emojis verwendet wird (engl.: “That’s too funny. I’m dead!”).\n\nhostile_emojis &lt;- read.csv(\"hostile_emojis.csv\")\n\n\ncount_hostile_emojis &lt;- function(text) {\n  # Initialisiere einen leeren Vektor für die Zählungen\n  counts &lt;- numeric(length(hostile_emojis$emoji))\n\n  # Iteriere über jedes Emoji und zähle die Übereinstimmungen im Text\n  for (i in seq_along(hostile_emojis$emoji)) {\n    counts[i] &lt;- sum(lengths(str_extract_all(text, hostile_emojis$emoji[i])))\n  }\n\n  # Summiere die Gesamtanzahl der Übereinstimmungen\n  total_count &lt;- sum(counts)\n  return(total_count)\n}\n\ndummy &lt;- c(\"🗑\", \"bogen\", \"😠\", \"👹\", \"💩\", \"baby\", \"und\", \"🆗\")\ncount_hostile_emojis(dummy)\n\n[1] 3\n\n\n\nd_train %&gt;% \n  mutate(hostile_emojis_n = map_int(tweet, count_hostile_emojis)) %&gt;% \n  summarise(`Feindselige Emojis` = mean(hostile_emojis_n == 1))\n\n\n\n  \n\n\n\nDie Vermutung, dass Hate Speech feindselige Emojis enthält, stellt sich in diesem Fall als falsch heraus. Da es keinen einzigen Emoji dieser Art gibt, wird dieser Ansatz für die Modellierung verworfen."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#shallow-learning",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#shallow-learning",
    "title": "Hate Speech auf Twitter",
    "section": "3.1 Shallow-Learning",
    "text": "3.1 Shallow-Learning\nIn der Modellierung ist es nun das Ziel, einen Algorithmus darauf zu trainieren, möglichst präzise Hate Speech vorherzusagen. Der Algorithmus meiner Wahl ist der XGBoost. Zunächst werden jedoch noch Rezepte formuliert, die die Erkenntnisse aus der Analyse nun in nützliche Features umwandeln.\n\n3.1.1 Rezepte definieren\nRezept Eins enthält Schimpfwörter, Sentimentwerte und die Themenanalyse. Außerdem werden noch die üblichen Textverarbeitungsschritte durchgeführt sowie ein Tokenfilter angewandt.\n\nrec1 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;%\n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_lda(tweet, num_topics = 6)\n\nRezept Zwei enthält statt der Themenanalyse die Tf-idf-Maße.\n\nrec2 &lt;-\n  recipe(class ~ ., data = d_train) %&gt;%\n  update_role(id, new_role = \"id\") %&gt;% \n  step_text_normalization(tweet) %&gt;%\n  step_mutate(insult = get_sentiment(tweet,\n                                       method = \"custom\",\n                                       lexicon = insults)) %&gt;% \n  step_mutate(senti = get_sentiment(tweet)) %&gt;% \n  step_tokenize(tweet, token = \"words\") %&gt;%\n  step_tokenfilter(tweet, max_tokens = 1e2) %&gt;%\n  step_stopwords(tweet, language = \"en\", stopword_source = \"snowball\") %&gt;%\n  step_stem() %&gt;% \n  step_tfidf(tweet)\n\n\nbaked &lt;- rec1 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked\n\n\n\n  \n\n\n\n\nbaked2 &lt;- rec2 %&gt;% \n  prep() %&gt;% \n  bake(new_data = NULL)\nbaked2\n\n\n\n  \n\n\n\n\n\n3.1.2 Modell definieren\n\nxgb &lt;- \n  boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  tree_depth = tune(), \n  learn_rate = tune(), \n  min_n = tune(), \n  loss_reduction = tune()\n  ) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\") %&gt;%\n  translate()\n\n\n\n3.1.3 Workflowset\nDas Modell wird getuned. Hierfür wird zweifache Kreuzvalidierung mit einer Wiederholung verwendet. Der geringe Performance-Zuwachs durch intensiveres Tuning mit mehr Folds und Wiederholungen würde in diesem Fall nicht die höhere Rechenzeit rechtfertigen.\n\npreproc &lt;- list(rec1 = rec1, rec2 = rec2)\n\nmodels &lt;- list(xgb = xgb)\n\nall_workflows &lt;- workflow_set(preproc, models)\n\nmodel_set &lt;-\nall_workflows %&gt;%\nworkflow_map(\n  resamples = vfold_cv(d_train,\n  v = 2, \n  repeats = 1,\n  strata = class),\n  grid = 7,\n  seed = 42,\n  verbose = TRUE, \n  control = control_resamples(save_pred = TRUE))\n\ni 1 of 2 tuning:     rec1_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: package 'stopwords' was built under R version 4.2.3\n\n\nWarning: package 'SnowballC' was built under R version 4.2.3\n\n\nWarning: package 'xgboost' was built under R version 4.2.3\n\n\n✔ 1 of 2 tuning:     rec1_xgb (1m 15s)\n\n\ni 2 of 2 tuning:     rec2_xgb\n\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\n✔ 2 of 2 tuning:     rec2_xgb (1m 31.2s)\n\n\n\n\n3.1.4 Ergebnisse\n\ntune::autoplot(model_set) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nmodel_set %&gt;% \n  collect_metrics() %&gt;% \n  arrange(-mean)\n\n\n\n  \n\n\n\nRezept Zwei hat besser abgeschnitten als Rezept Eins. Wählen wir nun das beste Modell aus und fitten es:\n\n\n3.1.5 Finalisieren\n\nbest_model_params &lt;- \n  extract_workflow_set_result(model_set, \"rec2_xgb\") %&gt;% \n  select_best()\n\nWarning: No value of `metric` was given; metric 'roc_auc' will be used.\n\n\n\nbest_wf &lt;- \nall_workflows %&gt;% \n  extract_workflow(\"rec2_xgb\")\n\nbest_wf_finalized &lt;- \n  best_wf %&gt;% \n  finalize_workflow(best_model_params)\n\nfit_final &lt;- fit(best_wf_finalized, data = d_train)\n\n\nfit_final %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\n\n\n\n\nDie Analyse der wichtigsten Prädiktoren deckt sich mit den Erkenntnissen aus der EDA. Die mit Abstand wichtigsten Features sind die Beleidigungen und Sentimentwerte, während die Tf-idf-Maße von Beleidigungen ebenfalls viel zur Prediction beitragen.\n\nwf_preds &lt;-\n  collect_predictions(model_set)\n\nwf_preds %&gt;%\n  group_by(wflow_id) %&gt;% \n  roc_curve(truth = class, `.pred_hate speech`) %&gt;% \n  autoplot()\n\n\n\n\nDie Performance im Train-Sample fällt sehr gut aus, da die Vorhersagen mit einer Genauigkeit von rund 90 Prozent sehr präzise sind.\n\n\n3.1.6 Vorhersagen\n\npreds &lt;- predict(fit_final, d_test)\npreds\n\n\n\n  \n\n\n\n\nd_test1 &lt;-\n  d_test %&gt;%  \n   bind_cols(preds) %&gt;% \n  mutate(class = as.factor(class))\nd_test1\n\n\n\n  \n\n\n\n\nmy_metrics &lt;- metric_set(accuracy, f_meas)\nmy_metrics(d_test1,\n           truth = class,\n           estimate = .pred_class)\n\n\n\n  \n\n\n\nAuch im Test-Sample bewährt sich das Modell mit einer sehr hohen Genauigkeit."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#klassifikation-mit-transformer",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#klassifikation-mit-transformer",
    "title": "Hate Speech auf Twitter",
    "section": "3.2 Klassifikation mit Transformer",
    "text": "3.2 Klassifikation mit Transformer\nEin weiterer Ansatz zur Klassifikation von Hate Speech ist es, kein eigenes Modell zu trainieren, sondern Zero-Shot-Learning anzuwenden. Das ergibt natürlich am meisten Sinn mit einem sehr fortgeschrittenen und komplexen Transformer-Modell, das bereits auf die Erkennung von Hate Speech trainiert wurde. Im Folgenden wird daher das Modell roberta-hate-speech-dynabench-r4-target von Facebook, welches auf Huggingface verfügbar ist, um die Tweets nach Hate Speech zu klassifizieren. Hierzu wird der Befehl pipeline aus der transformers-Library von Huggingface genutzt, um das Modell zu laden und auf das Test-Sample anzuwenden.\n\nlibrary(reticulate)\n\nWarning: package 'reticulate' was built under R version 4.2.3\n\n\n\nuse_virtualenv(\"C:/Users/rapha/venv\")\n\n\nfrom transformers import pipeline\nimport tensorflow as tf\n\n\nclassifier = pipeline(\"text-classification\", model=\"facebook/roberta-hate-speech-dynabench-r4-target\")\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n\n\n\ntweets &lt;- d_test$tweet\n\n\ntweets = r.tweets\nresults = classifier(tweets)\n\n\nresult &lt;- py$results\nlabels &lt;- lapply(result, function(element) element$label)\ntweets_hate &lt;- cbind(d_test, pred = unlist(labels))\ntweets_hate &lt;- tweets_hate %&gt;% \n  mutate(class = as.factor(class),\n         pred = case_when(pred == \"hate\" ~ \"hate speech\",\n            pred == \"nothate\" ~ \"other\"),\n         pred = as.factor(pred))\n\n\nmy_metrics2 &lt;- metric_set(accuracy, f_meas)\nmy_metrics2(tweets_hate,\n           truth = class,\n           estimate = pred)\n\n\n\n  \n\n\n\nDie Performance des Modells ist objektiv gesehen gut, verglichen mit dem XGBoost mit einer Minute Trainingszeit fällt sie jedoch mager aus."
  },
  {
    "objectID": "posts/Hatespeech_Twitter/hatespeech_twitter.html#neuronales-netzwerk",
    "href": "posts/Hatespeech_Twitter/hatespeech_twitter.html#neuronales-netzwerk",
    "title": "Hate Speech auf Twitter",
    "section": "3.3 Neuronales Netzwerk",
    "text": "3.3 Neuronales Netzwerk\nBisher wurde Hate Speech sowohl mit Hilfe eines auf den konkreten Daten trainierten Shallow-Learner als auch mit Hilfe eines vortrainierten Transformers klassifiziert. Im letzten Schritt dieses Posts sollen die Stärken dieser beiden Ansätze kombiniert werden, indem ein Deep-Learning-Algorithums, genauer gesagt ein Neuronales Netzwerk, auf den vorliegenden Daten trainiert wird. Das neuronale Netz verwendet ein vortrainiertes Wort-Einbettungsmodell mit 50 Dimensionen, das für die deutsche Sprache optimiert ist. Dieses Embedding-Modell ermöglicht es dem Netzwerk, semantische Repräsentationen der Wörter zu erlernen. Das Netzwerk besteht aus einer Eingabeschicht, die das Embedding-Modell enthält, gefolgt von einer vollständig verbundenen Schicht mit 32 Neuronen und einer Sigmoid-Aktivierungsfunktion. Weiterhin gibt es eine Schicht mit 24 Neuronen und einer ReLU-Aktivierung. Die Ausgabeschicht besteht aus einem einzelnen Neuron für binäre Klassifikation. Das Netzwerk wird mit dem Adam-Optimizer kompiliert und die binäre Kreuzentropie wird als Verlustfunktion verwendet. Die Accuracy wird als Metrik überwacht. Das Training erfolgt über 3 Epochen mit einer Batch-Größe von 48, wobei die Validierung anhand des Test-Samples durchgeführt wird.\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\n\nd_train = r.d_train\nd_test = r.d_test\n\nX_train = d_train[\"tweet\"].values\nX_test = d_test[\"tweet\"].values\n\n\nd_train[\"y\"] = d_train[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_train = d_train.loc[:, \"y\"].values\n\nd_test[\"y\"] = d_test[\"class\"].map({\"other\" : 0, \"hate speech\" : 1})\ny_test = d_test.loc[:, \"y\"].values\n\n\nembedding = \"https://tfhub.dev/google/nnlm-de-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\ntf.random.set_seed(42)\n\n\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(32, activation='sigmoid'))\nmodel.add(tf.keras.layers.Dense(24, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\n\n\nmodel.fit(X_train, y_train,\nepochs=3,\nbatch_size=48,\nvalidation_data=(X_test, y_test),\nverbose = 1)\n\nEpoch 1/3\n\n 1/94 [..............................] - ETA: 2:17 - loss: 1.0055 - accuracy: 0.2500\n 2/94 [..............................] - ETA: 1:08 - loss: 0.9941 - accuracy: 0.2396\n 3/94 [..............................] - ETA: 43s - loss: 0.9701 - accuracy: 0.2500 \n 4/94 [&gt;.............................] - ETA: 35s - loss: 0.9375 - accuracy: 0.2760\n 5/94 [&gt;.............................] - ETA: 40s - loss: 0.9277 - accuracy: 0.2833\n 6/94 [&gt;.............................] - ETA: 37s - loss: 0.9264 - accuracy: 0.3229\n 7/94 [=&gt;............................] - ETA: 34s - loss: 0.9098 - accuracy: 0.3750\n 8/94 [=&gt;............................] - ETA: 31s - loss: 0.8973 - accuracy: 0.4245\n 9/94 [=&gt;............................] - ETA: 29s - loss: 0.8848 - accuracy: 0.4676\n10/94 [==&gt;...........................] - ETA: 28s - loss: 0.8702 - accuracy: 0.4917\n11/94 [==&gt;...........................] - ETA: 26s - loss: 0.8579 - accuracy: 0.5265\n12/94 [==&gt;...........................] - ETA: 25s - loss: 0.8456 - accuracy: 0.5451\n13/94 [===&gt;..........................] - ETA: 24s - loss: 0.8332 - accuracy: 0.5625\n14/94 [===&gt;..........................] - ETA: 24s - loss: 0.8219 - accuracy: 0.5759\n15/94 [===&gt;..........................] - ETA: 23s - loss: 0.8109 - accuracy: 0.5847\n16/94 [====&gt;.........................] - ETA: 22s - loss: 0.8012 - accuracy: 0.5938\n17/94 [====&gt;.........................] - ETA: 21s - loss: 0.7920 - accuracy: 0.6005\n18/94 [====&gt;.........................] - ETA: 21s - loss: 0.7836 - accuracy: 0.6053\n19/94 [=====&gt;........................] - ETA: 20s - loss: 0.7772 - accuracy: 0.6075\n20/94 [=====&gt;........................] - ETA: 20s - loss: 0.7696 - accuracy: 0.6125\n21/94 [=====&gt;........................] - ETA: 19s - loss: 0.7607 - accuracy: 0.6210\n22/94 [======&gt;.......................] - ETA: 19s - loss: 0.7528 - accuracy: 0.6269\n23/94 [======&gt;.......................] - ETA: 18s - loss: 0.7441 - accuracy: 0.6350\n24/94 [======&gt;.......................] - ETA: 18s - loss: 0.7391 - accuracy: 0.6372\n25/94 [======&gt;.......................] - ETA: 18s - loss: 0.7326 - accuracy: 0.6417\n26/94 [=======&gt;......................] - ETA: 17s - loss: 0.7306 - accuracy: 0.6394\n27/94 [=======&gt;......................] - ETA: 17s - loss: 0.7273 - accuracy: 0.6389\n28/94 [=======&gt;......................] - ETA: 17s - loss: 0.7213 - accuracy: 0.6436\n29/94 [========&gt;.....................] - ETA: 16s - loss: 0.7201 - accuracy: 0.6422\n30/94 [========&gt;.....................] - ETA: 16s - loss: 0.7127 - accuracy: 0.6486\n31/94 [========&gt;.....................] - ETA: 15s - loss: 0.7061 - accuracy: 0.6539\n32/94 [=========&gt;....................] - ETA: 15s - loss: 0.7009 - accuracy: 0.6576\n33/94 [=========&gt;....................] - ETA: 15s - loss: 0.6954 - accuracy: 0.6616\n34/94 [=========&gt;....................] - ETA: 15s - loss: 0.6937 - accuracy: 0.6618\n35/94 [==========&gt;...................] - ETA: 14s - loss: 0.6890 - accuracy: 0.6655\n36/94 [==========&gt;...................] - ETA: 14s - loss: 0.6849 - accuracy: 0.6684\n37/94 [==========&gt;...................] - ETA: 14s - loss: 0.6824 - accuracy: 0.6700\n38/94 [===========&gt;..................] - ETA: 13s - loss: 0.6769 - accuracy: 0.6743\n39/94 [===========&gt;..................] - ETA: 13s - loss: 0.6734 - accuracy: 0.6763\n40/94 [===========&gt;..................] - ETA: 13s - loss: 0.6692 - accuracy: 0.6792\n41/94 [============&gt;.................] - ETA: 12s - loss: 0.6656 - accuracy: 0.6814\n42/94 [============&gt;.................] - ETA: 12s - loss: 0.6608 - accuracy: 0.6850\n43/94 [============&gt;.................] - ETA: 12s - loss: 0.6609 - accuracy: 0.6841\n44/94 [=============&gt;................] - ETA: 12s - loss: 0.6590 - accuracy: 0.6851\n45/94 [=============&gt;................] - ETA: 11s - loss: 0.6572 - accuracy: 0.6856\n46/94 [=============&gt;................] - ETA: 11s - loss: 0.6552 - accuracy: 0.6870\n47/94 [==============&gt;...............] - ETA: 11s - loss: 0.6516 - accuracy: 0.6897\n48/94 [==============&gt;...............] - ETA: 11s - loss: 0.6489 - accuracy: 0.6914\n49/94 [==============&gt;...............] - ETA: 10s - loss: 0.6485 - accuracy: 0.6913\n50/94 [==============&gt;...............] - ETA: 10s - loss: 0.6467 - accuracy: 0.6925\n51/94 [===============&gt;..............] - ETA: 10s - loss: 0.6444 - accuracy: 0.6940\n52/94 [===============&gt;..............] - ETA: 10s - loss: 0.6421 - accuracy: 0.6955\n53/94 [===============&gt;..............] - ETA: 9s - loss: 0.6396 - accuracy: 0.6969 \n54/94 [================&gt;.............] - ETA: 9s - loss: 0.6390 - accuracy: 0.6971\n55/94 [================&gt;.............] - ETA: 9s - loss: 0.6406 - accuracy: 0.6955\n56/94 [================&gt;.............] - ETA: 9s - loss: 0.6405 - accuracy: 0.6949\n57/94 [=================&gt;............] - ETA: 8s - loss: 0.6372 - accuracy: 0.6974\n58/94 [=================&gt;............] - ETA: 8s - loss: 0.6351 - accuracy: 0.6990\n59/94 [=================&gt;............] - ETA: 8s - loss: 0.6336 - accuracy: 0.6999\n60/94 [==================&gt;...........] - ETA: 8s - loss: 0.6323 - accuracy: 0.7007\n61/94 [==================&gt;...........] - ETA: 7s - loss: 0.6304 - accuracy: 0.7022\n62/94 [==================&gt;...........] - ETA: 7s - loss: 0.6282 - accuracy: 0.7036\n63/94 [===================&gt;..........] - ETA: 7s - loss: 0.6289 - accuracy: 0.7027\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.6288 - accuracy: 0.7025\n65/94 [===================&gt;..........] - ETA: 6s - loss: 0.6281 - accuracy: 0.7029\n66/94 [====================&gt;.........] - ETA: 6s - loss: 0.6273 - accuracy: 0.7036\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.6262 - accuracy: 0.7040\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.6250 - accuracy: 0.7047\n69/94 [=====================&gt;........] - ETA: 5s - loss: 0.6241 - accuracy: 0.7053\n70/94 [=====================&gt;........] - ETA: 5s - loss: 0.6234 - accuracy: 0.7054\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.6224 - accuracy: 0.7057\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.6226 - accuracy: 0.7049\n73/94 [======================&gt;.......] - ETA: 4s - loss: 0.6212 - accuracy: 0.7058\n74/94 [======================&gt;.......] - ETA: 4s - loss: 0.6202 - accuracy: 0.7064\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.6202 - accuracy: 0.7061\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.6206 - accuracy: 0.7053\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.6205 - accuracy: 0.7051\n78/94 [=======================&gt;......] - ETA: 3s - loss: 0.6202 - accuracy: 0.7049\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.6191 - accuracy: 0.7054\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.6180 - accuracy: 0.7063\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.6160 - accuracy: 0.7081\n82/94 [=========================&gt;....] - ETA: 2s - loss: 0.6155 - accuracy: 0.7081\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.6146 - accuracy: 0.7086\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.6137 - accuracy: 0.7091\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.6124 - accuracy: 0.7100\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.6110 - accuracy: 0.7112\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.6108 - accuracy: 0.7110\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.6108 - accuracy: 0.7107\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.6091 - accuracy: 0.7121\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.6073 - accuracy: 0.7137\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.6054 - accuracy: 0.7150\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.6055 - accuracy: 0.7144\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.6044 - accuracy: 0.7153\n94/94 [==============================] - ETA: 0s - loss: 0.6039 - accuracy: 0.7157\n94/94 [==============================] - 24s 242ms/step - loss: 0.6039 - accuracy: 0.7157 - val_loss: 0.5418 - val_accuracy: 0.7444\nEpoch 2/3\n\n 1/94 [..............................] - ETA: 20s - loss: 0.6215 - accuracy: 0.6458\n 2/94 [..............................] - ETA: 19s - loss: 0.5817 - accuracy: 0.6875\n 3/94 [..............................] - ETA: 20s - loss: 0.5782 - accuracy: 0.6944\n 4/94 [&gt;.............................] - ETA: 20s - loss: 0.5656 - accuracy: 0.7083\n 5/94 [&gt;.............................] - ETA: 20s - loss: 0.5602 - accuracy: 0.7125\n 6/94 [&gt;.............................] - ETA: 20s - loss: 0.5445 - accuracy: 0.7257\n 7/94 [=&gt;............................] - ETA: 20s - loss: 0.5387 - accuracy: 0.7321\n 8/94 [=&gt;............................] - ETA: 19s - loss: 0.5409 - accuracy: 0.7292\n 9/94 [=&gt;............................] - ETA: 19s - loss: 0.5535 - accuracy: 0.7199\n10/94 [==&gt;...........................] - ETA: 19s - loss: 0.5506 - accuracy: 0.7229\n11/94 [==&gt;...........................] - ETA: 19s - loss: 0.5415 - accuracy: 0.7311\n12/94 [==&gt;...........................] - ETA: 18s - loss: 0.5354 - accuracy: 0.7361\n13/94 [===&gt;..........................] - ETA: 18s - loss: 0.5428 - accuracy: 0.7292\n14/94 [===&gt;..........................] - ETA: 18s - loss: 0.5496 - accuracy: 0.7217\n15/94 [===&gt;..........................] - ETA: 18s - loss: 0.5407 - accuracy: 0.7306\n16/94 [====&gt;.........................] - ETA: 17s - loss: 0.5352 - accuracy: 0.7357\n17/94 [====&gt;.........................] - ETA: 17s - loss: 0.5285 - accuracy: 0.7426\n18/94 [====&gt;.........................] - ETA: 17s - loss: 0.5292 - accuracy: 0.7407\n19/94 [=====&gt;........................] - ETA: 17s - loss: 0.5362 - accuracy: 0.7346\n20/94 [=====&gt;........................] - ETA: 17s - loss: 0.5386 - accuracy: 0.7323\n21/94 [=====&gt;........................] - ETA: 16s - loss: 0.5357 - accuracy: 0.7341\n22/94 [======&gt;.......................] - ETA: 16s - loss: 0.5331 - accuracy: 0.7367\n23/94 [======&gt;.......................] - ETA: 16s - loss: 0.5288 - accuracy: 0.7400\n24/94 [======&gt;.......................] - ETA: 16s - loss: 0.5296 - accuracy: 0.7396\n25/94 [======&gt;.......................] - ETA: 15s - loss: 0.5254 - accuracy: 0.7433\n26/94 [=======&gt;......................] - ETA: 15s - loss: 0.5200 - accuracy: 0.7484\n27/94 [=======&gt;......................] - ETA: 15s - loss: 0.5196 - accuracy: 0.7485\n28/94 [=======&gt;......................] - ETA: 15s - loss: 0.5203 - accuracy: 0.7470\n29/94 [========&gt;.....................] - ETA: 15s - loss: 0.5208 - accuracy: 0.7457\n30/94 [========&gt;.....................] - ETA: 14s - loss: 0.5207 - accuracy: 0.7451\n31/94 [========&gt;.....................] - ETA: 14s - loss: 0.5196 - accuracy: 0.7460\n32/94 [=========&gt;....................] - ETA: 14s - loss: 0.5188 - accuracy: 0.7461\n33/94 [=========&gt;....................] - ETA: 14s - loss: 0.5202 - accuracy: 0.7443\n34/94 [=========&gt;....................] - ETA: 14s - loss: 0.5202 - accuracy: 0.7439\n35/94 [==========&gt;...................] - ETA: 13s - loss: 0.5189 - accuracy: 0.7440\n36/94 [==========&gt;...................] - ETA: 13s - loss: 0.5181 - accuracy: 0.7448\n37/94 [==========&gt;...................] - ETA: 13s - loss: 0.5138 - accuracy: 0.7483\n38/94 [===========&gt;..................] - ETA: 13s - loss: 0.5140 - accuracy: 0.7473\n39/94 [===========&gt;..................] - ETA: 12s - loss: 0.5136 - accuracy: 0.7468\n40/94 [===========&gt;..................] - ETA: 12s - loss: 0.5134 - accuracy: 0.7469\n41/94 [============&gt;.................] - ETA: 12s - loss: 0.5133 - accuracy: 0.7464\n42/94 [============&gt;.................] - ETA: 12s - loss: 0.5134 - accuracy: 0.7460\n43/94 [============&gt;.................] - ETA: 12s - loss: 0.5132 - accuracy: 0.7452\n44/94 [=============&gt;................] - ETA: 11s - loss: 0.5123 - accuracy: 0.7453\n45/94 [=============&gt;................] - ETA: 11s - loss: 0.5109 - accuracy: 0.7458\n46/94 [=============&gt;................] - ETA: 11s - loss: 0.5121 - accuracy: 0.7437\n47/94 [==============&gt;...............] - ETA: 11s - loss: 0.5108 - accuracy: 0.7438\n48/94 [==============&gt;...............] - ETA: 10s - loss: 0.5098 - accuracy: 0.7444\n49/94 [==============&gt;...............] - ETA: 10s - loss: 0.5090 - accuracy: 0.7445\n50/94 [==============&gt;...............] - ETA: 10s - loss: 0.5096 - accuracy: 0.7437\n51/94 [===============&gt;..............] - ETA: 10s - loss: 0.5103 - accuracy: 0.7422\n52/94 [===============&gt;..............] - ETA: 9s - loss: 0.5083 - accuracy: 0.7436 \n53/94 [===============&gt;..............] - ETA: 9s - loss: 0.5088 - accuracy: 0.7429\n54/94 [================&gt;.............] - ETA: 9s - loss: 0.5084 - accuracy: 0.7427\n55/94 [================&gt;.............] - ETA: 9s - loss: 0.5061 - accuracy: 0.7443\n56/94 [================&gt;.............] - ETA: 8s - loss: 0.5068 - accuracy: 0.7422\n57/94 [=================&gt;............] - ETA: 8s - loss: 0.5062 - accuracy: 0.7427\n58/94 [=================&gt;............] - ETA: 8s - loss: 0.5053 - accuracy: 0.7432\n59/94 [=================&gt;............] - ETA: 8s - loss: 0.5039 - accuracy: 0.7440\n60/94 [==================&gt;...........] - ETA: 8s - loss: 0.5037 - accuracy: 0.7434\n61/94 [==================&gt;...........] - ETA: 7s - loss: 0.5029 - accuracy: 0.7439\n62/94 [==================&gt;...........] - ETA: 7s - loss: 0.5019 - accuracy: 0.7440\n63/94 [===================&gt;..........] - ETA: 7s - loss: 0.5019 - accuracy: 0.7421\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.5013 - accuracy: 0.7425\n65/94 [===================&gt;..........] - ETA: 6s - loss: 0.4999 - accuracy: 0.7436\n66/94 [====================&gt;.........] - ETA: 6s - loss: 0.4988 - accuracy: 0.7440\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.4970 - accuracy: 0.7453\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.4967 - accuracy: 0.7442\n69/94 [=====================&gt;........] - ETA: 5s - loss: 0.4974 - accuracy: 0.7418\n70/94 [=====================&gt;........] - ETA: 5s - loss: 0.4964 - accuracy: 0.7423\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.4939 - accuracy: 0.7441\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.4938 - accuracy: 0.7436\n73/94 [======================&gt;.......] - ETA: 4s - loss: 0.4922 - accuracy: 0.7449\n74/94 [======================&gt;.......] - ETA: 4s - loss: 0.4928 - accuracy: 0.7435\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.4908 - accuracy: 0.7450\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.4889 - accuracy: 0.7467\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.4880 - accuracy: 0.7470\n78/94 [=======================&gt;......] - ETA: 3s - loss: 0.4875 - accuracy: 0.7471\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.4876 - accuracy: 0.7460\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.4873 - accuracy: 0.7456\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.4871 - accuracy: 0.7456\n82/94 [=========================&gt;....] - ETA: 2s - loss: 0.4850 - accuracy: 0.7472\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.4838 - accuracy: 0.7482\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.4835 - accuracy: 0.7478\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.4833 - accuracy: 0.7473\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.4815 - accuracy: 0.7476\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.4816 - accuracy: 0.7471\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.4807 - accuracy: 0.7469\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.4794 - accuracy: 0.7474\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.4789 - accuracy: 0.7472\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.4786 - accuracy: 0.7470\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.4782 - accuracy: 0.7466\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.4766 - accuracy: 0.7475\n94/94 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.7479\n94/94 [==============================] - 23s 245ms/step - loss: 0.4762 - accuracy: 0.7479 - val_loss: 0.4208 - val_accuracy: 0.7712\nEpoch 3/3\n\n 1/94 [..............................] - ETA: 22s - loss: 0.3227 - accuracy: 0.8333\n 2/94 [..............................] - ETA: 20s - loss: 0.3428 - accuracy: 0.8021\n 3/94 [..............................] - ETA: 21s - loss: 0.3412 - accuracy: 0.8056\n 4/94 [&gt;.............................] - ETA: 21s - loss: 0.3389 - accuracy: 0.8073\n 5/94 [&gt;.............................] - ETA: 21s - loss: 0.3404 - accuracy: 0.7958\n 6/94 [&gt;.............................] - ETA: 21s - loss: 0.3527 - accuracy: 0.7882\n 7/94 [=&gt;............................] - ETA: 21s - loss: 0.3438 - accuracy: 0.8006\n 8/94 [=&gt;............................] - ETA: 21s - loss: 0.3502 - accuracy: 0.7943\n 9/94 [=&gt;............................] - ETA: 20s - loss: 0.3469 - accuracy: 0.8009\n10/94 [==&gt;...........................] - ETA: 20s - loss: 0.3406 - accuracy: 0.8083\n11/94 [==&gt;...........................] - ETA: 20s - loss: 0.3366 - accuracy: 0.8068\n12/94 [==&gt;...........................] - ETA: 20s - loss: 0.3341 - accuracy: 0.8108\n13/94 [===&gt;..........................] - ETA: 20s - loss: 0.3323 - accuracy: 0.8125\n14/94 [===&gt;..........................] - ETA: 19s - loss: 0.3327 - accuracy: 0.8125\n15/94 [===&gt;..........................] - ETA: 19s - loss: 0.3299 - accuracy: 0.8167\n16/94 [====&gt;.........................] - ETA: 19s - loss: 0.3246 - accuracy: 0.8203\n17/94 [====&gt;.........................] - ETA: 19s - loss: 0.3227 - accuracy: 0.8235\n18/94 [====&gt;.........................] - ETA: 18s - loss: 0.3232 - accuracy: 0.8241\n19/94 [=====&gt;........................] - ETA: 18s - loss: 0.3248 - accuracy: 0.8224\n20/94 [=====&gt;........................] - ETA: 18s - loss: 0.3228 - accuracy: 0.8250\n21/94 [=====&gt;........................] - ETA: 18s - loss: 0.3193 - accuracy: 0.8264\n22/94 [======&gt;.......................] - ETA: 17s - loss: 0.3154 - accuracy: 0.8305\n23/94 [======&gt;.......................] - ETA: 17s - loss: 0.3136 - accuracy: 0.8297\n24/94 [======&gt;.......................] - ETA: 17s - loss: 0.3116 - accuracy: 0.8316\n25/94 [======&gt;.......................] - ETA: 17s - loss: 0.3132 - accuracy: 0.8292\n26/94 [=======&gt;......................] - ETA: 16s - loss: 0.3118 - accuracy: 0.8301\n27/94 [=======&gt;......................] - ETA: 16s - loss: 0.3111 - accuracy: 0.8295\n28/94 [=======&gt;......................] - ETA: 16s - loss: 0.3135 - accuracy: 0.8266\n29/94 [========&gt;.....................] - ETA: 16s - loss: 0.3102 - accuracy: 0.8297\n30/94 [========&gt;.....................] - ETA: 15s - loss: 0.3115 - accuracy: 0.8271\n31/94 [========&gt;.....................] - ETA: 15s - loss: 0.3115 - accuracy: 0.8286\n32/94 [=========&gt;....................] - ETA: 15s - loss: 0.3076 - accuracy: 0.8320\n33/94 [=========&gt;....................] - ETA: 15s - loss: 0.3058 - accuracy: 0.8346\n34/94 [=========&gt;....................] - ETA: 14s - loss: 0.3048 - accuracy: 0.8370\n35/94 [==========&gt;...................] - ETA: 14s - loss: 0.3034 - accuracy: 0.8393\n36/94 [==========&gt;...................] - ETA: 14s - loss: 0.3014 - accuracy: 0.8414\n37/94 [==========&gt;...................] - ETA: 14s - loss: 0.2994 - accuracy: 0.8446\n38/94 [===========&gt;..................] - ETA: 13s - loss: 0.2986 - accuracy: 0.8443\n39/94 [===========&gt;..................] - ETA: 13s - loss: 0.2963 - accuracy: 0.8462\n40/94 [===========&gt;..................] - ETA: 13s - loss: 0.2952 - accuracy: 0.8469\n41/94 [============&gt;.................] - ETA: 13s - loss: 0.2949 - accuracy: 0.8460\n42/94 [============&gt;.................] - ETA: 12s - loss: 0.2930 - accuracy: 0.8472\n43/94 [============&gt;.................] - ETA: 12s - loss: 0.2942 - accuracy: 0.8469\n44/94 [=============&gt;................] - ETA: 12s - loss: 0.2948 - accuracy: 0.8466\n45/94 [=============&gt;................] - ETA: 12s - loss: 0.2936 - accuracy: 0.8477\n46/94 [=============&gt;................] - ETA: 11s - loss: 0.2924 - accuracy: 0.8478\n47/94 [==============&gt;...............] - ETA: 11s - loss: 0.2911 - accuracy: 0.8480\n48/94 [==============&gt;...............] - ETA: 11s - loss: 0.2902 - accuracy: 0.8485\n49/94 [==============&gt;...............] - ETA: 11s - loss: 0.2892 - accuracy: 0.8495\n50/94 [==============&gt;...............] - ETA: 10s - loss: 0.2894 - accuracy: 0.8487\n51/94 [===============&gt;..............] - ETA: 10s - loss: 0.2882 - accuracy: 0.8497\n52/94 [===============&gt;..............] - ETA: 10s - loss: 0.2883 - accuracy: 0.8494\n53/94 [===============&gt;..............] - ETA: 10s - loss: 0.2868 - accuracy: 0.8510\n54/94 [================&gt;.............] - ETA: 9s - loss: 0.2846 - accuracy: 0.8530 \n55/94 [================&gt;.............] - ETA: 9s - loss: 0.2834 - accuracy: 0.8534\n56/94 [================&gt;.............] - ETA: 9s - loss: 0.2834 - accuracy: 0.8542\n57/94 [=================&gt;............] - ETA: 9s - loss: 0.2849 - accuracy: 0.8534\n58/94 [=================&gt;............] - ETA: 8s - loss: 0.2835 - accuracy: 0.8542\n59/94 [=================&gt;............] - ETA: 8s - loss: 0.2826 - accuracy: 0.8545\n60/94 [==================&gt;...........] - ETA: 8s - loss: 0.2808 - accuracy: 0.8559\n61/94 [==================&gt;...........] - ETA: 8s - loss: 0.2801 - accuracy: 0.8562\n62/94 [==================&gt;...........] - ETA: 7s - loss: 0.2792 - accuracy: 0.8572\n63/94 [===================&gt;..........] - ETA: 7s - loss: 0.2790 - accuracy: 0.8565\n64/94 [===================&gt;..........] - ETA: 7s - loss: 0.2773 - accuracy: 0.8584\n65/94 [===================&gt;..........] - ETA: 7s - loss: 0.2757 - accuracy: 0.8599\n66/94 [====================&gt;.........] - ETA: 6s - loss: 0.2745 - accuracy: 0.8611\n67/94 [====================&gt;.........] - ETA: 6s - loss: 0.2750 - accuracy: 0.8610\n68/94 [====================&gt;.........] - ETA: 6s - loss: 0.2731 - accuracy: 0.8621\n69/94 [=====================&gt;........] - ETA: 6s - loss: 0.2737 - accuracy: 0.8617\n70/94 [=====================&gt;........] - ETA: 5s - loss: 0.2718 - accuracy: 0.8631\n71/94 [=====================&gt;........] - ETA: 5s - loss: 0.2707 - accuracy: 0.8638\n72/94 [=====================&gt;........] - ETA: 5s - loss: 0.2695 - accuracy: 0.8646\n73/94 [======================&gt;.......] - ETA: 5s - loss: 0.2690 - accuracy: 0.8647\n74/94 [======================&gt;.......] - ETA: 4s - loss: 0.2687 - accuracy: 0.8646\n75/94 [======================&gt;.......] - ETA: 4s - loss: 0.2675 - accuracy: 0.8656\n76/94 [=======================&gt;......] - ETA: 4s - loss: 0.2650 - accuracy: 0.8673\n77/94 [=======================&gt;......] - ETA: 4s - loss: 0.2630 - accuracy: 0.8688\n78/94 [=======================&gt;......] - ETA: 3s - loss: 0.2617 - accuracy: 0.8694\n79/94 [========================&gt;.....] - ETA: 3s - loss: 0.2610 - accuracy: 0.8697\n80/94 [========================&gt;.....] - ETA: 3s - loss: 0.2602 - accuracy: 0.8706\n81/94 [========================&gt;.....] - ETA: 3s - loss: 0.2598 - accuracy: 0.8704\n82/94 [=========================&gt;....] - ETA: 2s - loss: 0.2590 - accuracy: 0.8707\n83/94 [=========================&gt;....] - ETA: 2s - loss: 0.2590 - accuracy: 0.8707\n84/94 [=========================&gt;....] - ETA: 2s - loss: 0.2579 - accuracy: 0.8713\n85/94 [==========================&gt;...] - ETA: 2s - loss: 0.2566 - accuracy: 0.8721\n86/94 [==========================&gt;...] - ETA: 1s - loss: 0.2552 - accuracy: 0.8728\n87/94 [==========================&gt;...] - ETA: 1s - loss: 0.2539 - accuracy: 0.8738\n88/94 [===========================&gt;..] - ETA: 1s - loss: 0.2530 - accuracy: 0.8741\n89/94 [===========================&gt;..] - ETA: 1s - loss: 0.2529 - accuracy: 0.8736\n90/94 [===========================&gt;..] - ETA: 0s - loss: 0.2530 - accuracy: 0.8743\n91/94 [============================&gt;.] - ETA: 0s - loss: 0.2515 - accuracy: 0.8755\n92/94 [============================&gt;.] - ETA: 0s - loss: 0.2513 - accuracy: 0.8757\n93/94 [============================&gt;.] - ETA: 0s - loss: 0.2506 - accuracy: 0.8763\n94/94 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.8762\n94/94 [==============================] - 24s 254ms/step - loss: 0.2507 - accuracy: 0.8762 - val_loss: 0.3095 - val_accuracy: 0.8686\n&lt;keras.src.callbacks.History object at 0x0000022A33ACF810&gt;\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\nWARNING:tensorflow:From C:\\Users\\rapha\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n\n\n\ny_pred_probs = model.predict(X_test)\n\n\n 1/35 [..............................] - ETA: 4s\n 4/35 [==&gt;...........................] - ETA: 0s\n 8/35 [=====&gt;........................] - ETA: 0s\n12/35 [=========&gt;....................] - ETA: 0s\n15/35 [===========&gt;..................] - ETA: 0s\n19/35 [===============&gt;..............] - ETA: 0s\n23/35 [==================&gt;...........] - ETA: 0s\n26/35 [=====================&gt;........] - ETA: 0s\n30/35 [========================&gt;.....] - ETA: 0s\n34/35 [============================&gt;.] - ETA: 0s\n35/35 [==============================] - 1s 17ms/step\n\ny_pred = (model.predict(X_test) &gt; 0.5).astype(\"int32\")\n\n\n 1/35 [..............................] - ETA: 1s\n 4/35 [==&gt;...........................] - ETA: 0s\n 8/35 [=====&gt;........................] - ETA: 0s\n11/35 [========&gt;.....................] - ETA: 0s\n15/35 [===========&gt;..................] - ETA: 0s\n18/35 [==============&gt;...............] - ETA: 0s\n22/35 [=================&gt;............] - ETA: 0s\n25/35 [====================&gt;.........] - ETA: 0s\n29/35 [=======================&gt;......] - ETA: 0s\n33/35 [===========================&gt;..] - ETA: 0s\n35/35 [==============================] - 1s 17ms/step\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy}\")\n\nTest Accuracy: 0.868632707774799\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\n\nConfusion Matrix:\n\nprint(conf_matrix)\n\n[[817  16]\n [131 155]]\n\n\nDas neuronale Netzwerk sagt das Train-Sample zwar perfekt vorher, hat jedoch vergleichsweise große Schwierigkeiten beim Test-Sample."
  }
]