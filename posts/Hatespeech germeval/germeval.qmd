---
title: "Hatespeech Klassifikation"
author: "Raphael Balzer"
date: "2023-11-25"
image: "gemaelde.jpeg"
categories:
  - Textanalyse
  - Klassifikation
  - tidymodels
---

# Hatespeech Klassifikation

Klassifikation von Hatespeech auf Grundlage der Germeval-Daten.

## Vorbereitung
### Pakte laden
```{r output=FALSE}
library(tidymodels)
library(textrecipes)
library(syuzhet)
library(stringr)
library(slider)
library(tidytext)
library(furrr)
library(widyr)
library(irlba)
library(datawizard)
library(lightgbm)
library(bonsai)
library(vip)
data("schimpfwoerter", package = "pradadata")
data("sentiws", package = "pradadata")
data(wild_emojis, package = "pradadata")
```

### Datenimport
Bei den Daten handelt es sich um die Trainings- und Testdaten (deutsche Tweets) aus der GermEval 2018 Shared Task zum Erkennen von beleidigender Sprache.

```{r}
d_train <- 
  data_read("germeval2018.training.txt",
         header = FALSE,
         quote = "")
d_test <- 
  data_read("germeval2018.test.txt",
         header = FALSE,
         quote = "")
names(d_train) <- c("text", "c1", "c2")
names(d_test) <- c("text", "c1", "c2")
```

## Feature Engineering
Ziel ist es, auf Grundlage der Tweets einige n√ºtzliche Features zu generieren, die sich als Pr√§diktor f√ºr die AV (Hatespeech oder nicht) eignen.\
Da das Attribut `lexicon` Funktion `get_sentiment` ein Dataframe mit mindestens zwei Spalten mit dem Namen "word" und "value" haben muss, f√ºge ich die Spalte "value" zum Schimpfw√∂rterlexikon hinzu, um W√∂rter als Schimpfwort zu kennzeichnen.

```{r}
schimpfwoerter$value <- 1
```

### Emojis
Um "wilde" Emojis zu kennzeichnen, verwende ich ein Lexikon, das solche Emojis enth√§lt und schreibe eine Funktion, die z√§hlt, wieviele wilde Emojis in einem Tweet vorkommen.

```{r}
count_wild_emojis <- function(text) {
  # Initialisiere einen leeren Vektor f√ºr die Z√§hlungen
  counts <- numeric(length(wild_emojis$emoji))

  # Iteriere √ºber jedes Emoji und z√§hle die √úbereinstimmungen im Text
  for (i in seq_along(wild_emojis$emoji)) {
    counts[i] <- sum(lengths(str_extract_all(text, wild_emojis$emoji[i])))
  }

  # Summiere die Gesamtanzahl der √úbereinstimmungen
  total_count <- sum(counts)
  return(total_count)
}

dummy <- c("üóë", "bogen", "üò†", "üëπ", "üí©", "baby", "und", "üÜó")
count_wild_emojis(dummy)
```
### Word Embedding

```{r}
nested_words <- d_train %>%
  select(text) %>% 
  unnest_tokens(word, text) %>%
  nest(words = c(word))
```

Skipgrams identifizieren:
```{r}
slide_windows <- function(tbl, window_size) {
  skipgrams <- slider::slide(
    tbl, 
    ~.x, 
    .after = window_size - 1, 
    .step = 1, 
    .complete = TRUE
  )
  
  safe_mutate <- safely(mutate)
  
  out <- map2(skipgrams,
              1:length(skipgrams),
              ~ safe_mutate(.x, window_id = .y))
  
  out %>%
    transpose() %>%
    pluck("result") %>%
    compact() %>%
    bind_rows()
}
```

PMI berechnen:
```{r}
tidy_pmi <- nested_words %>%
  mutate(words = future_map(words, slide_windows, 4L)) %>%
  unnest(words) %>%
  pairwise_pmi(word, window_id)
tidy_pmi
```
Wortvektoren erstellen:
```{r}
tidy_word_vectors <- tidy_pmi %>%
  widely_svd(
    item1, item2, pmi,
    nv = 100, maxit = 1000
  )

tidy_word_vectors
```

## Modellierung

### Rezepte definieren

Rezept 1 enth√§lt Schimpfw√∂rter, Sentimentwerte, aggressive Emojis und Word Embeddings
```{r}
rec1 <-
  recipe(c1 ~ ., data = d_train) %>% 
  update_role(c2, new_role = "ignore") %>%  
  step_text_normalization(text) %>%
  step_mutate(schimpfw = get_sentiment(text,
                                       method = "custom",
                                       lexicon = schimpfwoerter)) %>% 
  step_mutate(senti = get_sentiment(text,
                                    method = "custom",
                                    lexicon = sentiws)) %>%
  step_mutate(wild_emojis_n = map_int(text, 
                                      count_wild_emojis)) %>% 
  step_tokenize(text, token = "words") %>% 
  step_stem(text) %>% 
  step_tokenfilter(text, max_tokens = 1e2) %>%
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_word_embeddings(text,
                       embeddings = tidy_word_vectors,
                       aggregation = "mean")
```

Rezept 2 enth√§lt statt Word-Embeddings tfidf.
```{r}
rec2 <-
  recipe(c1 ~ ., data = d_train) %>% 
  update_role(c2, new_role = "ignore") %>%  
  step_text_normalization(text) %>%
  step_mutate(schimpfw = get_sentiment(text,
                                       method = "custom",
                                       lexicon = schimpfwoerter)) %>% 
  step_mutate(senti = get_sentiment(text,
                                    method = "custom",
                                    lexicon = sentiws)) %>%
  step_mutate(wild_emojis_n = map_int(text, 
                                      count_wild_emojis)) %>% 
  step_tokenize(text, token = "words") %>% 
  step_stem(text) %>% 
  step_tokenfilter(text, max_tokens = 1e2) %>%
  step_stopwords(text, language = "de", stopword_source = "snowball") %>% 
  step_tfidf(text)
```

```{r}
baked <- rec1 %>% 
  prep() %>% 
  bake(new_data = NULL)
baked
```

```{r}
baked2 <- rec1 %>% 
  prep() %>% 
  bake(new_data = NULL)
baked2
```

### Modelle definieren
Ich verwende f√ºr die Modellierung zum einen einen K-Nearest-Neighbour-Algorithums und zum anderen XGBoost.

```{r}
knn <- 
  nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune()
) %>% 
  set_engine("kknn") %>% 
  set_mode("classification") %>% 
  translate()

xgb <- 
  boost_tree(
  mtry = tune(), 
  trees = tune(), 
  tree_depth = tune(), 
  learn_rate = tune(), 
  min_n = tune(), 
  loss_reduction = tune()
  ) %>%
  set_engine("xgboost") %>%
  set_mode("classification") %>%
  translate()
```

### Workflowset erstellen

```{r}
preproc <- list(rec1 = rec1, rec2 = rec2)

models <- list(xgb = xgb, knn = knn)

all_workflows <- workflow_set(preproc, models)

model_set <-
all_workflows %>%
workflow_map(
  resamples = vfold_cv(d_train,
  v = 2, 
  repeats = 1),
  grid = 5,
  seed = 42,
  verbose = TRUE)
```

### Ergebnisse
```{r}
tune::autoplot(model_set) +
  theme(legend.position = "bottom")
```

```{r}
model_set %>% 
  collect_metrics() %>% 
  arrange(-mean)
```
LightGBM hat deutlich besser abgeschnitten als KNN. W√§hlen wir nun das beste Modell aus und fitten es:

### Finalisieren
```{r}
best_model_params <- 
  extract_workflow_set_result(model_set, "rec1_xgb") %>% 
  select_best()
```

```{r}
best_wf <- 
all_workflows %>% 
  extract_workflow("rec1_xgb")

best_wf_finalized <- 
  best_wf %>% 
  finalize_workflow(best_model_params)

fit_final <- fit(best_wf_finalized, data = d_train)
```

```{r}
fit_final %>% 
  extract_fit_parsnip() %>% 
  vip() 
```

## Vorhersagen
```{r}
preds <- predict(fit_final, d_test)
preds
```

### Bestimmen der Vorhersageg√ºte im Test-Sample
```{r}
d_test <-
  d_test %>%  
   bind_cols(preds) %>% 
  mutate(c1 = as.factor(c1))
d_test
```


```{r}
my_metrics <- metric_set(accuracy, f_meas)
my_metrics(d_test,
           truth = c1,
           estimate = .pred_class)
```